{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1: \n",
    "Model Classical Brownian Motion, arithmetic return system, and log return (Gerometric Brownian Motion) and compare theoretical and simulated mean and standard deviation. `r_t` is assumed to follow `N(0, Ïƒ^2)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Approach\n",
    "1. Calculation of expected mean and standard deviation is done by manual calculation using the knowledge of normal distribution of `r_t`. \n",
    "2. For the estimated price, I simulated 10,000 iterations for each of the three methods of calculating the price at time `t`, using a standard deviation (`sigma`) of 0.1 for the normal distribution of returns `r_t` and an initial price (`p_{t-1}`) of 100. \n",
    "3. Mean and standard deviation are calculated from the saved list. \n",
    "\n",
    "### Expected Results\n",
    "It is assumed that `P_{t-1} = m`, where `P_{t-1}` is known as today's price and is a constant.\n",
    "\n",
    "Given `r_t \\sim N(0, \\sigma^2)`, the following are the derivations for each model:\n",
    "\n",
    "1. **Classical Brownian Motion**:\n",
    "   - Expected Mean:  \\( E[P_t] = E[P_{t-1} + r_t] = P_{t-1} + E[r_t] = m \\)\n",
    "   - Standard Deviation: \\( SD(P_t) = SD(P_{t-1} + r_t) = \\sigma \\)\n",
    "\n",
    "2. **Arithmetic Return System**:\n",
    "   - Expected Mean: \\( E[P_t] = E[P_{t-1} \\cdot (1 + r_t)] = P_{t-1} + E[r_t] \\cdot P_{t-1} = m \\)\n",
    "   - Standard Deviation: \\( SD(P_t) = SD(P_{t-1} \\cdot (1 + r_t)) = P_{t-1} \\cdot SD(r_t) = m \\cdot \\sigma \\)\n",
    "\n",
    "3. **Log Return or Geometric Brownian Motion**:\n",
    "   - Expected Mean:  \\( E[P_t] = E[P_{t-1} \\cdot e^{r_t}] = P_{t-1} \\cdot E[e^{r_t}] = m \\cdot e^{\\frac{\\sigma^2}{2}} \\)\n",
    "   - Standard Deviation: \\( SD(P_t) = SD(P_{t-1} \\cdot e^{r_t}) = P_{t-1} \\cdot SD(e^{r_t}) = m \\cdot \\sqrt{e^{\\sigma^2} - 1} \\)\n",
    "\n",
    "In the case of the log return or geometric Brownian motion, the expected mean is multiplied by \\( e^{\\frac{\\sigma^2}{2}} \\) due to the properties of the log-normal distribution when exponentiating a normally distributed variable. The standard deviation is the initial price multiplied by the square root of \\( e^{\\sigma^2} - 1 \\), reflecting the variance of the log-normal distribution.\n",
    "\n",
    "### Simulated Results and Analysis\n",
    "\n",
    "1. For the classical Brownian motion (using the formula `P_t = P_{t-1} + r_t`):\n",
    "   - Mean price: Approximately 100.00 (Expected 100.00)\n",
    "   - Standard Deviation of the price: Approximately 0.10 (Expected 0.10)\n",
    "\n",
    "2. For the arithmetic return system (using the formula `P_t = P_{t-1} \\cdot (1 + r_t)`):\n",
    "   - Mean price: Approximately 99.82 (Expected 100.00)\n",
    "   - Standard Deviation of the price: Approximately 9.88 (Expected 10.00)\n",
    "\n",
    "3. For the log return (geometric Brownian motion using the formula `P_t = P_{t-1} \\cdot \\exp(r_t)`):\n",
    "   - Mean price: Approximately 100.30 (Expected 100.50)\n",
    "   - Standard Deviation of the price: Approximately 9.94 (Expected 10.03)\n",
    "\n",
    "As expected, the mean prices are around the starting price of 100, and the standard deviations reflect the volatility introduced by the returns `r_t` simulated from the normal distribution. The arithmetic and log return methods yield a higher standard deviation due to the multiplicative effect on the initial price, while the classical Brownian motion reflects only the standard deviation of the return since it's added directly to the initial price.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Codes for Expected Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classical Brownian Motion: Expected Mean = 100.00, SD = 0.10\n",
      "Arithmetic Return System: Expected Mean = 100.00, SD = 10.00\n",
      "Log Return (Geometric Brownian Motion): Expected Mean = 100.50, SD = 10.03\n"
     ]
    }
   ],
   "source": [
    "# Expected results\n",
    "\n",
    "# Set values\n",
    "m = 100  # The price at time t-1\n",
    "sigma = 0.1  # The standard deviation of returns\n",
    "\n",
    "# Classical Brownian Motion\n",
    "mean_classical_bm = m\n",
    "std_classical_bm = sigma\n",
    "\n",
    "# Arithmetic Return System\n",
    "mean_arithmetic_return = m\n",
    "std_arithmetic_return = m * sigma\n",
    "\n",
    "# Log Return or Geometric Brownian Motion\n",
    "mean_log_return = m * np.exp(sigma**2 / 2)\n",
    "std_log_return = m * np.sqrt(np.exp(sigma**2) - 1)\n",
    "\n",
    "result = (\n",
    "    f'Classical Brownian Motion: Expected Mean = {mean_classical_bm:.2f}, '\n",
    "    f'SD = {std_classical_bm:.2f}\\n'\n",
    "    f'Arithmetic Return System: Expected Mean = {mean_arithmetic_return:.2f}, '\n",
    "    f'SD = {std_arithmetic_return:.2f}\\n'\n",
    "    f'Log Return (Geometric Brownian Motion): Expected Mean = {mean_log_return:.2f}, '\n",
    "    f'SD = {std_log_return:.2f}'\n",
    ")\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Codes for Simulated Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classical Brownian Motion: Mean = 100.00, SD = 0.10\n",
      "Arithmetic Return System: Mean = 99.82, SD = 9.88\n",
      "Log Return (Geometric Brownian Motion): Mean = 100.30, SD = 9.94\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "# Define the number of simulations and the value of sigma\n",
    "num_simulations = 10000\n",
    "sigma = 0.1  # Assumed standard deviation of returns for simulation\n",
    "\n",
    "# Assume a starting price p_t_minus_1\n",
    "p_t_minus_1 = 100  # Assumed starting price for simulation\n",
    "\n",
    "# Generate random normal returns\n",
    "r_t = np.random.normal(0, sigma, num_simulations)\n",
    "\n",
    "# Calculate prices for each method\n",
    "prices_classical_bm = p_t_minus_1 + r_t\n",
    "prices_arithmetic_return = p_t_minus_1 * (1 + r_t)\n",
    "prices_log_return = p_t_minus_1 * np.exp(r_t)\n",
    "\n",
    "# Calculate mean and standard deviation for each method\n",
    "mean_classical_bm = np.mean(prices_classical_bm)\n",
    "std_classical_bm = np.std(prices_classical_bm)\n",
    "\n",
    "mean_arithmetic_return = np.mean(prices_arithmetic_return)\n",
    "std_arithmetic_return = np.std(prices_arithmetic_return)\n",
    "\n",
    "mean_log_return = np.mean(prices_log_return)\n",
    "std_log_return = np.std(prices_log_return)\n",
    "\n",
    "result = (\n",
    "    f'Classical Brownian Motion: Mean = {mean_classical_bm:.2f}, '\n",
    "    f'SD = {std_classical_bm:.2f}\\n'\n",
    "    f'Arithmetic Return System: Mean = {mean_arithmetic_return:.2f}, '\n",
    "    f'SD = {std_arithmetic_return:.2f}\\n'\n",
    "    f'Log Return (Geometric Brownian Motion): Mean = {mean_log_return:.2f}, '\n",
    "    f'SD = {std_log_return:.2f}'\n",
    ")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2: VaR\n",
    "Allowing the user to specify the method of return calculation, calculate the arithmetic returns of all prices in DailyPrices.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'MachAr'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/pj/nglxn63961qbm5z03b353k1c0000gn/T/ipykernel_23193/929059353.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtsa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mar_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoReg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/statsmodels/tsa/ar_model.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgaussian_kde\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mbase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtsa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPredictionResults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrapper\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mwrap\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/statsmodels/base/model.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m                                           cached_value, cached_data)\n\u001b[1;32m     15\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrapper\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mwrap\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumdiff\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mapprox_fprime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msm_exceptions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mValueWarning\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mHessianInversionWarning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/statsmodels/tools/numdiff.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;31m# NOTE: we only do double precision internally so far\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m \u001b[0mEPS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMachAr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m _hessian_docs = \"\"\"\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/numpy/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    331\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Tester was removed in NumPy 1.25.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 333\u001b[0;31m         raise AttributeError(\"module {!r} has no attribute \"\n\u001b[0m\u001b[1;32m    334\u001b[0m                              \"{!r}\".format(__name__, attr))\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'MachAr'"
     ]
    }
   ],
   "source": [
    "# I will now write the Python code to calculate the Value at Risk (VaR) for the META stock returns\n",
    "# at the 95% confidence level using the different methods specified.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import norm, t\n",
    "from statsmodels.tsa.ar_model import AutoReg\n",
    "import scipy\n",
    "\n",
    "# Load the data\n",
    "daily_prices_df = pd.read_csv('/Users/angelaliang/Documents/fintech545/Week04/DailyPrices.csv')\n",
    "daily_prices_df.set_index('Date', inplace=True)\n",
    "\n",
    "# Calculate arithmetic returns\n",
    "arithmetic_returns = daily_prices_df.pct_change().dropna()\n",
    "\n",
    "# Remove the mean from the META returns to have a mean of 0\n",
    "arithmetic_returns['META'] = arithmetic_returns['META'] - arithmetic_returns['META'].mean()\n",
    "\n",
    "# Confidence level\n",
    "confidence_level = 0.95\n",
    "alpha = 1 - confidence_level\n",
    "\n",
    "# VaR using a normal distribution\n",
    "mean = arithmetic_returns['META'].mean()\n",
    "std_dev = arithmetic_returns['META'].std()\n",
    "var_normal = norm.ppf(alpha, mean, std_dev)\n",
    "\n",
    "# VaR using a normal distribution with an Exponentially Weighted variance (lambda = 0.94)\n",
    "lambda_param = 0.94\n",
    "ewma_variance = arithmetic_returns['META'].ewm(alpha=(1 - lambda_param)).var().iloc[-1]\n",
    "var_normal_ewma = norm.ppf(alpha, 0, np.sqrt(ewma_variance))\n",
    "\n",
    "# VaR using a MLE fitted T distribution\n",
    "params = t.fit(arithmetic_returns['META'])\n",
    "var_t = t.ppf(alpha, *params)\n",
    "\n",
    "# VaR using a fitted AR(1) model\n",
    "ar_model = AutoReg(arithmetic_returns['META'], lags=1).fit()\n",
    "forecast = ar_model.predict(start=len(arithmetic_returns), end=len(arithmetic_returns))\n",
    "simulated_returns = np.random.normal(forecast, ar_model.resid.std(), 10000)\n",
    "var_ar1 = np.percentile(simulated_returns, alpha * 100)\n",
    "\n",
    "# VaR using a Historic Simulation\n",
    "var_historic = np.percentile(arithmetic_returns['META'], alpha * 100)\n",
    "var_results = {\n",
    "    \"Normal Distribution\": var_normal,\n",
    "    \"Normal EWMA\": var_normal_ewma,\n",
    "    \"MLE T Distribution\": var_t,\n",
    "    \"AR(1) Model\": var_ar1,\n",
    "    \"Historic Simulation\": var_historic\n",
    "}\n",
    "# Return all the VaR values calculated\n",
    "print(\"Normal Distribution: \" + var_normal)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach\n",
    "\n",
    "For this problem, I used an exponentially weighted moving covariance (EWMC) method to account for the fact that recent returns have more impact on the current risk profile than older returns. This is reflected in the choice of `LAMDA = 0.94`, which implies that recent observations have a higher weight in the calculation. I've used the covariance matrix to calculate the portfolio variance, which is then used to calculate the standard deviation and VaR. The results represent the maximum expected loss over a specified period at a 95% confidence level:\n",
    "\n",
    "* Portfolio A: VaR = `$15,206.39`\n",
    "* Portfolio B: VaR = `$7,741.25`\n",
    "* Portfolio C: VaR = `$17,877.73`\n",
    "* Total Portfolio VaR = `$40,825.38`\n",
    "\n",
    "### Choice of Alternative Model: Historical Simulation \n",
    "In the second part, I chose historical simulation because I believe actual historical return distribution is a better measure of risk than the assumption of a normal distribution as the data exhibits skewness after calculating the third moment. For example, NVDA shows a skewness of 1.602, which is quite high and indicates a distribution with a pronounced right tail, implying the presence of extreme positive returns. On the other hand, ZTS shows a slight negative skewness, implying a distribution with a longer left tail. These skewness metrics can inform the risk management process by highlighting the potential for asymmetric risk or extreme values that would not be captured by models assuming a normal distribution of returns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of Results \n",
    "\n",
    "In historical method, the actual historical returns are used to simulate potential future outcomes, and VaR is determined based on the percentile of these outcomes. The results are:\n",
    "\n",
    "Portfolio A: VaR = `$16,987.48`, \\\n",
    "Portfolio B: VaR = `$10,980.36`, \\\n",
    "Portfolio C: VaR = `$22,143.33`, \\\n",
    "Total Portfolio VaR = `$50,111.17`\n",
    "\n",
    "These values represent the maximum expected loss at a 95% confidence level, based on historical data. \n",
    "\n",
    "This result is different from that generated by EWMC because the latter assumes that recent returns are more indicative of future risk while the former assumes equal weighting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A': 15206.39096435521, 'B': 7741.250980957807, 'C': 17877.733059250822} 40825.375004563844\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "def calculate_ewm_cov(portfolio_df, daily_returns_df, lambda_value):\n",
    "    ewm_cov_by_portfolio = {}\n",
    "    span = (2 / (1 - lambda_value)) - 1\n",
    "\n",
    "    for portfolio_label in portfolio_df['Portfolio'].unique():\n",
    "        stocks = portfolio_df[portfolio_df['Portfolio'] == portfolio_label]['Stock']\n",
    "        portfolio_returns = daily_returns_df[stocks]\n",
    "        ewm_cov = portfolio_returns.ewm(span=span).cov()\n",
    "        last_day_cov_matrix = ewm_cov.loc[daily_returns_df.index[-1]]\n",
    "        ewm_cov_by_portfolio[portfolio_label] = last_day_cov_matrix\n",
    "    \n",
    "    return ewm_cov_by_portfolio\n",
    "\n",
    "def calculate_var(ewm_cov_by_portfolio, portfolio_df, daily_prices_df, z_score):\n",
    "    portfolio_var = {}\n",
    "    last_day_prices = daily_prices_df.iloc[-1]\n",
    "\n",
    "    for portfolio_label in portfolio_df['Portfolio'].unique():\n",
    "        holdings = portfolio_df[portfolio_df['Portfolio'] == portfolio_label]\n",
    "        last_day_cov_matrix = ewm_cov_by_portfolio[portfolio_label]\n",
    "\n",
    "        # Calculate the dollar value of each holding\n",
    "        holdings_values = holdings.set_index('Stock')['Holding'] * last_day_prices.reindex(holdings['Stock']).fillna(0)\n",
    "\n",
    "        # Calculate the total dollar value of the portfolio\n",
    "        portfolio_value = np.sum(holdings_values)\n",
    "\n",
    "        # Calculate the proportion (weight) of each holding in the portfolio\n",
    "        delta = holdings_values / portfolio_value\n",
    "\n",
    "        # Calculate the portfolio variance using the weights (proportions)\n",
    "        portfolio_variance = delta.T @ last_day_cov_matrix @ delta\n",
    "        portfolio_std = np.sqrt(portfolio_variance)\n",
    "\n",
    "        # Calculate VaR at 95% confidence level\n",
    "        VaR_dollar = z_score * portfolio_std * portfolio_value\n",
    "        portfolio_var[portfolio_label] = VaR_dollar\n",
    "\n",
    "    return portfolio_var\n",
    "\n",
    "\n",
    "# Constants\n",
    "LAMBDA = 0.94\n",
    "Z_SCORE = norm.ppf(1 - 0.05)\n",
    "\n",
    "# File paths (consider using relative paths or arguments)\n",
    "portfolio_path = '/Users/angelaliang/Documents/fintech545/Week04/Project/portfolio.csv'\n",
    "daily_prices_path = '/Users/angelaliang/Documents/fintech545/Week04/DailyPrices.csv'\n",
    "\n",
    "# Read the data\n",
    "portfolio_df = pd.read_csv(portfolio_path)\n",
    "daily_prices_df = pd.read_csv(daily_prices_path)\n",
    "daily_prices_df.set_index('Date', inplace=True)\n",
    "daily_returns_df = daily_prices_df.pct_change().dropna()\n",
    "\n",
    "# Calculate exponentially weighted covariance matrix\n",
    "ewm_cov_by_portfolio = calculate_ewm_cov(portfolio_df, daily_returns_df, LAMBDA)\n",
    "\n",
    "# Calculate VaR for each portfolio\n",
    "portfolio_var = calculate_var(ewm_cov_by_portfolio, portfolio_df, daily_prices_df, Z_SCORE)\n",
    "\n",
    "# Output the VaR as a dictionary\n",
    "print(portfolio_var, sum(portfolio_var.values()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check Skewness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SPY     0.358500\n",
       "AAPL    0.416325\n",
       "MSFT    0.328436\n",
       "AMZN    0.415782\n",
       "NVDA    1.602140\n",
       "          ...   \n",
       "LRCX    0.463317\n",
       "ZTS    -0.192214\n",
       "C       0.123583\n",
       "BSX     0.540970\n",
       "AMT     0.501227\n",
       "Length: 101, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import skew\n",
    "\n",
    "# Calculate the skewness for each stock in the daily returns\n",
    "skewness = daily_returns_df.skew()\n",
    "\n",
    "skewness\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Historical Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'A': -16987.47846706814, 'B': -10980.358676761549, 'C': -22143.334643946997},\n",
       " -50111.171787776686)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_historical_var(holdings_values, historical_returns, confidence_level):\n",
    "    # Calculate portfolio historical returns\n",
    "    portfolio_historical_returns = (historical_returns * holdings_values).sum(axis=1)\n",
    "    \n",
    "    # Calculate the VaR as the percentile of the historical returns\n",
    "    VaR = np.percentile(portfolio_historical_returns, (1 - confidence_level) * 100)\n",
    "    return VaR\n",
    "\n",
    "def calculate_historical_var_by_portfolio(portfolio_df, daily_prices_df, confidence_level):\n",
    "    historical_returns = daily_prices_df.pct_change().dropna()\n",
    "    last_day_prices = daily_prices_df.iloc[-1]\n",
    "    portfolio_var = {}\n",
    "\n",
    "    for portfolio_label in portfolio_df['Portfolio'].unique():\n",
    "        holdings = portfolio_df[portfolio_df['Portfolio'] == portfolio_label]\n",
    "        \n",
    "        # Calculate the dollar value of each holding\n",
    "        holdings_values = holdings.set_index('Stock')['Holding'] * last_day_prices.reindex(holdings['Stock']).fillna(0)\n",
    "\n",
    "        # Calculate historical VaR\n",
    "        VaR_dollar = calculate_historical_var(holdings_values, historical_returns, confidence_level)\n",
    "        portfolio_var[portfolio_label] = VaR_dollar\n",
    "\n",
    "    return portfolio_var\n",
    "\n",
    "# Constants\n",
    "CONFIDENCE_LEVEL = 0.95\n",
    "\n",
    "# File paths (consider using relative paths or arguments)\n",
    "portfolio_path = '/Users/angelaliang/Documents/fintech545/Week04/Project/portfolio.csv'\n",
    "daily_prices_path = '/Users/angelaliang/Documents/fintech545/Week04/DailyPrices.csv'\n",
    "\n",
    "# Read the data\n",
    "portfolio_df = pd.read_csv(portfolio_path)\n",
    "daily_prices_df = pd.read_csv(daily_prices_path)\n",
    "daily_prices_df.set_index('Date', inplace=True)\n",
    "\n",
    "# Calculate historical VaR for each portfolio\n",
    "portfolio_var = calculate_historical_var_by_portfolio(portfolio_df, daily_prices_df, CONFIDENCE_LEVEL)\n",
    "\n",
    "# Output the VaR as a dictionary\n",
    "portfolio_var, sum(portfolio_var.values())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
