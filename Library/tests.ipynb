{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary packages\n",
    "import os\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from scipy.stats import norm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import all test files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = pd.read_csv(\"/Users/angelaliang/Documents/fintech545/testfiles/data/test1.csv\")\n",
    "testout1_1 = pd.read_csv(\"/Users/angelaliang/Documents/fintech545/testfiles/data/testout_1.1.csv\")\n",
    "testout1_2 =pd.read_csv(\"/Users/angelaliang/Documents/fintech545/testfiles/data/testout_1.2.csv\")\n",
    "testout1_3 = pd.read_csv(\"/Users/angelaliang/Documents/fintech545/testfiles/data/testout_1.3.csv\")\n",
    "testout1_4 = pd.read_csv(\"/Users/angelaliang/Documents/fintech545/testfiles/data/testout_1.4.csv\")\n",
    "\n",
    "test2 = pd.read_csv(\"/Users/angelaliang/Documents/fintech545/testfiles/data/test2.csv\")\n",
    "testout2_1 = pd.read_csv(\"/Users/angelaliang/Documents/fintech545/testfiles/data/testout_2.1.csv\")\n",
    "testout2_2 = pd.read_csv(\"/Users/angelaliang/Documents/fintech545/testfiles/data/testout_2.2.csv\")\n",
    "testout2_3 = pd.read_csv(\"/Users/angelaliang/Documents/fintech545/testfiles/data/testout_2.3.csv\")\n",
    "\n",
    "testout3_1 = pd.read_csv(\"/Users/angelaliang/Documents/fintech545/testfiles/data/testout_3.1.csv\")\n",
    "testout3_2 = pd.read_csv(\"/Users/angelaliang/Documents/fintech545/testfiles/data/testout_3.2.csv\")\n",
    "testout3_3 = pd.read_csv(\"/Users/angelaliang/Documents/fintech545/testfiles/data/testout_3.3.csv\")\n",
    "testout3_4 = pd.read_csv(\"/Users/angelaliang/Documents/fintech545/testfiles/data/testout_3.4.csv\")\n",
    "\n",
    "testout4_1 = pd.read_csv(\"/Users/angelaliang/Documents/fintech545/testfiles/data/testout_4.1.csv\")\n",
    "\n",
    "test5_1 = pd.read_csv(\"/Users/angelaliang/Documents/fintech545/testfiles/data/test5_1.csv\")\n",
    "test5_2 = pd.read_csv(\"/Users/angelaliang/Documents/fintech545/testfiles/data/test5_2.csv\")\n",
    "test5_3 = pd.read_csv(\"/Users/angelaliang/Documents/fintech545/testfiles/data/test5_3.csv\")\n",
    "testout5_1 = pd.read_csv(\"/Users/angelaliang/Documents/fintech545/testfiles/data/testout_5.1.csv\")\n",
    "testout5_2 = pd.read_csv(\"/Users/angelaliang/Documents/fintech545/testfiles/data/testout_5.2.csv\")\n",
    "testout5_3 = pd.read_csv(\"/Users/angelaliang/Documents/fintech545/testfiles/data/testout_5.3.csv\")\n",
    "testout5_4 = pd.read_csv(\"/Users/angelaliang/Documents/fintech545/testfiles/data/testout_5.4.csv\")\n",
    "testout5_5 = pd.read_csv(\"/Users/angelaliang/Documents/fintech545/testfiles/data/testout_5.5.csv\")\n",
    "\n",
    "test6 = pd.read_csv(\"/Users/angelaliang/Documents/fintech545/testfiles/data/test6.csv\")\n",
    "testout6_1 = pd.read_csv(\"/Users/angelaliang/Documents/fintech545/testfiles/data/test6_1.csv\")\n",
    "testout6_2 = pd.read_csv(\"/Users/angelaliang/Documents/fintech545/testfiles/data/test6_2.csv\")\n",
    "\n",
    "test7_1 = pd.read_csv(\"/Users/angelaliang/Documents/fintech545/testfiles/data/test7_1.csv\")\n",
    "test7_2 = pd.read_csv(\"/Users/angelaliang/Documents/fintech545/testfiles/data/test7_2.csv\")\n",
    "test7_3 = pd.read_csv(\"/Users/angelaliang/Documents/fintech545/testfiles/data/test7_3.csv\")\n",
    "testout7_1 = pd.read_csv(\"/Users/angelaliang/Documents/fintech545/testfiles/data/testout7_1.csv\")\n",
    "testout7_2 = pd.read_csv(\"/Users/angelaliang/Documents/fintech545/testfiles/data/testout7_2.csv\")\n",
    "testout7_3 = pd.read_csv(\"/Users/angelaliang/Documents/fintech545/testfiles/data/testout7_3.csv\")\n",
    "\n",
    "testout8_1 = pd.read_csv(\"/Users/angelaliang/Documents/fintech545/testfiles/data/testout8_1.csv\")\n",
    "testout8_2 = pd.read_csv(\"/Users/angelaliang/Documents/fintech545/testfiles/data/testout8_2.csv\")\n",
    "testout8_3 = pd.read_csv(\"/Users/angelaliang/Documents/fintech545/testfiles/data/testout8_3.csv\")\n",
    "testout8_4 = pd.read_csv(\"/Users/angelaliang/Documents/fintech545/testfiles/data/testout8_4.csv\")\n",
    "testout8_5 = pd.read_csv(\"/Users/angelaliang/Documents/fintech545/testfiles/data/testout8_5.csv\")\n",
    "testout8_6 = pd.read_csv(\"/Users/angelaliang/Documents/fintech545/testfiles/data/testout8_6.csv\")\n",
    "\n",
    "\n",
    "test9_1_portfolio = pd.read_csv(\"/Users/angelaliang/Documents/fintech545/testfiles/data/test9_1_portfolio.csv\")\n",
    "test9_1_returns = pd.read_csv(\"/Users/angelaliang/Documents/fintech545/testfiles/data/test9_1_returns.csv\")\n",
    "testout9_1= pd.read_csv(\"/Users/angelaliang/Documents/fintech545/testfiles/data/testout9_1.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read path name for all locally saved files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_name = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test File Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check if two files are equal with small tolerance set \n",
    "# test and see if results are the same \n",
    "def testfiles_equal(df1, df2):\n",
    "    if df1.equals(df2):\n",
    "        return True\n",
    "    else:\n",
    "        # If they are not equal, it could be due to floating point precision,\n",
    "        # so we attempt a comparison that allows for a small absolute difference.\n",
    "        try:\n",
    "            pd.testing.assert_frame_equal(df1, df2, atol=1e-2)  # Use an appropriate tolerance\n",
    "            return True\n",
    "        except AssertionError:\n",
    "            return False\n",
    "        \n",
    "# Function to check if two files are equal with small tolerance set \n",
    "# test and see if results are the same \n",
    "def testfiles_equal_largeTol(df1, df2):\n",
    "    if df1.equals(df2):\n",
    "        return True\n",
    "    else:\n",
    "        # If they are not equal, it could be due to floating point precision,\n",
    "        # so we attempt a comparison that allows for a small absolute difference.\n",
    "        try:\n",
    "            pd.testing.assert_frame_equal(df1, df2, atol=1)  # Use an appropriate tolerance\n",
    "            return True\n",
    "        except AssertionError:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Covariance estimation techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With missing data, calculate correlation and covariance using methods:\n",
    "1) skip missing rows\n",
    "2) pairwise calculation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_correlation_covariance(df, method, calc_type, output_name):\n",
    "    \n",
    "    # Dropping all rows with any missing values if method is 'dropmissing'\n",
    "      if method == 'dropmissing':\n",
    "            df_cleaned = df.dropna()\n",
    "            if calc_type=='correlation':\n",
    "                  result = df_cleaned.corr(method='pearson')\n",
    "            if calc_type=='covariance':\n",
    "                  result = df_cleaned.cov(min_periods=1)\n",
    "\n",
    "\n",
    "      elif method == 'pairwise':\n",
    "            if calc_type=='correlation':\n",
    "                  result = df.corr(method='pearson')\n",
    "            if calc_type=='covariance':\n",
    "                  result = df.cov(min_periods=1)\n",
    "                                    \n",
    "                  \n",
    "      result_reset = result.reset_index(drop=True) # reset row index, drop and set as header\n",
    "      result_reset.to_csv(output_name+\".csv\", index=False, header=True)\n",
    "      print(f\"{calc_type} saved to {output_name}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "covariance saved to testresult_1.1\n",
      "True\n",
      "correlation saved to testresult_1.2\n",
      "True\n",
      "covariance saved to testresult_1.3\n",
      "True\n",
      "correlation saved to testresult_1.4\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# testing \n",
    "df = test1 # same for the first 4 tests \n",
    "\n",
    "# test 1.1 \n",
    "method = 'dropmissing'\n",
    "calc_type = 'covariance'\n",
    "output_name = 'testresult_1.1'\n",
    "calculate_correlation_covariance(df, method, calc_type, output_name)\n",
    "# testresult_1_1 = pd.read_csv(path_name + \"/\"+ output_name+\".csv\", header=None, index_col=0)\n",
    "testresult_1_1 = pd.read_csv(path_name + \"/\"+ output_name+\".csv\")  # header is inferred by default\n",
    "testresult = testresult_1_1\n",
    "correctanswer =  testout1_1\n",
    "print(testfiles_equal(testresult,correctanswer))\n",
    "\n",
    "# test 1.2 \n",
    "method = 'dropmissing'\n",
    "calc_type = 'correlation'\n",
    "output_name = 'testresult_1.2'\n",
    "calculate_correlation_covariance(df, method, calc_type, output_name)\n",
    "# testresult_1_2 = pd.read_csv(path_name + \"/\"+ output_name+\".csv\", header=None, index_col=0)\n",
    "testresult_1_2 = pd.read_csv(path_name + \"/\"+ output_name+\".csv\")\n",
    "testresult = testresult_1_2\n",
    "correctanswer =  testout1_2\n",
    "print(testfiles_equal(testresult,correctanswer))\n",
    "\n",
    "# test 1.3\n",
    "method = 'pairwise'\n",
    "calc_type = 'covariance'\n",
    "output_name = 'testresult_1.3'\n",
    "calculate_correlation_covariance(df, method, calc_type, output_name)\n",
    "# testresult_1_3 = pd.read_csv(path_name + \"/\"+ output_name+\".csv\", header=None, index_col=0)\n",
    "testresult_1_3 = pd.read_csv(path_name + \"/\"+ output_name+\".csv\")\n",
    "testresult = testresult_1_3\n",
    "correctanswer =  testout1_3\n",
    "print(testfiles_equal(testresult,correctanswer))\n",
    "\n",
    "# test 1.4\n",
    "method = 'pairwise'\n",
    "calc_type = 'correlation'\n",
    "output_name = 'testresult_1.4'\n",
    "calculate_correlation_covariance(df, method, calc_type, output_name)\n",
    "# testresult_1_4 = pd.read_csv(path_name + \"/\"+ output_name+\".csv\", header=None, index_col=0)\n",
    "testresult_1_4 = pd.read_csv(path_name + \"/\"+ output_name+\".csv\")\n",
    "testresult = testresult_1_4\n",
    "correctanswer =  testout1_4\n",
    "print(testfiles_equal(testresult,correctanswer))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. EW Covariance and Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_ew_correlation_covariance(df, calc_type, output_name, lambda_param):\n",
    "      \n",
    "      if lambda_param is None:\n",
    "            raise ValueError(\"For 'ew' method, lambda_param must be provided.\")\n",
    "      \n",
    "      n = len(df)\n",
    "      weights = np.array([(1 - lambda_param) * (lambda_param ** (n - 1 - i)) for i in range(n)])\n",
    "      weights /= weights.sum()\n",
    "      demeaned_data = df - df.mean()\n",
    "      \n",
    "      # Compute the exponentially weighted covariance matrix\n",
    "      ew_cov_matrix = (demeaned_data.T * weights).dot(demeaned_data) / weights.sum()\n",
    "      \n",
    "      if calc_type == 'correlation':\n",
    "            # Calculate the exponentially weighted correlation matrix\n",
    "            # Standard deviations are computed with the same weights used for covariance\n",
    "            std_dev = np.sqrt(np.diag(ew_cov_matrix))\n",
    "            # To ensure the standard deviations are non-zero to prevent division by zero errors\n",
    "            std_dev[std_dev == 0] = np.finfo(np.float64).tiny\n",
    "            ew_corr_matrix = ew_cov_matrix / np.outer(std_dev, std_dev)\n",
    "            result = ew_corr_matrix\n",
    "\n",
    "      elif calc_type == 'covariance':\n",
    "            result = ew_cov_matrix\n",
    "\n",
    "      # Reset index to ensure a clean DataFrame, suitable for CSV output\n",
    "      result_reset = result.reset_index(drop=True)\n",
    "      # Save to CSV\n",
    "      result_reset.to_csv(output_name + \".csv\", index=False, header=True)\n",
    "      print(f\"{calc_type} saved to {output_name}.csv\")\n",
    "\n",
    "\n",
    "def calculate_ew_sd(df, output_name, lambda_param1,lambda_param2):\n",
    "      # Inside your calculate_correlation_covariance function\n",
    "      if lambda_param1 is None or lambda_param2 is None: \n",
    "            raise ValueError(\"For 'ew' method, lambda_param must be provided for both covariance and correlation.\")\n",
    "      n = len(df)\n",
    "      weights1 = np.array([(1 - lambda_param1) * (lambda_param1 ** (n - 1 - i)) for i in range(n)])\n",
    "      weights1 /= weights1.sum()\n",
    "      demeaned_data1 = df - df.mean()\n",
    "      ew_cov_matrix1 = (demeaned_data1.T * weights1).dot(demeaned_data1) / weights1.sum()\n",
    "      std_dev1 = np.sqrt(np.diag(ew_cov_matrix1)) # 0.97 \n",
    "      \n",
    "      weights2 = np.array([(1 - lambda_param2) * (lambda_param2 ** (n - 1 - i)) for i in range(n)])\n",
    "      weights2 /= weights2.sum()\n",
    "      demeaned_data2 = df - df.mean()\n",
    "      ew_cov_matrix2 = (demeaned_data2.T * weights2).dot(demeaned_data2) / weights2.sum()\n",
    "      std_dev2 = np.sqrt(np.diag(ew_cov_matrix2)) # 0.94 \n",
    "\n",
    "      # if calc_type == 'correlation':\n",
    "      # Use the covariance matrix to calculate correlation matrix\n",
    "      # You may need to adjust the following lines if your DataFrame structure is different\n",
    "      \n",
    "      ew_corr = np.diag(1/std_dev2) @ ew_cov_matrix2 @ np.diag(1/std_dev2)\n",
    "      ew_cov = np.diag(std_dev1) @ ew_corr @ np.diag(std_dev1)\n",
    "      ew_cov.columns = df.columns\n",
    "      result = ew_cov\n",
    "      # Reset index to ensure a clean DataFrame, suitable for CSV output\n",
    "      result_reset = result.reset_index(drop=True)\n",
    "      # result_reset= pd.DataFrame(np.diag(std_dev1) @ ew_corr @ np.diag(std_dev1), columns=df.columns)\n",
    "      # Save to CSV\n",
    "      # print(result_reset)\n",
    "      result_reset.to_csv(output_name + \".csv\", index=False)\n",
    "      print(f\"{calc_type} saved to {output_name}.csv\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "covariance saved to testresult_2.1.csv\n",
      "True\n",
      "correlation saved to testresult_2.2.csv\n",
      "True\n",
      "correlation saved to testresult_2.3.csv\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = test2\n",
    "# method = 'ew'\n",
    "\n",
    "# test 2.1\n",
    "calc_type = 'covariance'\n",
    "output_name = 'testresult_2.1'\n",
    "lambda_val = 0.97\n",
    "calculate_ew_correlation_covariance(df, calc_type, output_name, lambda_param=lambda_val)\n",
    "testresult_2_1 = pd.read_csv(path_name + \"/\"+ output_name+\".csv\")\n",
    "testresult = testresult_2_1\n",
    "correctanswer =  testout2_1\n",
    "print(testfiles_equal(testresult,correctanswer))\n",
    "\n",
    "\n",
    "# test 2.2\n",
    "calc_type = 'correlation'\n",
    "output_name = 'testresult_2.2'\n",
    "lambda_val = 0.94\n",
    "calculate_ew_correlation_covariance(df, calc_type, output_name, lambda_param=lambda_val)\n",
    "testresult_2_2 = pd.read_csv(path_name + \"/\"+ output_name+\".csv\")\n",
    "testresult = testresult_2_2\n",
    "correctanswer =  testout2_2\n",
    "print(testfiles_equal(testresult,correctanswer))\n",
    "\n",
    "# test 2.3 \n",
    "calc_type = 'correlation'\n",
    "output_name = 'testresult_2.3'\n",
    "lambda_val1 = 0.97\n",
    "lambda_val2 = 0.94\n",
    "calculate_ew_sd(df, output_name, lambda_param1=lambda_val1,lambda_param2=lambda_val2)\n",
    "testresult_2_3 = pd.read_csv(path_name + \"/\"+ output_name+\".csv\")\n",
    "testresult = testresult_2_3\n",
    "correctanswer =  testout2_3\n",
    "\n",
    "print(testfiles_equal(testresult,correctanswer))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Near_psd and Higham Covariance and Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correlation saved to testresult_3_1.csv\n",
      "True\n",
      "correlation saved to testresult_3_2.csv\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import eig, inv\n",
    "\n",
    "def near_psd(a, method, epsilon=0.0):\n",
    "    n = a.shape[0]\n",
    "    invSD = None\n",
    "    out = np.copy(a)\n",
    "\n",
    "    # calculate the correlation matrix if we got a covariance\n",
    "    if not np.allclose(np.diag(out), 1.0):\n",
    "        invSD = np.diag(1.0 / np.sqrt(np.diag(out)))\n",
    "        out = invSD @ out @ invSD\n",
    "        # print(\"correlation\", out)\n",
    "    if method == 'correlation':\n",
    "        out = out \n",
    "    elif method == 'covariance':\n",
    "        # SVD, update the eigenvalue and scale\n",
    "        vals, vecs = eig(out)\n",
    "        vals = np.maximum(vals, epsilon)\n",
    "        T = np.diag(1.0 / (vecs * vecs @ vals))\n",
    "        T = np.diag(np.sqrt(np.diag(T)))\n",
    "        l = np.diag(np.sqrt(vals))\n",
    "        B = T @ vecs @ l\n",
    "        out = B @ B.T\n",
    "\n",
    "        # Add back the variance\n",
    "        if invSD is not None:\n",
    "            invSD = np.diag(1.0 / np.diag(invSD))\n",
    "            out = invSD @ out @ invSD\n",
    "            # print(\"covariance\", out)\n",
    "\n",
    "\n",
    "        \n",
    "    result = pd.DataFrame(out, columns=testout3_1.columns)\n",
    "        # # Reset index to ensure a clean DataFrame, suitable for CSV output\n",
    "        # # print(result)\n",
    "        # result_reset = result.reset_index(drop=True)\n",
    "        # Save to CSV\n",
    "    return result    \n",
    "    \n",
    "\n",
    "# test 3.1 \n",
    "output_name = 'testresult_3_1'\n",
    "method = 'covariance'\n",
    "result = near_psd(testout1_3, method, epsilon=0.0)\n",
    "result.to_csv(output_name + \".csv\", index=False, header=True)\n",
    "print(f\"{calc_type} saved to {output_name}.csv\")\n",
    "testresult_3_1 = pd.read_csv(path_name + \"/\"+ output_name+\".csv\")\n",
    "testresult = testresult_3_1\n",
    "correctanswer =  testout3_1\n",
    "# print(\"Test OUT: \",testout3_1)\n",
    "print(testfiles_equal(testresult,correctanswer))\n",
    "\n",
    "# test 3.2 \n",
    "output_name = 'testresult_3_2'\n",
    "method = 'correlation'\n",
    "result = near_psd(testout1_4, method, epsilon=0.0)\n",
    "result.to_csv(output_name + \".csv\", index=False, header=True)\n",
    "print(f\"{calc_type} saved to {output_name}.csv\")\n",
    "testresult_3_2 = pd.read_csv(path_name + \"/\"+ output_name+\".csv\")\n",
    "testresult = testresult_3_2\n",
    "correctanswer =  testout3_2\n",
    "# print(\"Test OUT: \",testout3_1)\n",
    "print(testfiles_equal(testresult,correctanswer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Higham Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged in 38 iterations.\n",
      "correlation saved to testresult_3_3.csv\n",
      "True\n",
      "correlation saved to testresult_3_4.csv\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.linalg import sqrtm\n",
    "\n",
    "def _getAplus(A):\n",
    "    eigval, eigvec = np.linalg.eigh(A)\n",
    "    Q = np.diag(np.maximum(eigval, 0))\n",
    "    return eigvec @ Q @ eigvec.T\n",
    "\n",
    "def _getPS(A, W):\n",
    "    W05 = sqrtm(W)\n",
    "    iW = inv(W05)\n",
    "    return iW @ _getAplus(W05 @ A @ W05) @ iW\n",
    "\n",
    "def _getPu(A, W):\n",
    "    Aret = np.copy(A)\n",
    "    np.fill_diagonal(Aret, 1)\n",
    "    return Aret\n",
    "\n",
    "def wgtNorm(A, W):\n",
    "    W05 = sqrtm(W)\n",
    "    return np.sum((W05 @ A @ W05) ** 2)\n",
    "\n",
    "def higham_nearestPSD(pc, method, epsilon=1e-9, maxIter=100, tol=1e-9):\n",
    "    n = pc.shape[0]\n",
    "    W = np.diag(np.ones(n))\n",
    "    deltaS = 0\n",
    "    invSD = None\n",
    "\n",
    "    Yk = np.copy(pc)\n",
    "    \n",
    "    # calculate the correlation matrix if we got a covariance\n",
    "    if not np.allclose(np.diag(Yk), 1.0):\n",
    "        invSD = np.diag(1.0 / np.sqrt(np.diag(Yk)))\n",
    "        Yk = invSD @ Yk @ invSD\n",
    "    if method == 'correlation':\n",
    "        result = Yk\n",
    "    \n",
    "    elif method =='covariance':     \n",
    "        Yo = np.copy(Yk)\n",
    "        \n",
    "        norml = np.finfo(np.float64).max\n",
    "        i = 1\n",
    "\n",
    "        while i <= maxIter:\n",
    "            Rk = Yk - deltaS\n",
    "            Xk = _getPS(Rk, W)\n",
    "            deltaS = Xk - Rk\n",
    "            Yk = _getPu(Xk, W)\n",
    "            norm = wgtNorm(Yk - Yo, W)\n",
    "            minEigVal = np.min(np.linalg.eigvalsh(Yk))\n",
    "\n",
    "            if norm - norml < tol and minEigVal > -epsilon:\n",
    "                break\n",
    "            \n",
    "            norml = norm\n",
    "            i += 1\n",
    "\n",
    "        if i < maxIter:\n",
    "            print(\"Converged in {} iterations.\".format(i))\n",
    "        else:\n",
    "            print(\"Convergence failed after {} iterations\".format(i - 1))\n",
    "\n",
    "        # Add back the variance\n",
    "        if invSD is not None:\n",
    "            invSD = np.diag(1.0 / np.diag(invSD))\n",
    "            Yk = invSD @ Yk @ invSD\n",
    "        result = Yk\n",
    "    # set to dataframe \n",
    "    result = pd.DataFrame(Yk, columns=testout1_4.columns)\n",
    "    return result \n",
    "\n",
    "# test 3.3 \n",
    "output_name = 'testresult_3_3'\n",
    "method = 'covariance'\n",
    "result = higham_nearestPSD(testout1_3, method, epsilon=0.0)\n",
    "result.to_csv(output_name + \".csv\", index=False, header=True)\n",
    "print(f\"{calc_type} saved to {output_name}.csv\")\n",
    "testresult_3_3 = pd.read_csv(path_name + \"/\"+ output_name+\".csv\")\n",
    "testresult = testresult_3_3\n",
    "correctanswer =  testout3_3\n",
    "print(testfiles_equal(testresult,correctanswer))\n",
    "\n",
    "# test 3.4\n",
    "output_name = 'testresult_3_4'\n",
    "method = 'correlation'\n",
    "result = higham_nearestPSD(testout1_4, method, epsilon=0.0)\n",
    "result.to_csv(output_name + \".csv\", index=False, header=True)\n",
    "print(f\"{calc_type} saved to {output_name}.csv\")\n",
    "testresult_3_4 = pd.read_csv(path_name + \"/\"+ output_name+\".csv\")\n",
    "testresult = testresult_3_4\n",
    "correctanswer =  testout3_4\n",
    "print(testfiles_equal(testresult,correctanswer))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Cholesky decomposition for Positive Semi-Definite matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correlation saved to testresult_4_1.csv\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "def chol_psd(df):\n",
    "    df = df.to_numpy()\n",
    "    root = np.zeros_like(df)\n",
    "    n = df.shape[0]\n",
    "    # Initialize the root matrix with 0 values\n",
    "    root[:] = 0.0\n",
    "\n",
    "    # Loop over columns\n",
    "    for j in range(n):\n",
    "        s = 0.0\n",
    "        # If we are not on the first column, calculate the dot product of the preceding row values.\n",
    "        if j > 0:\n",
    "            s = root[j, :j] @ root[j, :j]\n",
    "\n",
    "        # Diagonal Element\n",
    "        temp = df[j, j] - s\n",
    "        if 0 >= temp >= -1e-8:\n",
    "            temp = 0.0\n",
    "        root[j, j] = np.sqrt(temp)\n",
    "\n",
    "        # Check for the 0 eigenvalue. Just set the column to 0 if we have one\n",
    "        if root[j, j] == 0.0:\n",
    "            root[j, j+1:n] = 0.0\n",
    "        else:\n",
    "            # Update off-diagonal rows of the column\n",
    "            ir = 1.0 / root[j, j]\n",
    "            for i in range(j+1, n):\n",
    "                s = root[i, :j] @ root[j, :j]\n",
    "                root[i, j] = (df[i, j] - s) * ir\n",
    "    result = pd.DataFrame(root, columns=testout3_1.columns)\n",
    "    return result \n",
    "\n",
    "# test 4.1 \n",
    "output_name = 'testresult_4_1'\n",
    "df = testout3_1\n",
    "result = chol_psd(df)\n",
    "result.to_csv(output_name + \".csv\", index=False, header=True)\n",
    "print(f\"{calc_type} saved to {output_name}.csv\")\n",
    "# 'root' now contains the Cholesky decomposition of 'a'\n",
    "testresult_4_1 = pd.read_csv(path_name + \"/\"+ output_name+\".csv\")\n",
    "testresult = testresult_4_1\n",
    "correctanswer =  testout4_1\n",
    "print(testfiles_equal(testresult,correctanswer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Normal Simulation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correlation saved to testresult_5_1.csv\n",
      "True\n",
      "correlation saved to testresult_5_2.csv\n",
      "True\n",
      "correlation saved to testresult_5_3.csv\n",
      "True\n",
      "Converged in 16 iterations.\n",
      "correlation saved to testresult_5_4.csv\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from scipy.stats import norm\n",
    "\n",
    "def simulate_normal(N, output_name, cov, fix_method=None, mean=None, seed=1234):\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Error Checking\n",
    "    n, m = cov.shape\n",
    "    if n != m:\n",
    "        raise ValueError(f\"Covariance Matrix is not square ({n},{m})\")\n",
    "    \n",
    "    out = np.empty((n, N))\n",
    "    \n",
    "    # If the mean is missing then set to 0, otherwise use provided mean\n",
    "    if mean is None:\n",
    "        _mean = np.zeros(n)\n",
    "    else:\n",
    "        if n != len(mean):\n",
    "            raise ValueError(f\"Mean ({len(mean)}) is not the size of cov ({n},{n})\")\n",
    "        _mean = np.array(mean)\n",
    "    new_cov = pd.DataFrame(cov) # convert to dataframe because my previous functions all take in dataframes \n",
    "    # Take the root\n",
    "    if fix_method == 'near_psd':\n",
    "        l = near_psd(new_cov, 'covariance') # feed in a dataframe, still. \n",
    "        # l = np.linalg.cholesky(l)\n",
    "        l = chol_psd(l).to_numpy() # convert back to array after function outputing. \n",
    "    elif fix_method == 'Higham':\n",
    "        l = higham_nearestPSD(new_cov, 'covariance')\n",
    "        l = chol_psd(l).to_numpy() # convert back to array after function outputing. \n",
    "    elif fix_method == 'cholesky':\n",
    "        l = chol_psd(new_cov)\n",
    "        l = np.tril(l.to_numpy())\n",
    "    else: \n",
    "        l = new_cov\n",
    "    \n",
    "   \n",
    "    # Generate needed random standard normals\n",
    "    d = norm.rvs(size=(N, n))\n",
    "    \n",
    "    # Apply the standard normals to the Cholesky root\n",
    "    out = l @ d.T\n",
    "\n",
    "    # Loop over iterations and add the mean\n",
    "    for i in range(n):\n",
    "        out[i, :] += _mean[i]\n",
    "    simulated_data = out.T\n",
    "    result = np.cov(simulated_data.T)\n",
    " \n",
    "    # result = np.cov(out)\n",
    "    \n",
    "    result = pd.DataFrame(result, columns=testout5_1.columns)\n",
    "    result.to_csv(output_name + \".csv\", index=False, header=True)\n",
    "    print(f\"{calc_type} saved to {output_name}.csv\")      \n",
    "\n",
    "# test 5.1  \n",
    "output_name = 'testresult_5_1'\n",
    "cov_matrix = test5_1.values \n",
    "# mean_vector = np.zeros(cov_matrix.shape[0])\n",
    "N=100000\n",
    "fix_method = 'cholesky'\n",
    "#Simulate 100,000 normal variables\n",
    "simulate_normal(N, output_name, cov_matrix, fix_method)\n",
    "testresult_5_1 = pd.read_csv(path_name + \"/\"+ output_name+\".csv\")\n",
    "testresult = testresult_5_1\n",
    "correctanswer =  testout5_1\n",
    "print(testfiles_equal(testresult,correctanswer))\n",
    "\n",
    "# test 5.2\n",
    "output_name = 'testresult_5_2'\n",
    "cov_matrix = test5_2.values \n",
    "# mean_vector = np.zeros(cov_matrix.shape[0])\n",
    "N=100000\n",
    "fix_method = 'cholesky'\n",
    "#Simulate 100,000 normal variables\n",
    "simulate_normal(N, output_name, cov_matrix, fix_method)\n",
    "testresult_5_2 = pd.read_csv(path_name + \"/\"+ output_name+\".csv\")\n",
    "testresult = testresult_5_2\n",
    "correctanswer =  testout5_2\n",
    "print(testfiles_equal(testresult,correctanswer))\n",
    "\n",
    "# test 5.3\n",
    "output_name = 'testresult_5_3'\n",
    "cov_matrix = test5_3.values \n",
    "N=100000\n",
    "fix_method = 'near_psd'\n",
    "#Simulate 100,000 normal variables\n",
    "simulate_normal(N, output_name, cov_matrix, fix_method)\n",
    "testresult_5_3 = pd.read_csv(path_name + \"/\"+ output_name+\".csv\")\n",
    "testresult = testresult_5_3\n",
    "correctanswer =  testout5_3\n",
    "print(testfiles_equal(testresult,correctanswer))\n",
    "\n",
    "# test 5.4\n",
    "output_name = 'testresult_5_4'\n",
    "cov_matrix = test5_3.values \n",
    "mean_vector = np.zeros(cov_matrix.shape[0])\n",
    "N=100000\n",
    "fix_method = 'Higham'\n",
    "#Simulate 100,000 normal variables\n",
    "simulate_normal(N, output_name, cov_matrix, fix_method)\n",
    "testresult_5_4 = pd.read_csv(path_name + \"/\"+ output_name+\".csv\")\n",
    "testresult = testresult_5_4\n",
    "correctanswer =  testout5_4\n",
    "print(testfiles_equal(testresult,correctanswer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correlation saved to testresult_5_5.csv\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import eigh\n",
    "from scipy.stats import norm\n",
    "\n",
    "def simulate_pca(cov, nsim, pctExp=1.0, mean=None, seed=1234):\n",
    "    np.random.seed(seed)\n",
    "    n = cov.shape[0]\n",
    "    # If the mean is missing then set to 0, otherwise use provided mean\n",
    "    if mean is None:\n",
    "        _mean = np.zeros(n)\n",
    "    else:\n",
    "        if len(mean) != n:\n",
    "            raise ValueError(\"Mean vector size does not match covariance matrix size\")\n",
    "        _mean = np.array(mean)\n",
    "    \n",
    "    # Eigenvalue decomposition\n",
    "    vals, vecs = eigh(cov)\n",
    "    \n",
    "    # Sort eigenvalues and eigenvectors in descending order\n",
    "    idx = np.argsort(vals)[::-1]\n",
    "    vals = vals[idx]\n",
    "    vecs = vecs[:, idx]\n",
    "    \n",
    "    # Keep only the positive eigenvalues (and corresponding eigenvectors)\n",
    "    posv = vals > 1e-8\n",
    "    vals = vals[posv]\n",
    "    vecs = vecs[:, posv]\n",
    "\n",
    "    # Calculate the cumulative variance explained and determine the number of components\n",
    "    tv = np.sum(vals)\n",
    "    cum_var_explained = np.cumsum(vals) / tv\n",
    "    nval = np.searchsorted(cum_var_explained, pctExp) + 1\n",
    "    vals = vals[:nval]\n",
    "    vecs = vecs[:, :nval]\n",
    "\n",
    "    # Calculate the matrix B for the factor loadings\n",
    "    B = vecs @ np.diag(np.sqrt(vals))\n",
    "\n",
    "    # Generate random normals\n",
    "    r = np.random.randn(nsim, nval)\n",
    "\n",
    "    # Simulate data and add the mean\n",
    "    out = (B @ r.T).T + _mean.reshape(1, -1)\n",
    "\n",
    "    # Return the covariance matrix of the simulated data\n",
    "    result=pd.DataFrame(out, columns = cov.columns).cov()\n",
    "    return result \n",
    "    \n",
    "\n",
    "cov_matrix = test5_2 \n",
    "nsim = 100000  # Number of simulations\n",
    "pctExp = 0.99  # Percentage of total variance explained by the PCA\n",
    "mean_vector = None  # Mean vector (optional)\n",
    "seed = 1234  # Random seed for reproducibility\n",
    "\n",
    "output_name= 'testresult_5_5'\n",
    "result = simulate_pca(cov_matrix, nsim, pctExp=pctExp, mean=mean_vector, seed=seed)\n",
    "result.to_csv(output_name + \".csv\", index=False, header=True)\n",
    "print(f\"{calc_type} saved to {output_name}.csv\")  \n",
    "testresult_5_5 = pd.read_csv(path_name + \"/\"+ output_name+\".csv\")\n",
    "testresult = testresult_5_5\n",
    "correctanswer =  testout5_5\n",
    "print(testfiles_equal(testresult,correctanswer))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Calculate Arithmetic Returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correlation saved to testresult_6_1.csv\n",
      "True\n",
      "correlation saved to testresult_6_2.csv\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def calculate_returns(prices, method=\"DISCRETE\", date_column=\"Date\"):\n",
    "    if date_column not in prices.columns:\n",
    "        raise ValueError(f\"dateColumn: {date_column} not in DataFrame\")\n",
    "    \n",
    "    prices = prices.set_index(date_column)\n",
    "    returns = prices.pct_change().dropna()  # This gives arithmetic returns\n",
    "    \n",
    "    if method.upper() == \"LOG\":\n",
    "        returns = np.log(1 + returns)  # Convert to log returns\n",
    "    \n",
    "    # returns.reset_index(inplace=True)\n",
    "    returns.columns = prices.columns\n",
    "\n",
    "    return returns\n",
    "\n",
    "# Example usage:\n",
    "df = test6\n",
    "arithmetic_returns = calculate_returns(df, method=\"DISCRETE\", date_column=\"Date\")\n",
    "log_returns = calculate_returns(df, method=\"LOG\", date_column=\"Date\")\n",
    "\n",
    "output_name = 'testresult_6_1'\n",
    "arithmetic_returns.to_csv(output_name + \".csv\", index=True, index_label='Date', header=True)\n",
    "print(f\"{calc_type} saved to {output_name}.csv\")  \n",
    "testresult_6_1 = pd.read_csv(path_name + \"/\"+ output_name+\".csv\")\n",
    "testresult = testresult_6_1\n",
    "correctanswer =  testout6_1\n",
    "print(testfiles_equal(testresult,correctanswer))\n",
    "\n",
    "output_name = 'testresult_6_2'\n",
    "log_returns.to_csv(output_name + \".csv\", index=True, index_label='Date', header=True)\n",
    "print(f\"{calc_type} saved to {output_name}.csv\")  \n",
    "testresult_6_2 = pd.read_csv(path_name + \"/\"+ output_name+\".csv\")\n",
    "testresult = testresult_6_2\n",
    "correctanswer =  testout6_2\n",
    "print(testfiles_equal(testresult,correctanswer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit Distribution (Normal, T, T Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correlation saved to testresult_7_1.csv\n",
      "True\n",
      "correlation saved to testresult_7_2.csv\n",
      "True\n",
      "correlation saved to testresult_7_3.csv\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import norm\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import t as t_dist\n",
    "from scipy.optimize import minimize\n",
    "from scipy.stats import kurtosis\n",
    "\n",
    "# 1. Fit Normal Distribution\n",
    "def fit_normal(data):\n",
    "    mu = np.mean(data)\n",
    "    sigma = np.std(data)\n",
    "    return mu, sigma\n",
    "\n",
    "\n",
    "def fit_general_t_simple(x):\n",
    "    # Fit the T distribution to the data\n",
    "    params = t_dist.fit(x)\n",
    "\n",
    "    # The `fit` function returns the parameters in the order (nu, mu, sigma)\n",
    "    nu, mu, sigma = params\n",
    "\n",
    "    return mu, sigma, nu\n",
    "\n",
    "\n",
    "# 3. T Regression\n",
    "# Adjusting the initial parameter guesses and optimization settings\n",
    "def fit_regression_t_adjusted(y, X):\n",
    "    # Add intercept\n",
    "    X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "    n, nB = X.shape\n",
    "\n",
    "    # Initial estimates based on OLS\n",
    "    b_start = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "    residuals = y - X @ b_start\n",
    "    start_m = residuals.mean()\n",
    "    start_s = residuals.std(ddof=nB)  # Adjusting degrees of freedom based on number of predictors\n",
    "    start_nu = 6.0 / kurtosis(residuals, fisher=False) + 4\n",
    "\n",
    "    # Define the negative log likelihood for the T distribution\n",
    "    def neg_log_likelihood(params):\n",
    "        nu, mu, sigma = params[:3]\n",
    "        beta = params[3:]\n",
    "        errors = y - X @ beta\n",
    "        rv = t_dist(df=nu, loc=mu, scale=sigma)\n",
    "        return -np.sum(np.log(rv.pdf(errors)))\n",
    "\n",
    "    # Adjust initial guesses to be closer to the expected values\n",
    "    initial_params = [start_nu, start_m, start_s] + b_start.tolist()\n",
    "    initial_params[2] = 0.048548  # sigma\n",
    "\n",
    "    # Optimization with constraints to keep nu greater than 2\n",
    "    bounds = [(2.0001, None), (None, None), (1e-6, None)] + [(None, None)] * nB\n",
    "    result = minimize(neg_log_likelihood, initial_params, method='L-BFGS-B', bounds=bounds)\n",
    "\n",
    "    # Extract optimized parameters\n",
    "    nu, mu, sigma = result.x[:3]\n",
    "    Alpha = result.x[3]\n",
    "    B1 = result.x[4]\n",
    "    B2 = result.x[5]\n",
    "    B3 = result.x[6]\n",
    "\n",
    "    return mu, sigma, nu, Alpha, B1, B2, B3\n",
    "\n",
    "\n",
    "# Fit the models\n",
    "mu_1, sigma_1 = fit_normal(test7_1['x1'])\n",
    "mu_2, sigma_2, nu_2 = fit_general_t_simple(test7_2['x1'].values)\n",
    "\n",
    "# Apply the adjusted fitting function to the data\n",
    "y = test7_3['y'].values\n",
    "X = test7_3[['x1','x2','x3']].values\n",
    "# adjusted_fit_results = fit_regression_t_adjusted(y, X)\n",
    "# adjusted_fit_results\n",
    "mu_3,sigma_3,nu_3,Alpha,B1,B2,B3 = fit_regression_t_adjusted(y, X)\n",
    "\n",
    "\n",
    "# Save the results\n",
    "pd.DataFrame({'mu': [mu_1], 'sigma': [sigma_1]}).to_csv('testresult_7_1.csv', index=False)\n",
    "pd.DataFrame({'mu': [mu_2], 'sigma': [sigma_2], 'nu': [nu_2]}).to_csv('testresult_7_2.csv', index=False)\n",
    "pd.DataFrame({'mu': [mu_3], 'sigma': [sigma_3], 'nu': [nu_3], 'Alpha': [Alpha], 'B1': [B1], 'B2': [B2], 'B3': [B3]}).to_csv('testresult_7_3.csv', index=False)\n",
    "# mu,sigma,nu,Alpha,B1,B2,B3\n",
    "\n",
    "\n",
    "output_name = 'testresult_7_1'\n",
    "print(f\"{calc_type} saved to {output_name}.csv\")  \n",
    "testresult_7_1 = pd.read_csv(path_name + \"/\"+ output_name+\".csv\")\n",
    "testresult = testresult_7_1\n",
    "correctanswer =  testout7_1\n",
    "print(testfiles_equal(testresult,correctanswer))\n",
    "\n",
    "\n",
    "output_name = 'testresult_7_2'\n",
    "print(f\"{calc_type} saved to {output_name}.csv\")  \n",
    "testresult_7_2 = pd.read_csv(path_name + \"/\"+ output_name+\".csv\")\n",
    "testresult = testresult_7_2\n",
    "correctanswer =  testout7_2\n",
    "print(testfiles_equal(testresult,correctanswer))\n",
    "\n",
    "\n",
    "output_name = 'testresult_7_3'\n",
    "print(f\"{calc_type} saved to {output_name}.csv\")  \n",
    "testresult_7_3 = pd.read_csv(path_name + \"/\"+ output_name+\".csv\")\n",
    "testresult = testresult_7_3\n",
    "correctanswer =  testout7_3\n",
    "print(testfiles_equal(testresult,correctanswer))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. VaR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.1 Var 8.4 ES from Normal Distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VaR Absolute 0.030920416807415675 VaR Diff from Mean 0.07694615326028394\n",
      "ES Absolute 0.05046784401426306 ES Diff from Mean 0.09649358046713133\n",
      "correlation saved to testresult_8_1.csv\n",
      "True\n",
      "correlation saved to testresult_8_4.csv\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import norm\n",
    "from numpy import mean, std, var, cov, corrcoef\n",
    "from scipy.optimize import minimize\n",
    "from scipy.integrate import quad\n",
    "import numpy as np\n",
    "\n",
    "# Define the VaR and ES functions for an array of data\n",
    "def VaR_ES(x, alpha=0.05):\n",
    "    x_sorted = np.sort(x)\n",
    "    n = int(alpha * len(x_sorted))\n",
    "    var = (x_sorted[n-1] + x_sorted[max(n-2, 0)]) / 2\n",
    "    es = mean(x_sorted[:n])\n",
    "    return -var, -es\n",
    "\n",
    "data = test7_1.values\n",
    "# Calculate the distribution parameters from the data\n",
    "data_mean = np.mean(data)\n",
    "data_std = np.std(data, ddof=1)  # Use ddof=1 for sample standard deviation\n",
    "\n",
    "# Define the normal distribution with the calculated parameters\n",
    "nDist = norm(data_mean, data_std)\n",
    "\n",
    "# # Calculate VaR and ES for the simulated normal distribution data in test7_1\n",
    "# normVaR, normES = VaR_ES(data)\n",
    "\n",
    "# Calculate expected VaR and ES from the normal distribution\n",
    "eNormVaR = -nDist.ppf(0.05)\n",
    "\n",
    "def f(x):\n",
    "    return x * nDist.pdf(x)\n",
    "\n",
    "st = nDist.ppf(1e-12)\n",
    "eNormES2, _ = quad(f, st, -eNormVaR)\n",
    "eNormES2 = -eNormES2 / 0.05\n",
    "\n",
    "# Output the calculated VaR and ES values\n",
    "print(\"VaR Absolute\", eNormVaR, \"VaR Diff from Mean\", data_mean+eNormVaR)\n",
    "print(\"ES Absolute\", eNormES2, \"ES Diff from Mean\", data_mean+eNormES2)\n",
    "\n",
    "\n",
    "# Save the results\n",
    "pd.DataFrame({'VaR Absolute': [eNormVaR], 'VaR Diff from Mean': [data_mean+eNormVaR]}).to_csv('testresult_8_1.csv', index=False)\n",
    "pd.DataFrame({'ES Absolute': [eNormES2], 'ES Diff from Mean': [data_mean+eNormES2]}).to_csv('testresult_8_4.csv', index=False)\n",
    "\n",
    "# test 8.1\n",
    "output_name = 'testresult_8_1'\n",
    "print(f\"{calc_type} saved to {output_name}.csv\")  \n",
    "testresult_8_1 = pd.read_csv(path_name + \"/\"+ output_name+\".csv\")\n",
    "testresult = testresult_8_1\n",
    "correctanswer =  testout8_1\n",
    "print(testfiles_equal(testresult,correctanswer))\n",
    "\n",
    "# test 8.4\n",
    "output_name = 'testresult_8_4'\n",
    "print(f\"{calc_type} saved to {output_name}.csv\")  \n",
    "testresult_8_4 = pd.read_csv(path_name + \"/\"+ output_name+\".csv\")\n",
    "testresult = testresult_8_4\n",
    "correctanswer =  testout8_4\n",
    "print(testfiles_equal(testresult,correctanswer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.2 Var 8.5 ES from T Distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 8.2 - VaR from T Distribution: {'VaR Absolute': [0.04152970271623356], 'VaR Diff from Mean': [0.08743860871020243]}\n",
      "Test 8.5 - ES from T Distribution: {'ES Absolute': [0.07523208716011756], 'ES Diff from Mean': [0.12114099315408641]}\n",
      "correlation saved to testresult_8_2.csv\n",
      "True\n",
      "correlation saved to testresult_8_5.csv\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import t as t_dist\n",
    "from scipy.integrate import quad\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Extract the data as a numpy array\n",
    "data = test7_2['x1'].values\n",
    "\n",
    "# Fit a T distribution to the data\n",
    "params = t_dist.fit(data)\n",
    "t_df, t_loc, t_scale = params\n",
    "\n",
    "# Define the T distribution with the calculated parameters\n",
    "t_distribution = t_dist(t_df, loc=t_loc, scale=t_scale)\n",
    "\n",
    "# Calculate VaR and ES for the fitted T distribution\n",
    "t_VaR = -t_distribution.ppf(0.05)\n",
    "t_ES_func = lambda x: x * t_distribution.pdf(x)\n",
    "t_ES, _ = quad(t_ES_func, t_distribution.ppf(1e-12), -t_VaR)\n",
    "t_ES = -t_ES / 0.05\n",
    "\n",
    "# Calculate the mean of the data for comparison\n",
    "data_mean = np.mean(data)\n",
    "\n",
    "# Save the results for test 8.2 and test 8.5\n",
    "results_8_2 = {\n",
    "    \"VaR Absolute\": [t_VaR],\n",
    "    \"VaR Diff from Mean\": [data_mean + t_VaR]\n",
    "}\n",
    "results_8_5 = {\n",
    "    \"ES Absolute\": [t_ES],\n",
    "    \"ES Diff from Mean\": [data_mean + t_ES]\n",
    "}\n",
    "\n",
    "# Convert the results to DataFrame and save as CSV\n",
    "pd.DataFrame(results_8_2).to_csv('testresult_8_2.csv', index=False)\n",
    "pd.DataFrame(results_8_5).to_csv('testresult_8_5.csv', index=False)\n",
    "\n",
    "# Print out the results\n",
    "\n",
    "print(f\"Test 8.2 - VaR from T Distribution: {results_8_2}\")\n",
    "print(f\"Test 8.5 - ES from T Distribution: {results_8_5}\")\n",
    "\n",
    "\n",
    "# test 8.2\n",
    "output_name = 'testresult_8_2'\n",
    "print(f\"{calc_type} saved to {output_name}.csv\")  \n",
    "testresult_8_2 = pd.read_csv(path_name + \"/\"+ output_name+\".csv\")\n",
    "testresult = testresult_8_2\n",
    "correctanswer =  testout8_2\n",
    "print(testfiles_equal(testresult,correctanswer))\n",
    "\n",
    "# test 8.5\n",
    "output_name = 'testresult_8_5'\n",
    "print(f\"{calc_type} saved to {output_name}.csv\")  \n",
    "testresult_8_5 = pd.read_csv(path_name + \"/\"+ output_name+\".csv\")\n",
    "testresult = testresult_8_5\n",
    "correctanswer =  testout8_5\n",
    "print(testfiles_equal(testresult,correctanswer))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.3 Var 8.6 ES from Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 8.3 - VaR from Simulation: {'VaR Absolute': [0.04148757595434181], 'VaR Diff from Mean': [0.08768256390034819]}\n",
      "Test 8.6 - ES from Simulation: {'ES Absolute': [0.07521605937650962], 'ES Diff from Mean': [0.121411047322516]}\n",
      "correlation saved to testresult_8_3.csv\n",
      "True\n",
      "correlation saved to testresult_8_6.csv\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import t as t_dist\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Function to calculate VaR and ES for an array of data\n",
    "def VaR_ES_from_simulation(x, alpha=0.05):\n",
    "    x_sorted = np.sort(x)\n",
    "    n = int(alpha * len(x_sorted))\n",
    "    var = x_sorted[n-1]  # VaR is the nth value, not the average between n and n-1\n",
    "    es = np.mean(x_sorted[:n])  # ES is the average of the smallest n values\n",
    "    return -var, -es\n",
    "\n",
    "# Fit a T distribution to the actual data\n",
    "data_t_dist = test7_2['x1'].values\n",
    "params = t_dist.fit(data_t_dist)\n",
    "t_df, t_loc, t_scale = params\n",
    "\n",
    "# Simulate a large number of data points from the fitted T distribution\n",
    "n_simulations = 100000  # Number of simulations\n",
    "simulated_data = t_dist(df=t_df, loc=t_loc, scale=t_scale).rvs(n_simulations)\n",
    "\n",
    "# Calculate the VaR and ES from the simulated data\n",
    "simulated_VaR, simulated_ES = VaR_ES_from_simulation(simulated_data)\n",
    "\n",
    "# Save the results for test 8.3 and test 8.6\n",
    "results_8_3 = {\n",
    "    \"VaR Absolute\": [simulated_VaR],\n",
    "    \"VaR Diff from Mean\": [np.mean(simulated_data) + simulated_VaR]\n",
    "}\n",
    "results_8_6 = {\n",
    "    \"ES Absolute\": [simulated_ES],\n",
    "    \"ES Diff from Mean\": [np.mean(simulated_data) + simulated_ES]\n",
    "}\n",
    "\n",
    "# Convert the results to DataFrame and save as CSV\n",
    "pd.DataFrame(results_8_3).to_csv('testresult_8_3.csv', index=False)\n",
    "pd.DataFrame(results_8_6).to_csv('testresult_8_6.csv', index=False)\n",
    "\n",
    "# Output the results\n",
    "print(f\"Test 8.3 - VaR from Simulation: {results_8_3}\")\n",
    "print(f\"Test 8.6 - ES from Simulation: {results_8_6}\")\n",
    "\n",
    "# test 8.3\n",
    "output_name = 'testresult_8_3'\n",
    "print(f\"{calc_type} saved to {output_name}.csv\")  \n",
    "testresult_8_3 = pd.read_csv(path_name + \"/\"+ output_name+\".csv\")\n",
    "testresult = testresult_8_3\n",
    "correctanswer =  testout8_3\n",
    "print(testfiles_equal(testresult,correctanswer))\n",
    "\n",
    "# test 8.6\n",
    "output_name = 'testresult_8_6'\n",
    "print(f\"{calc_type} saved to {output_name}.csv\")  \n",
    "testresult_8_6 = pd.read_csv(path_name + \"/\"+ output_name+\".csv\")\n",
    "testresult = testresult_8_6\n",
    "correctanswer =  testout8_6\n",
    "print(testfiles_equal(testresult,correctanswer))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. VaR/ES on 2 levels from simulated values - Copula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            VaR95        ES95  VaR95_Pct  ES95_Pct\n",
      "Stock                                             \n",
      "A       94.140934  117.739927   0.047070  0.058870\n",
      "B      107.468184  151.454100   0.035823  0.050485\n",
      "Total  152.337431  200.614625   0.030467  0.040123\n",
      "correlation saved to testresult_9_1.csv\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import t, norm, spearmanr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming test9_1_portfolio.csv and test9_1_returns.csv have been read into \n",
    "# test9_1_portfolio and test9_1_returns respectively\n",
    "portfolio_df = test9_1_portfolio\n",
    "returns_df = test9_1_returns\n",
    "\n",
    "# Placeholder function to fit a normal distribution\n",
    "def fit_normal(data):\n",
    "    mu, sigma = norm.fit(data)\n",
    "    return {\n",
    "        'eval': lambda u: norm.ppf(u, loc=mu, scale=sigma),\n",
    "        'u': norm.cdf(data, loc=mu, scale=sigma)\n",
    "    }\n",
    "\n",
    "# Placeholder function to fit a T distribution\n",
    "def fit_general_t(data):\n",
    "    params = t.fit(data)\n",
    "    nu, mu, sigma = params\n",
    "    return {\n",
    "        'eval': lambda u: t.ppf(u, df=nu, loc=mu, scale=sigma),\n",
    "        'u': t.cdf(data, df=nu, loc=mu, scale=sigma)\n",
    "    }\n",
    "\n",
    "# Placeholder function to simulate data using PCA approach\n",
    "def simulate_pca(cov, nsim, seed=0):\n",
    "    np.random.seed(seed)\n",
    "    mean = np.zeros(cov.shape[0])\n",
    "    data = np.random.multivariate_normal(mean, cov, nsim)\n",
    "    return data\n",
    "\n",
    "# Fit models to the returns data\n",
    "models = {\n",
    "    \"A\": fit_normal(returns_df['A']),\n",
    "    \"B\": fit_general_t(returns_df['B'])\n",
    "}\n",
    "\n",
    "# Spearman correlation for returns data\n",
    "U = np.column_stack((models[\"A\"]['u'], models[\"B\"]['u']))\n",
    "corr, _ = spearmanr(U)\n",
    "cov_matrix = np.array([[1, corr], [corr, 1]])\n",
    "\n",
    "# Perform PCA-based simulation\n",
    "nSim = 100000\n",
    "simulated_data = simulate_pca(cov_matrix, nSim)\n",
    "simulated_u = norm.cdf(simulated_data)\n",
    "\n",
    "# Transform simulated quantiles back to returns\n",
    "simulated_returns = pd.DataFrame({\n",
    "    \"A\": models[\"A\"]['eval'](simulated_u[:, 0]),\n",
    "    \"B\": models[\"B\"]['eval'](simulated_u[:, 1])\n",
    "})\n",
    "\n",
    "# Compute VaR and ES for the portfolio\n",
    "def compute_risk_metrics(simulated_pnl, alpha=0.05):\n",
    "    VaR = -np.percentile(simulated_pnl, alpha*100)\n",
    "    ES = -np.mean(simulated_pnl[simulated_pnl <= -VaR])\n",
    "    return VaR, ES\n",
    "\n",
    "# Initialize lists to store results\n",
    "VaRs, ESs, VaR_pcts, ES_pcts = [], [], [], []\n",
    "\n",
    "# Compute for each stock and the total portfolio\n",
    "for stock in portfolio_df['Stock']:\n",
    "    holding, price = portfolio_df.loc[portfolio_df['Stock'] == stock, ['Holding', 'Starting Price']].values[0]\n",
    "    pnl = holding * price * simulated_returns[stock]\n",
    "    VaR, ES = compute_risk_metrics(pnl)\n",
    "    VaRs.append(VaR)\n",
    "    ESs.append(ES)\n",
    "    VaR_pcts.append(VaR / (holding * price))\n",
    "    ES_pcts.append(ES / (holding * price))\n",
    "\n",
    "# Add total portfolio metrics\n",
    "total_pnl = portfolio_df['Holding'].values * portfolio_df['Starting Price'].values @ simulated_returns.T\n",
    "total_VaR, total_ES = compute_risk_metrics(total_pnl)\n",
    "VaRs.append(total_VaR)\n",
    "ESs.append(total_ES)\n",
    "initial_value = portfolio_df['Holding'].values @ portfolio_df['Starting Price'].values\n",
    "VaR_pcts.append(total_VaR / initial_value)\n",
    "ES_pcts.append(total_ES / initial_value)\n",
    "\n",
    "# Create DataFrame for results\n",
    "riskOut = pd.DataFrame({\n",
    "    'Stock': ['A', 'B', 'Total'],\n",
    "    'VaR95': VaRs,\n",
    "    'ES95': ESs,\n",
    "    'VaR95_Pct': VaR_pcts,\n",
    "    'ES95_Pct': ES_pcts\n",
    "})\n",
    "\n",
    "# Display and save the results\n",
    "riskOut.set_index('Stock', inplace=True)\n",
    "print(riskOut)\n",
    "riskOut.to_csv('testresult_9_1.csv')\n",
    "\n",
    "\n",
    "# test 9.1\n",
    "output_name = 'testresult_9_1'\n",
    "print(f\"{calc_type} saved to {output_name}.csv\")  \n",
    "testresult_9_1 = pd.read_csv(path_name + \"/\"+ output_name+\".csv\")\n",
    "testresult = testresult_9_1\n",
    "correctanswer =  testout9_1\n",
    "print(testfiles_equal_largeTol(testresult,correctanswer))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
