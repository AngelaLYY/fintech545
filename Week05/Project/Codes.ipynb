{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary packages\n",
    "import os\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from scipy.stats import norm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import all test files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = pd.read_csv(\"/Users/angelaliang/Documents/fintech545/testfiles/data/test1.csv\")\n",
    "testout1_1 = pd.read_csv(\"/Users/angelaliang/Documents/fintech545/testfiles/data/testout_1.1.csv\")\n",
    "testout1_2 =pd.read_csv(\"/Users/angelaliang/Documents/fintech545/testfiles/data/testout_1.2.csv\")\n",
    "testout1_3 = pd.read_csv(\"/Users/angelaliang/Documents/fintech545/testfiles/data/testout_1.3.csv\")\n",
    "testout1_4 = pd.read_csv(\"/Users/angelaliang/Documents/fintech545/testfiles/data/testout_1.4.csv\")\n",
    "\n",
    "test2 = pd.read_csv(\"/Users/angelaliang/Documents/fintech545/testfiles/data/test2.csv\")\n",
    "testout2_1 = pd.read_csv(\"/Users/angelaliang/Documents/fintech545/testfiles/data/testout_2.1.csv\")\n",
    "testout2_2 = pd.read_csv(\"/Users/angelaliang/Documents/fintech545/testfiles/data/testout_2.2.csv\")\n",
    "testout2_3 = pd.read_csv(\"/Users/angelaliang/Documents/fintech545/testfiles/data/testout_2.3.csv\")\n",
    "\n",
    "testout3_1 = pd.read_csv(\"/Users/angelaliang/Documents/fintech545/testfiles/data/testout_3.1.csv\")\n",
    "testout3_2 = pd.read_csv(\"/Users/angelaliang/Documents/fintech545/testfiles/data/testout_3.2.csv\")\n",
    "testout3_3 = pd.read_csv(\"/Users/angelaliang/Documents/fintech545/testfiles/data/testout_3.3.csv\")\n",
    "testout3_4 = pd.read_csv(\"/Users/angelaliang/Documents/fintech545/testfiles/data/testout_3.4.csv\")\n",
    "\n",
    "testout4_1 = pd.read_csv(\"/Users/angelaliang/Documents/fintech545/testfiles/data/testout_4.1.csv\")\n",
    "\n",
    "test5_1 = pd.read_csv(\"/Users/angelaliang/Documents/fintech545/testfiles/data/test5_1.csv\")\n",
    "test5_2 = pd.read_csv(\"/Users/angelaliang/Documents/fintech545/testfiles/data/test5_2.csv\")\n",
    "test5_3 = pd.read_csv(\"/Users/angelaliang/Documents/fintech545/testfiles/data/test5_3.csv\")\n",
    "testout5_1 = pd.read_csv(\"/Users/angelaliang/Documents/fintech545/testfiles/data/testout_5.1.csv\")\n",
    "testout5_2 = pd.read_csv(\"/Users/angelaliang/Documents/fintech545/testfiles/data/testout_5.2.csv\")\n",
    "testout5_3 = pd.read_csv(\"/Users/angelaliang/Documents/fintech545/testfiles/data/testout_5.3.csv\")\n",
    "testout5_4 = pd.read_csv(\"/Users/angelaliang/Documents/fintech545/testfiles/data/testout_5.4.csv\")\n",
    "testout5_5 = pd.read_csv(\"/Users/angelaliang/Documents/fintech545/testfiles/data/testout_5.5.csv\")\n",
    "\n",
    "test6 = pd.read_csv(\"/Users/angelaliang/Documents/fintech545/testfiles/data/test6.csv\")\n",
    "testout6_1 = pd.read_csv(\"/Users/angelaliang/Documents/fintech545/testfiles/data/test6_1.csv\")\n",
    "testout6_2 = pd.read_csv(\"/Users/angelaliang/Documents/fintech545/testfiles/data/test6_2.csv\")\n",
    "\n",
    "test7_1 = pd.read_csv(\"/Users/angelaliang/Documents/fintech545/testfiles/data/test7_1.csv\")\n",
    "test7_2 = pd.read_csv(\"/Users/angelaliang/Documents/fintech545/testfiles/data/test7_2.csv\")\n",
    "test7_3 = pd.read_csv(\"/Users/angelaliang/Documents/fintech545/testfiles/data/test7_3.csv\")\n",
    "testout7_1 = pd.read_csv(\"/Users/angelaliang/Documents/fintech545/testfiles/data/testout7_1.csv\")\n",
    "testout7_2 = pd.read_csv(\"/Users/angelaliang/Documents/fintech545/testfiles/data/testout7_2.csv\")\n",
    "testout7_3 = pd.read_csv(\"/Users/angelaliang/Documents/fintech545/testfiles/data/testout7_3.csv\")\n",
    "\n",
    "testout8_1 = pd.read_csv(\"/Users/angelaliang/Documents/fintech545/testfiles/data/testout8_1.csv\")\n",
    "testout8_2 = pd.read_csv(\"/Users/angelaliang/Documents/fintech545/testfiles/data/testout8_2.csv\")\n",
    "testout8_3 = pd.read_csv(\"/Users/angelaliang/Documents/fintech545/testfiles/data/testout8_3.csv\")\n",
    "testout8_4 = pd.read_csv(\"/Users/angelaliang/Documents/fintech545/testfiles/data/testout8_4.csv\")\n",
    "testout8_5 = pd.read_csv(\"/Users/angelaliang/Documents/fintech545/testfiles/data/testout8_5.csv\")\n",
    "testout8_6 = pd.read_csv(\"/Users/angelaliang/Documents/fintech545/testfiles/data/testout8_6.csv\")\n",
    "\n",
    "\n",
    "test9_1_portfolio = pd.read_csv(\"/Users/angelaliang/Documents/fintech545/testfiles/data/test9_1_portfolio.csv\")\n",
    "test9_1_returns = pd.read_csv(\"/Users/angelaliang/Documents/fintech545/testfiles/data/test9_1_returns.csv\")\n",
    "testout9_1= pd.read_csv(\"/Users/angelaliang/Documents/fintech545/testfiles/data/testout9_1.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read path name for all locally saved files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_name = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test File Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check if two files are equal with small tolerance set \n",
    "# test and see if results are the same \n",
    "def testfiles_equal(df1, df2):\n",
    "    if df1.equals(df2):\n",
    "        return True\n",
    "    else:\n",
    "        # If they are not equal, it could be due to floating point precision,\n",
    "        # so we attempt a comparison that allows for a small absolute difference.\n",
    "        try:\n",
    "            pd.testing.assert_frame_equal(df1, df2, atol=1e-3)  # Use an appropriate tolerance\n",
    "            return True\n",
    "        except AssertionError:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Covariance estimation techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With missing data, calculate correlation and covariance using methods:\n",
    "1) skip missing rows\n",
    "2) pairwise calculation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_correlation_covariance(df, method, calc_type, output_name):\n",
    "    \n",
    "    # Dropping all rows with any missing values if method is 'dropmissing'\n",
    "      if method == 'dropmissing':\n",
    "            df_cleaned = df.dropna()\n",
    "            if calc_type=='correlation':\n",
    "                  result = df_cleaned.corr(method='pearson')\n",
    "            if calc_type=='covariance':\n",
    "                  result = df_cleaned.cov(min_periods=1)\n",
    "\n",
    "\n",
    "      elif method == 'pairwise':\n",
    "            if calc_type=='correlation':\n",
    "                  result = df.corr(method='pearson')\n",
    "            if calc_type=='covariance':\n",
    "                  result = df.cov(min_periods=1)\n",
    "                                    \n",
    "                  \n",
    "      result_reset = result.reset_index(drop=True) # reset row index, drop and set as header\n",
    "      result_reset.to_csv(output_name+\".csv\", index=False, header=True)\n",
    "      print(f\"{calc_type} saved to {output_name}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "covariance saved to testresult_1.1\n",
      "True\n",
      "correlation saved to testresult_1.2\n",
      "True\n",
      "covariance saved to testresult_1.3\n",
      "True\n",
      "correlation saved to testresult_1.4\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# testing \n",
    "df = test1 # same for the first 4 tests \n",
    "\n",
    "# test 1.1 \n",
    "method = 'dropmissing'\n",
    "calc_type = 'covariance'\n",
    "output_name = 'testresult_1.1'\n",
    "calculate_correlation_covariance(df, method, calc_type, output_name)\n",
    "# testresult_1_1 = pd.read_csv(path_name + \"/\"+ output_name+\".csv\", header=None, index_col=0)\n",
    "testresult_1_1 = pd.read_csv(path_name + \"/\"+ output_name+\".csv\")  # header is inferred by default\n",
    "testresult = testresult_1_1\n",
    "correctanswer =  testout1_1\n",
    "print(testfiles_equal(testresult,correctanswer))\n",
    "\n",
    "# test 1.2 \n",
    "method = 'dropmissing'\n",
    "calc_type = 'correlation'\n",
    "output_name = 'testresult_1.2'\n",
    "calculate_correlation_covariance(df, method, calc_type, output_name)\n",
    "# testresult_1_2 = pd.read_csv(path_name + \"/\"+ output_name+\".csv\", header=None, index_col=0)\n",
    "testresult_1_2 = pd.read_csv(path_name + \"/\"+ output_name+\".csv\")\n",
    "testresult = testresult_1_2\n",
    "correctanswer =  testout1_2\n",
    "print(testfiles_equal(testresult,correctanswer))\n",
    "\n",
    "# test 1.3\n",
    "method = 'pairwise'\n",
    "calc_type = 'covariance'\n",
    "output_name = 'testresult_1.3'\n",
    "calculate_correlation_covariance(df, method, calc_type, output_name)\n",
    "# testresult_1_3 = pd.read_csv(path_name + \"/\"+ output_name+\".csv\", header=None, index_col=0)\n",
    "testresult_1_3 = pd.read_csv(path_name + \"/\"+ output_name+\".csv\")\n",
    "testresult = testresult_1_3\n",
    "correctanswer =  testout1_3\n",
    "print(testfiles_equal(testresult,correctanswer))\n",
    "\n",
    "# test 1.4\n",
    "method = 'pairwise'\n",
    "calc_type = 'correlation'\n",
    "output_name = 'testresult_1.4'\n",
    "calculate_correlation_covariance(df, method, calc_type, output_name)\n",
    "# testresult_1_4 = pd.read_csv(path_name + \"/\"+ output_name+\".csv\", header=None, index_col=0)\n",
    "testresult_1_4 = pd.read_csv(path_name + \"/\"+ output_name+\".csv\")\n",
    "testresult = testresult_1_4\n",
    "correctanswer =  testout1_4\n",
    "print(testfiles_equal(testresult,correctanswer))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. EW Covariance and Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_ew_correlation_covariance(df, calc_type, output_name, lambda_param):\n",
    "      \n",
    "      if lambda_param is None:\n",
    "            raise ValueError(\"For 'ew' method, lambda_param must be provided.\")\n",
    "      \n",
    "      n = len(df)\n",
    "      weights = np.array([(1 - lambda_param) * (lambda_param ** (n - 1 - i)) for i in range(n)])\n",
    "      weights /= weights.sum()\n",
    "      demeaned_data = df - df.mean()\n",
    "      \n",
    "      # Compute the exponentially weighted covariance matrix\n",
    "      ew_cov_matrix = (demeaned_data.T * weights).dot(demeaned_data) / weights.sum()\n",
    "      \n",
    "      if calc_type == 'correlation':\n",
    "            # Calculate the exponentially weighted correlation matrix\n",
    "            # Standard deviations are computed with the same weights used for covariance\n",
    "            std_dev = np.sqrt(np.diag(ew_cov_matrix))\n",
    "            # To ensure the standard deviations are non-zero to prevent division by zero errors\n",
    "            std_dev[std_dev == 0] = np.finfo(np.float64).tiny\n",
    "            ew_corr_matrix = ew_cov_matrix / np.outer(std_dev, std_dev)\n",
    "            result = ew_corr_matrix\n",
    "\n",
    "      elif calc_type == 'covariance':\n",
    "            result = ew_cov_matrix\n",
    "\n",
    "      # Reset index to ensure a clean DataFrame, suitable for CSV output\n",
    "      result_reset = result.reset_index(drop=True)\n",
    "      # Save to CSV\n",
    "      result_reset.to_csv(output_name + \".csv\", index=False, header=True)\n",
    "      print(f\"{calc_type} saved to {output_name}.csv\")\n",
    "\n",
    "\n",
    "def calculate_ew_sd(df, output_name, lambda_param1,lambda_param2):\n",
    "      # Inside your calculate_correlation_covariance function\n",
    "      if lambda_param1 is None or lambda_param2 is None: \n",
    "            raise ValueError(\"For 'ew' method, lambda_param must be provided for both covariance and correlation.\")\n",
    "      n = len(df)\n",
    "      weights1 = np.array([(1 - lambda_param1) * (lambda_param1 ** (n - 1 - i)) for i in range(n)])\n",
    "      weights1 /= weights1.sum()\n",
    "      demeaned_data1 = df - df.mean()\n",
    "      ew_cov_matrix1 = (demeaned_data1.T * weights1).dot(demeaned_data1) / weights1.sum()\n",
    "      std_dev1 = np.sqrt(np.diag(ew_cov_matrix1)) # 0.97 \n",
    "      \n",
    "      weights2 = np.array([(1 - lambda_param2) * (lambda_param2 ** (n - 1 - i)) for i in range(n)])\n",
    "      weights2 /= weights2.sum()\n",
    "      demeaned_data2 = df - df.mean()\n",
    "      ew_cov_matrix2 = (demeaned_data2.T * weights2).dot(demeaned_data2) / weights2.sum()\n",
    "      std_dev2 = np.sqrt(np.diag(ew_cov_matrix2)) # 0.94 \n",
    "\n",
    "      # if calc_type == 'correlation':\n",
    "      # Use the covariance matrix to calculate correlation matrix\n",
    "      # You may need to adjust the following lines if your DataFrame structure is different\n",
    "      \n",
    "      ew_corr = np.diag(1/std_dev2) @ ew_cov_matrix2 @ np.diag(1/std_dev2)\n",
    "      ew_cov = np.diag(std_dev1) @ ew_corr @ np.diag(std_dev1)\n",
    "      ew_cov.columns = df.columns\n",
    "      result = ew_cov\n",
    "      # Reset index to ensure a clean DataFrame, suitable for CSV output\n",
    "      result_reset = result.reset_index(drop=True)\n",
    "      # result_reset= pd.DataFrame(np.diag(std_dev1) @ ew_corr @ np.diag(std_dev1), columns=df.columns)\n",
    "      # Save to CSV\n",
    "      # print(result_reset)\n",
    "      result_reset.to_csv(output_name + \".csv\", index=False)\n",
    "      print(f\"{calc_type} saved to {output_name}.csv\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "covariance saved to testresult_2.1.csv\n",
      "True\n",
      "correlation saved to testresult_2.2.csv\n",
      "True\n",
      "correlation saved to testresult_2.3.csv\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = test2\n",
    "# method = 'ew'\n",
    "\n",
    "# test 2.1\n",
    "calc_type = 'covariance'\n",
    "output_name = 'testresult_2.1'\n",
    "lambda_val = 0.97\n",
    "calculate_ew_correlation_covariance(df, calc_type, output_name, lambda_param=lambda_val)\n",
    "testresult_2_1 = pd.read_csv(path_name + \"/\"+ output_name+\".csv\")\n",
    "testresult = testresult_2_1\n",
    "correctanswer =  testout2_1\n",
    "print(testfiles_equal(testresult,correctanswer))\n",
    "\n",
    "\n",
    "# test 2.2\n",
    "calc_type = 'correlation'\n",
    "output_name = 'testresult_2.2'\n",
    "lambda_val = 0.94\n",
    "calculate_ew_correlation_covariance(df, calc_type, output_name, lambda_param=lambda_val)\n",
    "testresult_2_2 = pd.read_csv(path_name + \"/\"+ output_name+\".csv\")\n",
    "testresult = testresult_2_2\n",
    "correctanswer =  testout2_2\n",
    "print(testfiles_equal(testresult,correctanswer))\n",
    "\n",
    "# test 2.3 \n",
    "calc_type = 'correlation'\n",
    "output_name = 'testresult_2.3'\n",
    "lambda_val1 = 0.97\n",
    "lambda_val2 = 0.94\n",
    "calculate_ew_sd(df, output_name, lambda_param1=lambda_val1,lambda_param2=lambda_val2)\n",
    "testresult_2_3 = pd.read_csv(path_name + \"/\"+ output_name+\".csv\")\n",
    "testresult = testresult_2_3\n",
    "correctanswer =  testout2_3\n",
    "\n",
    "print(testfiles_equal(testresult,correctanswer))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Near_psd and Higham Covariance and Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correlation saved to testresult_3_1.csv\n",
      "True\n",
      "correlation saved to testresult_3_2.csv\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import eig, inv\n",
    "\n",
    "def near_psd(a, method, epsilon=0.0):\n",
    "    n = a.shape[0]\n",
    "    invSD = None\n",
    "    out = np.copy(a)\n",
    "\n",
    "    # calculate the correlation matrix if we got a covariance\n",
    "    if not np.allclose(np.diag(out), 1.0):\n",
    "        invSD = np.diag(1.0 / np.sqrt(np.diag(out)))\n",
    "        out = invSD @ out @ invSD\n",
    "        # print(\"correlation\", out)\n",
    "    if method == 'correlation':\n",
    "        out = out \n",
    "    elif method == 'covariance':\n",
    "        # SVD, update the eigenvalue and scale\n",
    "        vals, vecs = eig(out)\n",
    "        vals = np.maximum(vals, epsilon)\n",
    "        T = np.diag(1.0 / (vecs * vecs @ vals))\n",
    "        T = np.diag(np.sqrt(np.diag(T)))\n",
    "        l = np.diag(np.sqrt(vals))\n",
    "        B = T @ vecs @ l\n",
    "        out = B @ B.T\n",
    "\n",
    "        # Add back the variance\n",
    "        if invSD is not None:\n",
    "            invSD = np.diag(1.0 / np.diag(invSD))\n",
    "            out = invSD @ out @ invSD\n",
    "            # print(\"covariance\", out)\n",
    "\n",
    "\n",
    "        \n",
    "    result = pd.DataFrame(out, columns=testout3_1.columns)\n",
    "        # # Reset index to ensure a clean DataFrame, suitable for CSV output\n",
    "        # # print(result)\n",
    "        # result_reset = result.reset_index(drop=True)\n",
    "        # Save to CSV\n",
    "    return result    \n",
    "    \n",
    "\n",
    "# test 3.1 \n",
    "output_name = 'testresult_3_1'\n",
    "method = 'covariance'\n",
    "result = near_psd(testout1_3, method, epsilon=0.0)\n",
    "result.to_csv(output_name + \".csv\", index=False, header=True)\n",
    "print(f\"{calc_type} saved to {output_name}.csv\")\n",
    "testresult_3_1 = pd.read_csv(path_name + \"/\"+ output_name+\".csv\")\n",
    "testresult = testresult_3_1\n",
    "correctanswer =  testout3_1\n",
    "# print(\"Test OUT: \",testout3_1)\n",
    "print(testfiles_equal(testresult,correctanswer))\n",
    "\n",
    "# test 3.2 \n",
    "output_name = 'testresult_3_2'\n",
    "method = 'correlation'\n",
    "result = near_psd(testout1_4, method, epsilon=0.0)\n",
    "result.to_csv(output_name + \".csv\", index=False, header=True)\n",
    "print(f\"{calc_type} saved to {output_name}.csv\")\n",
    "testresult_3_2 = pd.read_csv(path_name + \"/\"+ output_name+\".csv\")\n",
    "testresult = testresult_3_2\n",
    "correctanswer =  testout3_2\n",
    "# print(\"Test OUT: \",testout3_1)\n",
    "print(testfiles_equal(testresult,correctanswer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Higham Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged in 38 iterations.\n",
      "correlation saved to testresult_3_3.csv\n",
      "True\n",
      "correlation saved to testresult_3_4.csv\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.linalg import sqrtm\n",
    "\n",
    "def _getAplus(A):\n",
    "    eigval, eigvec = np.linalg.eigh(A)\n",
    "    Q = np.diag(np.maximum(eigval, 0))\n",
    "    return eigvec @ Q @ eigvec.T\n",
    "\n",
    "def _getPS(A, W):\n",
    "    W05 = sqrtm(W)\n",
    "    iW = inv(W05)\n",
    "    return iW @ _getAplus(W05 @ A @ W05) @ iW\n",
    "\n",
    "def _getPu(A, W):\n",
    "    Aret = np.copy(A)\n",
    "    np.fill_diagonal(Aret, 1)\n",
    "    return Aret\n",
    "\n",
    "def wgtNorm(A, W):\n",
    "    W05 = sqrtm(W)\n",
    "    return np.sum((W05 @ A @ W05) ** 2)\n",
    "\n",
    "def higham_nearestPSD(pc, method, epsilon=1e-9, maxIter=100, tol=1e-9):\n",
    "    n = pc.shape[0]\n",
    "    W = np.diag(np.ones(n))\n",
    "    deltaS = 0\n",
    "    invSD = None\n",
    "\n",
    "    Yk = np.copy(pc)\n",
    "    \n",
    "    # calculate the correlation matrix if we got a covariance\n",
    "    if not np.allclose(np.diag(Yk), 1.0):\n",
    "        invSD = np.diag(1.0 / np.sqrt(np.diag(Yk)))\n",
    "        Yk = invSD @ Yk @ invSD\n",
    "    if method == 'correlation':\n",
    "        result = Yk\n",
    "    \n",
    "    elif method =='covariance':     \n",
    "        Yo = np.copy(Yk)\n",
    "        \n",
    "        norml = np.finfo(np.float64).max\n",
    "        i = 1\n",
    "\n",
    "        while i <= maxIter:\n",
    "            Rk = Yk - deltaS\n",
    "            Xk = _getPS(Rk, W)\n",
    "            deltaS = Xk - Rk\n",
    "            Yk = _getPu(Xk, W)\n",
    "            norm = wgtNorm(Yk - Yo, W)\n",
    "            minEigVal = np.min(np.linalg.eigvalsh(Yk))\n",
    "\n",
    "            if norm - norml < tol and minEigVal > -epsilon:\n",
    "                break\n",
    "            \n",
    "            norml = norm\n",
    "            i += 1\n",
    "\n",
    "        if i < maxIter:\n",
    "            print(\"Converged in {} iterations.\".format(i))\n",
    "        else:\n",
    "            print(\"Convergence failed after {} iterations\".format(i - 1))\n",
    "\n",
    "        # Add back the variance\n",
    "        if invSD is not None:\n",
    "            invSD = np.diag(1.0 / np.diag(invSD))\n",
    "            Yk = invSD @ Yk @ invSD\n",
    "        result = Yk\n",
    "    # set to dataframe \n",
    "    result = pd.DataFrame(Yk, columns=testout1_4.columns)\n",
    "    return result \n",
    "\n",
    "# test 3.3 \n",
    "output_name = 'testresult_3_3'\n",
    "method = 'covariance'\n",
    "result = higham_nearestPSD(testout1_3, method, epsilon=0.0)\n",
    "result.to_csv(output_name + \".csv\", index=False, header=True)\n",
    "print(f\"{calc_type} saved to {output_name}.csv\")\n",
    "testresult_3_3 = pd.read_csv(path_name + \"/\"+ output_name+\".csv\")\n",
    "testresult = testresult_3_3\n",
    "correctanswer =  testout3_3\n",
    "print(testfiles_equal(testresult,correctanswer))\n",
    "\n",
    "# test 3.4\n",
    "output_name = 'testresult_3_4'\n",
    "method = 'correlation'\n",
    "result = higham_nearestPSD(testout1_4, method, epsilon=0.0)\n",
    "result.to_csv(output_name + \".csv\", index=False, header=True)\n",
    "print(f\"{calc_type} saved to {output_name}.csv\")\n",
    "testresult_3_4 = pd.read_csv(path_name + \"/\"+ output_name+\".csv\")\n",
    "testresult = testresult_3_4\n",
    "correctanswer =  testout3_4\n",
    "print(testfiles_equal(testresult,correctanswer))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Cholesky decomposition for Positive Semi-Definite matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correlation saved to testresult_4_1.csv\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "def chol_psd(df):\n",
    "    df = df.to_numpy()\n",
    "    root = np.zeros_like(df)\n",
    "    n = df.shape[0]\n",
    "    # Initialize the root matrix with 0 values\n",
    "    root[:] = 0.0\n",
    "\n",
    "    # Loop over columns\n",
    "    for j in range(n):\n",
    "        s = 0.0\n",
    "        # If we are not on the first column, calculate the dot product of the preceding row values.\n",
    "        if j > 0:\n",
    "            s = root[j, :j] @ root[j, :j]\n",
    "\n",
    "        # Diagonal Element\n",
    "        temp = df[j, j] - s\n",
    "        if 0 >= temp >= -1e-8:\n",
    "            temp = 0.0\n",
    "        root[j, j] = np.sqrt(temp)\n",
    "\n",
    "        # Check for the 0 eigenvalue. Just set the column to 0 if we have one\n",
    "        if root[j, j] == 0.0:\n",
    "            root[j, j+1:n] = 0.0\n",
    "        else:\n",
    "            # Update off-diagonal rows of the column\n",
    "            ir = 1.0 / root[j, j]\n",
    "            for i in range(j+1, n):\n",
    "                s = root[i, :j] @ root[j, :j]\n",
    "                root[i, j] = (df[i, j] - s) * ir\n",
    "    result = pd.DataFrame(root, columns=testout3_1.columns)\n",
    "    return result \n",
    "\n",
    "# test 4.1 \n",
    "output_name = 'testresult_4_1'\n",
    "df = testout3_1\n",
    "result = chol_psd(df)\n",
    "result.to_csv(output_name + \".csv\", index=False, header=True)\n",
    "print(f\"{calc_type} saved to {output_name}.csv\")\n",
    "# 'root' now contains the Cholesky decomposition of 'a'\n",
    "testresult_4_1 = pd.read_csv(path_name + \"/\"+ output_name+\".csv\")\n",
    "testresult = testresult_4_1\n",
    "correctanswer =  testout4_1\n",
    "print(testfiles_equal(testresult,correctanswer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Normal Simulation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correlation saved to testresult_5_1.csv\n",
      "True\n",
      "correlation saved to testresult_5_2.csv\n",
      "True\n",
      "correlation saved to testresult_5_3.csv\n",
      "True\n",
      "Converged in 16 iterations.\n",
      "correlation saved to testresult_5_4.csv\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from scipy.stats import norm\n",
    "\n",
    "def simulate_normal(N, output_name, cov, fix_method=None, mean=None, seed=1234):\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Error Checking\n",
    "    n, m = cov.shape\n",
    "    if n != m:\n",
    "        raise ValueError(f\"Covariance Matrix is not square ({n},{m})\")\n",
    "    \n",
    "    out = np.empty((n, N))\n",
    "    \n",
    "    # If the mean is missing then set to 0, otherwise use provided mean\n",
    "    if mean is None:\n",
    "        _mean = np.zeros(n)\n",
    "    else:\n",
    "        if n != len(mean):\n",
    "            raise ValueError(f\"Mean ({len(mean)}) is not the size of cov ({n},{n})\")\n",
    "        _mean = np.array(mean)\n",
    "    new_cov = pd.DataFrame(cov) # convert to dataframe because my previous functions all take in dataframes \n",
    "    # Take the root\n",
    "    if fix_method == 'near_psd':\n",
    "        l = near_psd(new_cov, 'covariance') # feed in a dataframe, still. \n",
    "        # l = np.linalg.cholesky(l)\n",
    "        l = chol_psd(l).to_numpy() # convert back to array after function outputing. \n",
    "    elif fix_method == 'Higham':\n",
    "        l = higham_nearestPSD(new_cov, 'covariance')\n",
    "        l = chol_psd(l).to_numpy() # convert back to array after function outputing. \n",
    "    elif fix_method == 'cholesky':\n",
    "        l = chol_psd(new_cov)\n",
    "        l = np.tril(l.to_numpy())\n",
    "    else: \n",
    "        l = new_cov\n",
    "    \n",
    "   \n",
    "    # Generate needed random standard normals\n",
    "    d = norm.rvs(size=(N, n))\n",
    "    \n",
    "    # Apply the standard normals to the Cholesky root\n",
    "    out = l @ d.T\n",
    "\n",
    "    # Loop over iterations and add the mean\n",
    "    for i in range(n):\n",
    "        out[i, :] += _mean[i]\n",
    "    simulated_data = out.T\n",
    "    result = np.cov(simulated_data.T)\n",
    " \n",
    "    # result = np.cov(out)\n",
    "    \n",
    "    result = pd.DataFrame(result, columns=testout5_1.columns)\n",
    "    result.to_csv(output_name + \".csv\", index=False, header=True)\n",
    "    print(f\"{calc_type} saved to {output_name}.csv\")      \n",
    "\n",
    "# test 5.1  \n",
    "output_name = 'testresult_5_1'\n",
    "cov_matrix = test5_1.values \n",
    "# mean_vector = np.zeros(cov_matrix.shape[0])\n",
    "N=100000\n",
    "fix_method = 'cholesky'\n",
    "#Simulate 100,000 normal variables\n",
    "simulate_normal(N, output_name, cov_matrix, fix_method)\n",
    "testresult_5_1 = pd.read_csv(path_name + \"/\"+ output_name+\".csv\")\n",
    "testresult = testresult_5_1\n",
    "correctanswer =  testout5_1\n",
    "print(testfiles_equal(testresult,correctanswer))\n",
    "\n",
    "# test 5.2\n",
    "output_name = 'testresult_5_2'\n",
    "cov_matrix = test5_2.values \n",
    "# mean_vector = np.zeros(cov_matrix.shape[0])\n",
    "N=100000\n",
    "fix_method = 'cholesky'\n",
    "#Simulate 100,000 normal variables\n",
    "simulate_normal(N, output_name, cov_matrix, fix_method)\n",
    "testresult_5_2 = pd.read_csv(path_name + \"/\"+ output_name+\".csv\")\n",
    "testresult = testresult_5_2\n",
    "correctanswer =  testout5_2\n",
    "print(testfiles_equal(testresult,correctanswer))\n",
    "\n",
    "# test 5.3\n",
    "output_name = 'testresult_5_3'\n",
    "cov_matrix = test5_3.values \n",
    "N=100000\n",
    "fix_method = 'near_psd'\n",
    "#Simulate 100,000 normal variables\n",
    "simulate_normal(N, output_name, cov_matrix, fix_method)\n",
    "testresult_5_3 = pd.read_csv(path_name + \"/\"+ output_name+\".csv\")\n",
    "testresult = testresult_5_3\n",
    "correctanswer =  testout5_3\n",
    "print(testfiles_equal(testresult,correctanswer))\n",
    "\n",
    "# test 5.4\n",
    "output_name = 'testresult_5_4'\n",
    "cov_matrix = test5_3.values \n",
    "mean_vector = np.zeros(cov_matrix.shape[0])\n",
    "N=100000\n",
    "fix_method = 'Higham'\n",
    "#Simulate 100,000 normal variables\n",
    "simulate_normal(N, output_name, cov_matrix, fix_method)\n",
    "testresult_5_4 = pd.read_csv(path_name + \"/\"+ output_name+\".csv\")\n",
    "testresult = testresult_5_4\n",
    "correctanswer =  testout5_4\n",
    "print(testfiles_equal(testresult,correctanswer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "correlation saved to testresult_5_5.csv\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import eigh\n",
    "from scipy.stats import norm\n",
    "\n",
    "def simulate_pca(cov, nsim, pctExp=1.0, mean=None, seed=1234):\n",
    "    np.random.seed(seed)\n",
    "    n = cov.shape[0]\n",
    "    print(n)\n",
    "    # If the mean is missing then set to 0, otherwise use provided mean\n",
    "    if mean is None:\n",
    "        _mean = np.zeros(n)\n",
    "    else:\n",
    "        if len(mean) != n:\n",
    "            raise ValueError(\"Mean vector size does not match covariance matrix size\")\n",
    "        _mean = np.array(mean)\n",
    "    \n",
    "    # Eigenvalue decomposition\n",
    "    vals, vecs = eigh(cov)\n",
    "    \n",
    "    # Sort eigenvalues and eigenvectors in descending order\n",
    "    idx = np.argsort(vals)[::-1]\n",
    "    vals = vals[idx]\n",
    "    vecs = vecs[:, idx]\n",
    "    \n",
    "    # Keep only the positive eigenvalues (and corresponding eigenvectors)\n",
    "    posv = vals > 1e-8\n",
    "    vals = vals[posv]\n",
    "    vecs = vecs[:, posv]\n",
    "\n",
    "    # Calculate the cumulative variance explained and determine the number of components\n",
    "    tv = np.sum(vals)\n",
    "    cum_var_explained = np.cumsum(vals) / tv\n",
    "    nval = np.searchsorted(cum_var_explained, pctExp) + 1\n",
    "    vals = vals[:nval]\n",
    "    vecs = vecs[:, :nval]\n",
    "\n",
    "    # Calculate the matrix B for the factor loadings\n",
    "    B = vecs @ np.diag(np.sqrt(vals))\n",
    "\n",
    "    # Generate random normals\n",
    "    r = np.random.randn(nsim, nval)\n",
    "\n",
    "    # Simulate data and add the mean\n",
    "    out = (B @ r.T).T + _mean.reshape(1, -1)\n",
    "\n",
    "    # Return the covariance matrix of the simulated data\n",
    "    result=pd.DataFrame(out, columns = cov.columns).cov()\n",
    "    return result \n",
    "    \n",
    "\n",
    "cov_matrix = test5_2  # Replace test5_1 with your covariance matrix\n",
    "nsim = 100000  # Number of simulations\n",
    "pctExp = 0.99  # Percentage of total variance explained by the PCA\n",
    "mean_vector = None  # Mean vector (optional)\n",
    "seed = 1234  # Random seed for reproducibility\n",
    "\n",
    "output_name= 'testresult_5_5'\n",
    "result = simulate_pca(cov_matrix, nsim, pctExp=pctExp, mean=mean_vector, seed=seed)\n",
    "result.to_csv(output_name + \".csv\", index=False, header=True)\n",
    "print(f\"{calc_type} saved to {output_name}.csv\")  \n",
    "testresult_5_5 = pd.read_csv(path_name + \"/\"+ output_name+\".csv\")\n",
    "testresult = testresult_5_5\n",
    "correctanswer =  testout5_5\n",
    "print(testfiles_equal(testresult,correctanswer))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Calculate Arithmetic Returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correlation saved to testresult_6_1.csv\n",
      "True\n",
      "correlation saved to testresult_6_2.csv\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def calculate_returns(prices, method=\"DISCRETE\", date_column=\"Date\"):\n",
    "    if date_column not in prices.columns:\n",
    "        raise ValueError(f\"dateColumn: {date_column} not in DataFrame\")\n",
    "    \n",
    "    prices = prices.set_index(date_column)\n",
    "    returns = prices.pct_change().dropna()  # This gives arithmetic returns\n",
    "    \n",
    "    if method.upper() == \"LOG\":\n",
    "        returns = np.log(1 + returns)  # Convert to log returns\n",
    "    \n",
    "    # returns.reset_index(inplace=True)\n",
    "    returns.columns = prices.columns\n",
    "\n",
    "    return returns\n",
    "\n",
    "# Example usage:\n",
    "df = test6\n",
    "arithmetic_returns = calculate_returns(df, method=\"DISCRETE\", date_column=\"Date\")\n",
    "log_returns = calculate_returns(df, method=\"LOG\", date_column=\"Date\")\n",
    "\n",
    "output_name = 'testresult_6_1'\n",
    "arithmetic_returns.to_csv(output_name + \".csv\", index=True, index_label='Date', header=True)\n",
    "print(f\"{calc_type} saved to {output_name}.csv\")  \n",
    "testresult_6_1 = pd.read_csv(path_name + \"/\"+ output_name+\".csv\")\n",
    "testresult = testresult_6_1\n",
    "correctanswer =  testout6_1\n",
    "print(testfiles_equal(testresult,correctanswer))\n",
    "\n",
    "output_name = 'testresult_6_2'\n",
    "log_returns.to_csv(output_name + \".csv\", index=True, index_label='Date', header=True)\n",
    "print(f\"{calc_type} saved to {output_name}.csv\")  \n",
    "testresult_6_2 = pd.read_csv(path_name + \"/\"+ output_name+\".csv\")\n",
    "testresult = testresult_6_2\n",
    "correctanswer =  testout6_2\n",
    "print(testfiles_equal(testresult,correctanswer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Non-PSD fixes for correlation matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'timeit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 77\u001b[0m\n\u001b[1;32m     74\u001b[0m sigma[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.7357\u001b[39m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m# Time the near_psd function\u001b[39;00m\n\u001b[0;32m---> 77\u001b[0m near_psd_time \u001b[38;5;241m=\u001b[39m \u001b[43mtimeit\u001b[49m\u001b[38;5;241m.\u001b[39mtimeit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnear_psd(sigma)\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mglobals\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mglobals\u001b[39m(), number\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnear_psd function executed in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnear_psd_time\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# Time Higham's method\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'timeit' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "def near_psd(a, epsilon=0.0):\n",
    "    n = a.shape[0]\n",
    "    invSD = None\n",
    "    out = a.copy()\n",
    "\n",
    "    # Calculate the correlation matrix if we got a covariance matrix\n",
    "    if not np.all(np.isclose(np.diag(out), 1.0)):\n",
    "        invSD = np.diag(1.0 / np.sqrt(np.diag(out)))\n",
    "        out = invSD @ out @ invSD\n",
    "\n",
    "    # SVD, update the eigenvalue and scale\n",
    "    vals, vecs = eigh(out)\n",
    "    vals = np.maximum(vals, epsilon)\n",
    "    T = np.diag(1.0 / np.sqrt(np.sum(vecs ** 2 * vals, axis=0)))\n",
    "    l = np.diag(np.sqrt(vals))\n",
    "    B = T @ vecs @ l\n",
    "    out = B @ B.T\n",
    "\n",
    "    # Add back the variance\n",
    "    if invSD is not None:\n",
    "        invSD = np.diag(1.0 / np.diag(invSD))\n",
    "        out = invSD @ out @ invSD\n",
    "\n",
    "    return out\n",
    "\n",
    "def higham_nearestPSD(A, epsilon=1e-10, maxIter=100, tol=1e-5):\n",
    "    n = A.shape[0]\n",
    "    W = np.identity(n)\n",
    "    deltaS = 0\n",
    "\n",
    "    Yk = A.copy()\n",
    "    norml = np.finfo(np.float64).max\n",
    "    for k in range(maxIter):\n",
    "        Rk = Yk - deltaS\n",
    "        Xk = _getPS(Rk, W)\n",
    "        deltaS = Xk - Rk\n",
    "        Yk = _getPu(Xk, W)\n",
    "        norm = np.linalg.norm(Yk - A, 'fro')\n",
    "\n",
    "        if np.abs(norm - norml) < tol:\n",
    "            break\n",
    "        norml = norm\n",
    "\n",
    "    if k == maxIter - 1:\n",
    "        print(\"Convergence failed after {} iterations\".format(k))\n",
    "    else:\n",
    "        print(\"Converged in {} iterations.\".format(k + 1))\n",
    "    return Yk\n",
    "\n",
    "def _getAplus(A):\n",
    "    eigval, eigvec = eigh(A)\n",
    "    Q = np.maximum(eigval, 0)\n",
    "    return eigvec @ np.diag(Q) @ eigvec.T\n",
    "\n",
    "def _getPS(A, W):\n",
    "    W05 = sqrtm(W)\n",
    "    iW = inv(W05)\n",
    "    return iW @ _getAplus(W05 @ A @ W05) @ iW\n",
    "\n",
    "def _getPu(A, W):\n",
    "    Aret = A.copy()\n",
    "    Aret[np.diag_indices_from(A)] = 1\n",
    "    return Aret\n",
    "\n",
    "def wgtNorm(A, W):\n",
    "    W05 = sqrtm(W)\n",
    "    return np.sum((W05 @ A @ W05) ** 2)\n",
    "\n",
    "# Example usage\n",
    "n = 500\n",
    "sigma = np.full((n, n), 0.9)\n",
    "np.fill_diagonal(sigma, 1.0)\n",
    "sigma[0, 1] = 0.7357\n",
    "sigma[1, 0] = 0.7357\n",
    "\n",
    "# Time the near_psd function\n",
    "near_psd_time = timeit.timeit('near_psd(sigma)', globals=globals(), number=10)\n",
    "print(f\"near_psd function executed in {near_psd_time} seconds\")\n",
    "\n",
    "# Time Higham's method\n",
    "higham_time = timeit.timeit('higham_nearestPSD(sigma)', globals=globals(), number=10)\n",
    "print(f\"Higham's method executed in {higham_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Simulation Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "# Define the number of simulations and the value of sigma\n",
    "num_simulations = 10000\n",
    "sigma = 0.1  # Assumed standard deviation of returns for simulation\n",
    "\n",
    "# Assume a starting price p_t_minus_1\n",
    "p_t_minus_1 = 100  # Assumed starting price for simulation\n",
    "\n",
    "# Generate random normal returns\n",
    "r_t = np.random.normal(0, sigma, num_simulations)\n",
    "\n",
    "# Calculate prices for each method\n",
    "prices_classical_bm = p_t_minus_1 + r_t\n",
    "prices_arithmetic_return = p_t_minus_1 * (1 + r_t)\n",
    "prices_log_return = p_t_minus_1 * np.exp(r_t)\n",
    "\n",
    "# Calculate mean and standard deviation for each method\n",
    "mean_classical_bm = np.mean(prices_classical_bm)\n",
    "std_classical_bm = np.std(prices_classical_bm)\n",
    "\n",
    "mean_arithmetic_return = np.mean(prices_arithmetic_return)\n",
    "std_arithmetic_return = np.std(prices_arithmetic_return)\n",
    "\n",
    "mean_log_return = np.mean(prices_log_return)\n",
    "std_log_return = np.std(prices_log_return)\n",
    "\n",
    "result = (\n",
    "    f'Classical Brownian Motion: Mean = {mean_classical_bm:.2f}, '\n",
    "    f'SD = {std_classical_bm:.2f}\\n'\n",
    "    f'Arithmetic Return System: Mean = {mean_arithmetic_return:.2f}, '\n",
    "    f'SD = {std_arithmetic_return:.2f}\\n'\n",
    "    f'Log Return (Geometric Brownian Motion): Mean = {mean_log_return:.2f}, '\n",
    "    f'SD = {std_log_return:.2f}'\n",
    ")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. VaR calculation methods (all discussed) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will now write the Python code to calculate the Value at Risk (VaR) for the META stock returns\n",
    "# at the 95% confidence level using the different methods specified.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import norm, t\n",
    "from statsmodels.tsa.ar_model import AutoReg\n",
    "import scipy\n",
    "\n",
    "# Load the data\n",
    "daily_prices_df = pd.read_csv('/Users/angelaliang/Documents/fintech545/Week04/DailyPrices.csv')\n",
    "daily_prices_df.set_index('Date', inplace=True)\n",
    "\n",
    "# Calculate arithmetic returns\n",
    "arithmetic_returns = daily_prices_df.pct_change().dropna()\n",
    "\n",
    "# Remove the mean from the META returns to have a mean of 0\n",
    "arithmetic_returns['META'] = arithmetic_returns['META'] - arithmetic_returns['META'].mean()\n",
    "\n",
    "# Confidence level\n",
    "confidence_level = 0.95\n",
    "alpha = 1 - confidence_level\n",
    "\n",
    "# VaR using a normal distribution\n",
    "mean = arithmetic_returns['META'].mean()\n",
    "std_dev = arithmetic_returns['META'].std()\n",
    "var_normal = norm.ppf(alpha, mean, std_dev)\n",
    "\n",
    "# VaR using a normal distribution with an Exponentially Weighted variance (lambda = 0.94)\n",
    "lambda_param = 0.94\n",
    "ewma_variance = arithmetic_returns['META'].ewm(alpha=(1 - lambda_param)).var().iloc[-1]\n",
    "var_normal_ewma = norm.ppf(alpha, 0, np.sqrt(ewma_variance))\n",
    "\n",
    "# VaR using a MLE fitted T distribution\n",
    "params = t.fit(arithmetic_returns['META'])\n",
    "var_t = t.ppf(alpha, *params)\n",
    "\n",
    "# VaR using a fitted AR(1) model\n",
    "ar_model = AutoReg(arithmetic_returns['META'], lags=1).fit()\n",
    "forecast = ar_model.predict(start=len(arithmetic_returns), end=len(arithmetic_returns))\n",
    "simulated_returns = np.random.normal(forecast, ar_model.resid.std(), 10000)\n",
    "var_ar1 = np.percentile(simulated_returns, alpha * 100)\n",
    "\n",
    "# VaR using a Historic Simulation\n",
    "var_historic = np.percentile(arithmetic_returns['META'], alpha * 100)\n",
    "var_results = {\n",
    "    \"Normal Distribution\": var_normal,\n",
    "    \"Normal EWMA\": var_normal_ewma,\n",
    "    \"MLE T Distribution\": var_t,\n",
    "    \"AR(1) Model\": var_ar1,\n",
    "    \"Historic Simulation\": var_historic\n",
    "}\n",
    "# Return all the VaR values calculated\n",
    "var_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exponentially Weighted Moving Covariance (EWMC) Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "def calculate_ewm_cov(portfolio_df, daily_returns_df, lambda_value):\n",
    "    ewm_cov_by_portfolio = {}\n",
    "    span = (2 / (1 - lambda_value)) - 1\n",
    "\n",
    "    for portfolio_label in portfolio_df['Portfolio'].unique():\n",
    "        stocks = portfolio_df[portfolio_df['Portfolio'] == portfolio_label]['Stock']\n",
    "        portfolio_returns = daily_returns_df[stocks]\n",
    "        ewm_cov = portfolio_returns.ewm(span=span).cov()\n",
    "        last_day_cov_matrix = ewm_cov.loc[daily_returns_df.index[-1]]\n",
    "        ewm_cov_by_portfolio[portfolio_label] = last_day_cov_matrix\n",
    "    \n",
    "    return ewm_cov_by_portfolio\n",
    "\n",
    "def calculate_var(ewm_cov_by_portfolio, portfolio_df, daily_prices_df, z_score):\n",
    "    portfolio_var = {}\n",
    "    last_day_prices = daily_prices_df.iloc[-1]\n",
    "\n",
    "    for portfolio_label in portfolio_df['Portfolio'].unique():\n",
    "        holdings = portfolio_df[portfolio_df['Portfolio'] == portfolio_label]\n",
    "        last_day_cov_matrix = ewm_cov_by_portfolio[portfolio_label]\n",
    "\n",
    "        # Calculate the dollar value of each holding\n",
    "        holdings_values = holdings.set_index('Stock')['Holding'] * last_day_prices.reindex(holdings['Stock']).fillna(0)\n",
    "\n",
    "        # Calculate the total dollar value of the portfolio\n",
    "        portfolio_value = np.sum(holdings_values)\n",
    "\n",
    "        # Calculate the proportion (weight) of each holding in the portfolio\n",
    "        delta = holdings_values / portfolio_value\n",
    "\n",
    "        # Calculate the portfolio variance using the weights (proportions)\n",
    "        portfolio_variance = delta.T @ last_day_cov_matrix @ delta\n",
    "        portfolio_std = np.sqrt(portfolio_variance)\n",
    "\n",
    "        # Calculate VaR at 95% confidence level\n",
    "        VaR_dollar = z_score * portfolio_std * portfolio_value\n",
    "        portfolio_var[portfolio_label] = VaR_dollar\n",
    "\n",
    "    return portfolio_var\n",
    "\n",
    "\n",
    "# Constants\n",
    "LAMBDA = 0.94\n",
    "Z_SCORE = norm.ppf(1 - 0.05)\n",
    "\n",
    "# File paths (consider using relative paths or arguments)\n",
    "portfolio_path = '/Users/angelaliang/Documents/fintech545/Week04/Project/portfolio.csv'\n",
    "daily_prices_path = '/Users/angelaliang/Documents/fintech545/Week04/DailyPrices.csv'\n",
    "\n",
    "# Read the data\n",
    "portfolio_df = pd.read_csv(portfolio_path)\n",
    "daily_prices_df = pd.read_csv(daily_prices_path)\n",
    "daily_prices_df.set_index('Date', inplace=True)\n",
    "daily_returns_df = daily_prices_df.pct_change().dropna()\n",
    "\n",
    "# Calculate exponentially weighted covariance matrix\n",
    "ewm_cov_by_portfolio = calculate_ewm_cov(portfolio_df, daily_returns_df, LAMBDA)\n",
    "\n",
    "# Calculate VaR for each portfolio\n",
    "portfolio_var = calculate_var(ewm_cov_by_portfolio, portfolio_df, daily_prices_df, Z_SCORE)\n",
    "\n",
    "# Output the VaR as a dictionary\n",
    "print(portfolio_var, sum(portfolio_var.values()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Historical Simulation Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_historical_var(holdings_values, historical_returns, confidence_level):\n",
    "    # Calculate portfolio historical returns\n",
    "    portfolio_historical_returns = (historical_returns * holdings_values).sum(axis=1)\n",
    "    \n",
    "    # Calculate the VaR as the percentile of the historical returns\n",
    "    VaR = np.percentile(portfolio_historical_returns, (1 - confidence_level) * 100)\n",
    "    return VaR\n",
    "\n",
    "def calculate_historical_var_by_portfolio(portfolio_df, daily_prices_df, confidence_level):\n",
    "    historical_returns = daily_prices_df.pct_change().dropna()\n",
    "    last_day_prices = daily_prices_df.iloc[-1]\n",
    "    portfolio_var = {}\n",
    "\n",
    "    for portfolio_label in portfolio_df['Portfolio'].unique():\n",
    "        holdings = portfolio_df[portfolio_df['Portfolio'] == portfolio_label]\n",
    "        \n",
    "        # Calculate the dollar value of each holding\n",
    "        holdings_values = holdings.set_index('Stock')['Holding'] * last_day_prices.reindex(holdings['Stock']).fillna(0)\n",
    "\n",
    "        # Calculate historical VaR\n",
    "        VaR_dollar = calculate_historical_var(holdings_values, historical_returns, confidence_level)\n",
    "        portfolio_var[portfolio_label] = VaR_dollar\n",
    "\n",
    "    return portfolio_var\n",
    "\n",
    "# Constants\n",
    "CONFIDENCE_LEVEL = 0.95\n",
    "\n",
    "# File paths (consider using relative paths or arguments)\n",
    "portfolio_path = '/Users/angelaliang/Documents/fintech545/Week04/Project/portfolio.csv'\n",
    "daily_prices_path = '/Users/angelaliang/Documents/fintech545/Week04/DailyPrices.csv'\n",
    "\n",
    "# Read the data\n",
    "portfolio_df = pd.read_csv(portfolio_path)\n",
    "daily_prices_df = pd.read_csv(daily_prices_path)\n",
    "daily_prices_df.set_index('Date', inplace=True)\n",
    "\n",
    "# Calculate historical VaR for each portfolio\n",
    "portfolio_var = calculate_historical_var_by_portfolio(portfolio_df, daily_prices_df, CONFIDENCE_LEVEL)\n",
    "\n",
    "# Output the VaR as a dictionary\n",
    "portfolio_var, sum(portfolio_var.values())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. ES calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function for Expected Shortfall (ES) calculation.\n",
    "def calculate_expected_shortfall(returns, confidence_level):\n",
    "    \"\"\"\n",
    "    Calculate the Expected Shortfall (ES) at a specified confidence level.\n",
    "\n",
    "    Parameters:\n",
    "    returns (list): A list of returns (or losses) for the asset/portfolio.\n",
    "    confidence_level (float): The confidence level for VaR and ES (e.g., 0.95 for 95%).\n",
    "\n",
    "    Returns:\n",
    "    float: The Expected Shortfall value.\n",
    "    \"\"\"\n",
    "    # Sort returns from smallest to largest\n",
    "    sorted_returns = sorted(returns)\n",
    "    \n",
    "    # Calculate the index for VaR\n",
    "    var_index = int((1 - confidence_level) * len(sorted_returns))\n",
    "    \n",
    "    # Calculate VaR at the specified confidence level\n",
    "    value_at_risk = sorted_returns[var_index]\n",
    "    \n",
    "    # Calculate ES as the mean of the losses beyond the VaR level\n",
    "    expected_shortfall = sum(sorted_returns[:var_index]) / var_index if var_index != 0 else 0\n",
    "\n",
    "    return expected_shortfall\n",
    "\n",
    "# Example usage (with dummy data):\n",
    "# confidence_level = 0.95\n",
    "# returns = [-0.02, -0.01, 0.00, 0.01, 0.02]  # Replace with actual returns\n",
    "# es_value = calculate_expected_shortfall(returns, confidence_level)\n",
    "# print(es_value)\n",
    "\n",
    "# Since this is an example, the actual function call is commented out. You would replace\n",
    "# the `returns` list with your actual data and specify your desired confidence level.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Calculate VaR and ES given data problem1.csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal Distribution: VaR=-0.0903, ES=-0.0060\n",
      "T Distribution: VaR=-0.0765, ES=-0.0059\n",
      "Historical Simulation: VaR=-0.0759, ES=-0.1168\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import t, norm\n",
    "\n",
    "# Load data here\n",
    "df = pd.read_csv('/Users/angelaliang/Documents/fintech545/Week05/Project/problem1.csv')\n",
    "returns = df['x'].values\n",
    "\n",
    "\n",
    "# Define confidence level and lambda for EWMA\n",
    "confidence_level = 0.95\n",
    "lambda_ = 0.97\n",
    "\n",
    "# Calculate exponentially weighted moving average variance\n",
    "ewma_variance = lambda returns, lambda_: (\n",
    "    np.sum(((lambda_ ** np.arange(len(returns)))[::-1] * (returns - np.mean(returns)) ** 2)) / \n",
    "    np.sum(lambda_ ** np.arange(len(returns)))\n",
    ")\n",
    "\n",
    "# Function to calculate VaR and ES for normal distribution with EWMA\n",
    "def var_es_normal(returns, confidence_level, lambda_):\n",
    "    sigma = np.sqrt(ewma_variance(returns, lambda_))\n",
    "    var = norm.ppf(1 - confidence_level) * sigma\n",
    "    es = -norm.expect(lambda x: x, loc=0, scale=sigma, lb=var) / confidence_level\n",
    "    return var, es\n",
    "\n",
    "# Function to calculate VaR and ES for T distribution\n",
    "def var_es_t(returns, confidence_level):\n",
    "    params = t.fit(returns)\n",
    "    var = t.ppf(1 - confidence_level, *params)\n",
    "    es = -t.expect(lambda x: x, args=(params[0],), loc=params[1], scale=params[2], lb=var) / confidence_level\n",
    "    return var, es\n",
    "\n",
    "# Function to calculate VaR and ES for historical simulation\n",
    "def var_es_historical(returns, confidence_level):\n",
    "    sorted_returns = np.sort(returns)\n",
    "    var_index = int((1 - confidence_level) * len(sorted_returns))\n",
    "    var = sorted_returns[var_index]\n",
    "    es = np.mean(sorted_returns[:var_index])\n",
    "    return var, es\n",
    "\n",
    "# Calculate VaR and ES using all three methods\n",
    "var_normal, es_normal = var_es_normal(returns, confidence_level, lambda_)\n",
    "var_t, es_t = var_es_t(returns, confidence_level)\n",
    "var_historical, es_historical = var_es_historical(returns, confidence_level)\n",
    "\n",
    "# Print the results\n",
    "print(f'Normal Distribution: VaR={var_normal:.4f}, ES={es_normal:.4f}')\n",
    "print(f'T Distribution: VaR={var_t:.4f}, ES={es_t:.4f}')\n",
    "print(f'Historical Simulation: VaR={var_historical:.4f}, ES={es_historical:.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
