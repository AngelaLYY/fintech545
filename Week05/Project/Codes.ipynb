{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary packages\n",
    "import os\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from scipy.stats import norm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import all test files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = pd.read_csv(\"/Users/angelaliang/Documents/fintech545/testfiles/data/test1.csv\")\n",
    "testout1_1 = pd.read_csv(\"/Users/angelaliang/Documents/fintech545/testfiles/data/testout_1.1.csv\")\n",
    "testout1_2 =pd.read_csv(\"/Users/angelaliang/Documents/fintech545/testfiles/data/testout_1.2.csv\")\n",
    "testout1_3 = pd.read_csv(\"/Users/angelaliang/Documents/fintech545/testfiles/data/testout_1.3.csv\")\n",
    "testout1_4 = pd.read_csv(\"/Users/angelaliang/Documents/fintech545/testfiles/data/testout_1.4.csv\")\n",
    "\n",
    "test2 = pd.read_csv(\"/Users/angelaliang/Documents/fintech545/testfiles/data/test2.csv\")\n",
    "testout2_1 = pd.read_csv(\"/Users/angelaliang/Documents/fintech545/testfiles/data/testout_2.1.csv\")\n",
    "testout2_2 = pd.read_csv(\"/Users/angelaliang/Documents/fintech545/testfiles/data/testout_2.2.csv\")\n",
    "testout2_3 = pd.read_csv(\"/Users/angelaliang/Documents/fintech545/testfiles/data/testout_2.3.csv\")\n",
    "\n",
    "testout3_1 = pd.read_csv(\"/Users/angelaliang/Documents/fintech545/testfiles/data/testout_3.1.csv\")\n",
    "testout3_2 = pd.read_csv(\"/Users/angelaliang/Documents/fintech545/testfiles/data/testout_3.2.csv\")\n",
    "testout3_3 = pd.read_csv(\"/Users/angelaliang/Documents/fintech545/testfiles/data/testout_3.3.csv\")\n",
    "testout3_4 = pd.read_csv(\"/Users/angelaliang/Documents/fintech545/testfiles/data/testout_3.4.csv\")\n",
    "\n",
    "testout4_1 = pd.read_csv(\"/Users/angelaliang/Documents/fintech545/testfiles/data/testout_4.1.csv\")\n",
    "\n",
    "test5_1 = pd.read_csv(\"/Users/angelaliang/Documents/fintech545/testfiles/data/test5_1.csv\")\n",
    "test5_2 = pd.read_csv(\"/Users/angelaliang/Documents/fintech545/testfiles/data/test5_2.csv\")\n",
    "test5_3 = pd.read_csv(\"/Users/angelaliang/Documents/fintech545/testfiles/data/test5_3.csv\")\n",
    "testout5_1 = pd.read_csv(\"/Users/angelaliang/Documents/fintech545/testfiles/data/testout_5.1.csv\")\n",
    "testout5_2 = pd.read_csv(\"/Users/angelaliang/Documents/fintech545/testfiles/data/testout_5.2.csv\")\n",
    "testout5_3 = pd.read_csv(\"/Users/angelaliang/Documents/fintech545/testfiles/data/testout_5.3.csv\")\n",
    "testout5_4 = pd.read_csv(\"/Users/angelaliang/Documents/fintech545/testfiles/data/testout_5.4.csv\")\n",
    "testout5_5 = pd.read_csv(\"/Users/angelaliang/Documents/fintech545/testfiles/data/testout_5.5.csv\")\n",
    "\n",
    "test6 = pd.read_csv(\"/Users/angelaliang/Documents/fintech545/testfiles/data/test6.csv\")\n",
    "testout6_1 = pd.read_csv(\"/Users/angelaliang/Documents/fintech545/testfiles/data/test6_1.csv\")\n",
    "testout6_2 = pd.read_csv(\"/Users/angelaliang/Documents/fintech545/testfiles/data/test6_2.csv\")\n",
    "\n",
    "test7_1 = pd.read_csv(\"/Users/angelaliang/Documents/fintech545/testfiles/data/test7_1.csv\")\n",
    "test7_2 = pd.read_csv(\"/Users/angelaliang/Documents/fintech545/testfiles/data/test7_2.csv\")\n",
    "test7_3 = pd.read_csv(\"/Users/angelaliang/Documents/fintech545/testfiles/data/test7_3.csv\")\n",
    "testout7_1 = pd.read_csv(\"/Users/angelaliang/Documents/fintech545/testfiles/data/testout7_1.csv\")\n",
    "testout7_2 = pd.read_csv(\"/Users/angelaliang/Documents/fintech545/testfiles/data/testout7_2.csv\")\n",
    "testout7_3 = pd.read_csv(\"/Users/angelaliang/Documents/fintech545/testfiles/data/testout7_3.csv\")\n",
    "\n",
    "testout8_1 = pd.read_csv(\"/Users/angelaliang/Documents/fintech545/testfiles/data/testout8_1.csv\")\n",
    "testout8_2 = pd.read_csv(\"/Users/angelaliang/Documents/fintech545/testfiles/data/testout8_2.csv\")\n",
    "testout8_3 = pd.read_csv(\"/Users/angelaliang/Documents/fintech545/testfiles/data/testout8_3.csv\")\n",
    "testout8_4 = pd.read_csv(\"/Users/angelaliang/Documents/fintech545/testfiles/data/testout8_4.csv\")\n",
    "testout8_5 = pd.read_csv(\"/Users/angelaliang/Documents/fintech545/testfiles/data/testout8_5.csv\")\n",
    "testout8_6 = pd.read_csv(\"/Users/angelaliang/Documents/fintech545/testfiles/data/testout8_6.csv\")\n",
    "\n",
    "\n",
    "test9_1_portfolio = pd.read_csv(\"/Users/angelaliang/Documents/fintech545/testfiles/data/test9_1_portfolio.csv\")\n",
    "test9_1_returns = pd.read_csv(\"/Users/angelaliang/Documents/fintech545/testfiles/data/test9_1_returns.csv\")\n",
    "testout9_1= pd.read_csv(\"/Users/angelaliang/Documents/fintech545/testfiles/data/testout9_1.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read path name for all locally saved files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_name = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test File Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check if two files are equal with small tolerance set \n",
    "# test and see if results are the same \n",
    "def testfiles_equal(df1, df2):\n",
    "    if df1.equals(df2):\n",
    "        return True\n",
    "    else:\n",
    "        # If they are not equal, it could be due to floating point precision,\n",
    "        # so we attempt a comparison that allows for a small absolute difference.\n",
    "        try:\n",
    "            pd.testing.assert_frame_equal(df1, df2, atol=1e-2)  # Use an appropriate tolerance\n",
    "            return True\n",
    "        except AssertionError:\n",
    "            return False\n",
    "        \n",
    "# Function to check if two files are equal with small tolerance set \n",
    "# test and see if results are the same \n",
    "def testfiles_equal_largeTol(df1, df2):\n",
    "    if df1.equals(df2):\n",
    "        return True\n",
    "    else:\n",
    "        # If they are not equal, it could be due to floating point precision,\n",
    "        # so we attempt a comparison that allows for a small absolute difference.\n",
    "        try:\n",
    "            pd.testing.assert_frame_equal(df1, df2, atol=1)  # Use an appropriate tolerance\n",
    "            return True\n",
    "        except AssertionError:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Covariance estimation techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With missing data, calculate correlation and covariance using methods:\n",
    "1) skip missing rows\n",
    "2) pairwise calculation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_correlation_covariance(df, method, calc_type, output_name):\n",
    "    \n",
    "    # Dropping all rows with any missing values if method is 'dropmissing'\n",
    "      if method == 'dropmissing':\n",
    "            df_cleaned = df.dropna()\n",
    "            if calc_type=='correlation':\n",
    "                  result = df_cleaned.corr(method='pearson')\n",
    "            if calc_type=='covariance':\n",
    "                  result = df_cleaned.cov(min_periods=1)\n",
    "\n",
    "\n",
    "      elif method == 'pairwise':\n",
    "            if calc_type=='correlation':\n",
    "                  result = df.corr(method='pearson')\n",
    "            if calc_type=='covariance':\n",
    "                  result = df.cov(min_periods=1)\n",
    "                                    \n",
    "                  \n",
    "      result_reset = result.reset_index(drop=True) # reset row index, drop and set as header\n",
    "      result_reset.to_csv(output_name+\".csv\", index=False, header=True)\n",
    "      print(f\"{calc_type} saved to {output_name}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "covariance saved to testresult_1.1\n",
      "True\n",
      "correlation saved to testresult_1.2\n",
      "True\n",
      "covariance saved to testresult_1.3\n",
      "True\n",
      "correlation saved to testresult_1.4\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# testing \n",
    "df = test1 # same for the first 4 tests \n",
    "\n",
    "# test 1.1 \n",
    "method = 'dropmissing'\n",
    "calc_type = 'covariance'\n",
    "output_name = 'testresult_1.1'\n",
    "calculate_correlation_covariance(df, method, calc_type, output_name)\n",
    "# testresult_1_1 = pd.read_csv(path_name + \"/\"+ output_name+\".csv\", header=None, index_col=0)\n",
    "testresult_1_1 = pd.read_csv(path_name + \"/\"+ output_name+\".csv\")  # header is inferred by default\n",
    "testresult = testresult_1_1\n",
    "correctanswer =  testout1_1\n",
    "print(testfiles_equal(testresult,correctanswer))\n",
    "\n",
    "# test 1.2 \n",
    "method = 'dropmissing'\n",
    "calc_type = 'correlation'\n",
    "output_name = 'testresult_1.2'\n",
    "calculate_correlation_covariance(df, method, calc_type, output_name)\n",
    "# testresult_1_2 = pd.read_csv(path_name + \"/\"+ output_name+\".csv\", header=None, index_col=0)\n",
    "testresult_1_2 = pd.read_csv(path_name + \"/\"+ output_name+\".csv\")\n",
    "testresult = testresult_1_2\n",
    "correctanswer =  testout1_2\n",
    "print(testfiles_equal(testresult,correctanswer))\n",
    "\n",
    "# test 1.3\n",
    "method = 'pairwise'\n",
    "calc_type = 'covariance'\n",
    "output_name = 'testresult_1.3'\n",
    "calculate_correlation_covariance(df, method, calc_type, output_name)\n",
    "# testresult_1_3 = pd.read_csv(path_name + \"/\"+ output_name+\".csv\", header=None, index_col=0)\n",
    "testresult_1_3 = pd.read_csv(path_name + \"/\"+ output_name+\".csv\")\n",
    "testresult = testresult_1_3\n",
    "correctanswer =  testout1_3\n",
    "print(testfiles_equal(testresult,correctanswer))\n",
    "\n",
    "# test 1.4\n",
    "method = 'pairwise'\n",
    "calc_type = 'correlation'\n",
    "output_name = 'testresult_1.4'\n",
    "calculate_correlation_covariance(df, method, calc_type, output_name)\n",
    "# testresult_1_4 = pd.read_csv(path_name + \"/\"+ output_name+\".csv\", header=None, index_col=0)\n",
    "testresult_1_4 = pd.read_csv(path_name + \"/\"+ output_name+\".csv\")\n",
    "testresult = testresult_1_4\n",
    "correctanswer =  testout1_4\n",
    "print(testfiles_equal(testresult,correctanswer))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. EW Covariance and Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_ew_correlation_covariance(df, calc_type, output_name, lambda_param):\n",
    "      \n",
    "      if lambda_param is None:\n",
    "            raise ValueError(\"For 'ew' method, lambda_param must be provided.\")\n",
    "      \n",
    "      n = len(df)\n",
    "      weights = np.array([(1 - lambda_param) * (lambda_param ** (n - 1 - i)) for i in range(n)])\n",
    "      weights /= weights.sum()\n",
    "      demeaned_data = df - df.mean()\n",
    "      \n",
    "      # Compute the exponentially weighted covariance matrix\n",
    "      ew_cov_matrix = (demeaned_data.T * weights).dot(demeaned_data) / weights.sum()\n",
    "      \n",
    "      if calc_type == 'correlation':\n",
    "            # Calculate the exponentially weighted correlation matrix\n",
    "            # Standard deviations are computed with the same weights used for covariance\n",
    "            std_dev = np.sqrt(np.diag(ew_cov_matrix))\n",
    "            # To ensure the standard deviations are non-zero to prevent division by zero errors\n",
    "            std_dev[std_dev == 0] = np.finfo(np.float64).tiny\n",
    "            ew_corr_matrix = ew_cov_matrix / np.outer(std_dev, std_dev)\n",
    "            result = ew_corr_matrix\n",
    "\n",
    "      elif calc_type == 'covariance':\n",
    "            result = ew_cov_matrix\n",
    "\n",
    "      # Reset index to ensure a clean DataFrame, suitable for CSV output\n",
    "      result_reset = result.reset_index(drop=True)\n",
    "      # Save to CSV\n",
    "      result_reset.to_csv(output_name + \".csv\", index=False, header=True)\n",
    "      print(f\"{calc_type} saved to {output_name}.csv\")\n",
    "\n",
    "\n",
    "def calculate_ew_sd(df, output_name, lambda_param1,lambda_param2):\n",
    "      # Inside your calculate_correlation_covariance function\n",
    "      if lambda_param1 is None or lambda_param2 is None: \n",
    "            raise ValueError(\"For 'ew' method, lambda_param must be provided for both covariance and correlation.\")\n",
    "      n = len(df)\n",
    "      weights1 = np.array([(1 - lambda_param1) * (lambda_param1 ** (n - 1 - i)) for i in range(n)])\n",
    "      weights1 /= weights1.sum()\n",
    "      demeaned_data1 = df - df.mean()\n",
    "      ew_cov_matrix1 = (demeaned_data1.T * weights1).dot(demeaned_data1) / weights1.sum()\n",
    "      std_dev1 = np.sqrt(np.diag(ew_cov_matrix1)) # 0.97 \n",
    "      \n",
    "      weights2 = np.array([(1 - lambda_param2) * (lambda_param2 ** (n - 1 - i)) for i in range(n)])\n",
    "      weights2 /= weights2.sum()\n",
    "      demeaned_data2 = df - df.mean()\n",
    "      ew_cov_matrix2 = (demeaned_data2.T * weights2).dot(demeaned_data2) / weights2.sum()\n",
    "      std_dev2 = np.sqrt(np.diag(ew_cov_matrix2)) # 0.94 \n",
    "\n",
    "      # if calc_type == 'correlation':\n",
    "      # Use the covariance matrix to calculate correlation matrix\n",
    "      # You may need to adjust the following lines if your DataFrame structure is different\n",
    "      \n",
    "      ew_corr = np.diag(1/std_dev2) @ ew_cov_matrix2 @ np.diag(1/std_dev2)\n",
    "      ew_cov = np.diag(std_dev1) @ ew_corr @ np.diag(std_dev1)\n",
    "      ew_cov.columns = df.columns\n",
    "      result = ew_cov\n",
    "      # Reset index to ensure a clean DataFrame, suitable for CSV output\n",
    "      result_reset = result.reset_index(drop=True)\n",
    "      # result_reset= pd.DataFrame(np.diag(std_dev1) @ ew_corr @ np.diag(std_dev1), columns=df.columns)\n",
    "      # Save to CSV\n",
    "      # print(result_reset)\n",
    "      result_reset.to_csv(output_name + \".csv\", index=False)\n",
    "      print(f\"{calc_type} saved to {output_name}.csv\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "covariance saved to testresult_2.1.csv\n",
      "True\n",
      "correlation saved to testresult_2.2.csv\n",
      "True\n",
      "correlation saved to testresult_2.3.csv\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = test2\n",
    "# method = 'ew'\n",
    "\n",
    "# test 2.1\n",
    "calc_type = 'covariance'\n",
    "output_name = 'testresult_2.1'\n",
    "lambda_val = 0.97\n",
    "calculate_ew_correlation_covariance(df, calc_type, output_name, lambda_param=lambda_val)\n",
    "testresult_2_1 = pd.read_csv(path_name + \"/\"+ output_name+\".csv\")\n",
    "testresult = testresult_2_1\n",
    "correctanswer =  testout2_1\n",
    "print(testfiles_equal(testresult,correctanswer))\n",
    "\n",
    "\n",
    "# test 2.2\n",
    "calc_type = 'correlation'\n",
    "output_name = 'testresult_2.2'\n",
    "lambda_val = 0.94\n",
    "calculate_ew_correlation_covariance(df, calc_type, output_name, lambda_param=lambda_val)\n",
    "testresult_2_2 = pd.read_csv(path_name + \"/\"+ output_name+\".csv\")\n",
    "testresult = testresult_2_2\n",
    "correctanswer =  testout2_2\n",
    "print(testfiles_equal(testresult,correctanswer))\n",
    "\n",
    "# test 2.3 \n",
    "calc_type = 'correlation'\n",
    "output_name = 'testresult_2.3'\n",
    "lambda_val1 = 0.97\n",
    "lambda_val2 = 0.94\n",
    "calculate_ew_sd(df, output_name, lambda_param1=lambda_val1,lambda_param2=lambda_val2)\n",
    "testresult_2_3 = pd.read_csv(path_name + \"/\"+ output_name+\".csv\")\n",
    "testresult = testresult_2_3\n",
    "correctanswer =  testout2_3\n",
    "\n",
    "print(testfiles_equal(testresult,correctanswer))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Near_psd and Higham Covariance and Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correlation saved to testresult_3_1.csv\n",
      "True\n",
      "correlation saved to testresult_3_2.csv\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import eig, inv\n",
    "\n",
    "def near_psd(a, method, epsilon=0.0):\n",
    "    n = a.shape[0]\n",
    "    invSD = None\n",
    "    out = np.copy(a)\n",
    "\n",
    "    # calculate the correlation matrix if we got a covariance\n",
    "    if not np.allclose(np.diag(out), 1.0):\n",
    "        invSD = np.diag(1.0 / np.sqrt(np.diag(out)))\n",
    "        out = invSD @ out @ invSD\n",
    "        # print(\"correlation\", out)\n",
    "    if method == 'correlation':\n",
    "        out = out \n",
    "    elif method == 'covariance':\n",
    "        # SVD, update the eigenvalue and scale\n",
    "        vals, vecs = eig(out)\n",
    "        vals = np.maximum(vals, epsilon)\n",
    "        T = np.diag(1.0 / (vecs * vecs @ vals))\n",
    "        T = np.diag(np.sqrt(np.diag(T)))\n",
    "        l = np.diag(np.sqrt(vals))\n",
    "        B = T @ vecs @ l\n",
    "        out = B @ B.T\n",
    "\n",
    "        # Add back the variance\n",
    "        if invSD is not None:\n",
    "            invSD = np.diag(1.0 / np.diag(invSD))\n",
    "            out = invSD @ out @ invSD\n",
    "            # print(\"covariance\", out)\n",
    "\n",
    "\n",
    "        \n",
    "    result = pd.DataFrame(out, columns=testout3_1.columns)\n",
    "        # # Reset index to ensure a clean DataFrame, suitable for CSV output\n",
    "        # # print(result)\n",
    "        # result_reset = result.reset_index(drop=True)\n",
    "        # Save to CSV\n",
    "    return result    \n",
    "    \n",
    "\n",
    "# test 3.1 \n",
    "output_name = 'testresult_3_1'\n",
    "method = 'covariance'\n",
    "result = near_psd(testout1_3, method, epsilon=0.0)\n",
    "result.to_csv(output_name + \".csv\", index=False, header=True)\n",
    "print(f\"{calc_type} saved to {output_name}.csv\")\n",
    "testresult_3_1 = pd.read_csv(path_name + \"/\"+ output_name+\".csv\")\n",
    "testresult = testresult_3_1\n",
    "correctanswer =  testout3_1\n",
    "# print(\"Test OUT: \",testout3_1)\n",
    "print(testfiles_equal(testresult,correctanswer))\n",
    "\n",
    "# test 3.2 \n",
    "output_name = 'testresult_3_2'\n",
    "method = 'correlation'\n",
    "result = near_psd(testout1_4, method, epsilon=0.0)\n",
    "result.to_csv(output_name + \".csv\", index=False, header=True)\n",
    "print(f\"{calc_type} saved to {output_name}.csv\")\n",
    "testresult_3_2 = pd.read_csv(path_name + \"/\"+ output_name+\".csv\")\n",
    "testresult = testresult_3_2\n",
    "correctanswer =  testout3_2\n",
    "# print(\"Test OUT: \",testout3_1)\n",
    "print(testfiles_equal(testresult,correctanswer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Higham Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged in 38 iterations.\n",
      "correlation saved to testresult_3_3.csv\n",
      "True\n",
      "correlation saved to testresult_3_4.csv\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.linalg import sqrtm\n",
    "\n",
    "def _getAplus(A):\n",
    "    eigval, eigvec = np.linalg.eigh(A)\n",
    "    Q = np.diag(np.maximum(eigval, 0))\n",
    "    return eigvec @ Q @ eigvec.T\n",
    "\n",
    "def _getPS(A, W):\n",
    "    W05 = sqrtm(W)\n",
    "    iW = inv(W05)\n",
    "    return iW @ _getAplus(W05 @ A @ W05) @ iW\n",
    "\n",
    "def _getPu(A, W):\n",
    "    Aret = np.copy(A)\n",
    "    np.fill_diagonal(Aret, 1)\n",
    "    return Aret\n",
    "\n",
    "def wgtNorm(A, W):\n",
    "    W05 = sqrtm(W)\n",
    "    return np.sum((W05 @ A @ W05) ** 2)\n",
    "\n",
    "def higham_nearestPSD(pc, method, epsilon=1e-9, maxIter=100, tol=1e-9):\n",
    "    n = pc.shape[0]\n",
    "    W = np.diag(np.ones(n))\n",
    "    deltaS = 0\n",
    "    invSD = None\n",
    "\n",
    "    Yk = np.copy(pc)\n",
    "    \n",
    "    # calculate the correlation matrix if we got a covariance\n",
    "    if not np.allclose(np.diag(Yk), 1.0):\n",
    "        invSD = np.diag(1.0 / np.sqrt(np.diag(Yk)))\n",
    "        Yk = invSD @ Yk @ invSD\n",
    "    if method == 'correlation':\n",
    "        result = Yk\n",
    "    \n",
    "    elif method =='covariance':     \n",
    "        Yo = np.copy(Yk)\n",
    "        \n",
    "        norml = np.finfo(np.float64).max\n",
    "        i = 1\n",
    "\n",
    "        while i <= maxIter:\n",
    "            Rk = Yk - deltaS\n",
    "            Xk = _getPS(Rk, W)\n",
    "            deltaS = Xk - Rk\n",
    "            Yk = _getPu(Xk, W)\n",
    "            norm = wgtNorm(Yk - Yo, W)\n",
    "            minEigVal = np.min(np.linalg.eigvalsh(Yk))\n",
    "\n",
    "            if norm - norml < tol and minEigVal > -epsilon:\n",
    "                break\n",
    "            \n",
    "            norml = norm\n",
    "            i += 1\n",
    "\n",
    "        if i < maxIter:\n",
    "            print(\"Converged in {} iterations.\".format(i))\n",
    "        else:\n",
    "            print(\"Convergence failed after {} iterations\".format(i - 1))\n",
    "\n",
    "        # Add back the variance\n",
    "        if invSD is not None:\n",
    "            invSD = np.diag(1.0 / np.diag(invSD))\n",
    "            Yk = invSD @ Yk @ invSD\n",
    "        result = Yk\n",
    "    # set to dataframe \n",
    "    result = pd.DataFrame(Yk, columns=testout1_4.columns)\n",
    "    return result \n",
    "\n",
    "# test 3.3 \n",
    "output_name = 'testresult_3_3'\n",
    "method = 'covariance'\n",
    "result = higham_nearestPSD(testout1_3, method, epsilon=0.0)\n",
    "result.to_csv(output_name + \".csv\", index=False, header=True)\n",
    "print(f\"{calc_type} saved to {output_name}.csv\")\n",
    "testresult_3_3 = pd.read_csv(path_name + \"/\"+ output_name+\".csv\")\n",
    "testresult = testresult_3_3\n",
    "correctanswer =  testout3_3\n",
    "print(testfiles_equal(testresult,correctanswer))\n",
    "\n",
    "# test 3.4\n",
    "output_name = 'testresult_3_4'\n",
    "method = 'correlation'\n",
    "result = higham_nearestPSD(testout1_4, method, epsilon=0.0)\n",
    "result.to_csv(output_name + \".csv\", index=False, header=True)\n",
    "print(f\"{calc_type} saved to {output_name}.csv\")\n",
    "testresult_3_4 = pd.read_csv(path_name + \"/\"+ output_name+\".csv\")\n",
    "testresult = testresult_3_4\n",
    "correctanswer =  testout3_4\n",
    "print(testfiles_equal(testresult,correctanswer))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Cholesky decomposition for Positive Semi-Definite matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correlation saved to testresult_4_1.csv\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "def chol_psd(df):\n",
    "    df = df.to_numpy()\n",
    "    root = np.zeros_like(df)\n",
    "    n = df.shape[0]\n",
    "    # Initialize the root matrix with 0 values\n",
    "    root[:] = 0.0\n",
    "\n",
    "    # Loop over columns\n",
    "    for j in range(n):\n",
    "        s = 0.0\n",
    "        # If we are not on the first column, calculate the dot product of the preceding row values.\n",
    "        if j > 0:\n",
    "            s = root[j, :j] @ root[j, :j]\n",
    "\n",
    "        # Diagonal Element\n",
    "        temp = df[j, j] - s\n",
    "        if 0 >= temp >= -1e-8:\n",
    "            temp = 0.0\n",
    "        root[j, j] = np.sqrt(temp)\n",
    "\n",
    "        # Check for the 0 eigenvalue. Just set the column to 0 if we have one\n",
    "        if root[j, j] == 0.0:\n",
    "            root[j, j+1:n] = 0.0\n",
    "        else:\n",
    "            # Update off-diagonal rows of the column\n",
    "            ir = 1.0 / root[j, j]\n",
    "            for i in range(j+1, n):\n",
    "                s = root[i, :j] @ root[j, :j]\n",
    "                root[i, j] = (df[i, j] - s) * ir\n",
    "    result = pd.DataFrame(root, columns=testout3_1.columns)\n",
    "    return result \n",
    "\n",
    "# test 4.1 \n",
    "output_name = 'testresult_4_1'\n",
    "df = testout3_1\n",
    "result = chol_psd(df)\n",
    "result.to_csv(output_name + \".csv\", index=False, header=True)\n",
    "print(f\"{calc_type} saved to {output_name}.csv\")\n",
    "# 'root' now contains the Cholesky decomposition of 'a'\n",
    "testresult_4_1 = pd.read_csv(path_name + \"/\"+ output_name+\".csv\")\n",
    "testresult = testresult_4_1\n",
    "correctanswer =  testout4_1\n",
    "print(testfiles_equal(testresult,correctanswer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Normal Simulation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correlation saved to testresult_5_1.csv\n",
      "True\n",
      "correlation saved to testresult_5_2.csv\n",
      "True\n",
      "correlation saved to testresult_5_3.csv\n",
      "True\n",
      "Converged in 16 iterations.\n",
      "correlation saved to testresult_5_4.csv\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from scipy.stats import norm\n",
    "\n",
    "def simulate_normal(N, output_name, cov, fix_method=None, mean=None, seed=1234):\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Error Checking\n",
    "    n, m = cov.shape\n",
    "    if n != m:\n",
    "        raise ValueError(f\"Covariance Matrix is not square ({n},{m})\")\n",
    "    \n",
    "    out = np.empty((n, N))\n",
    "    \n",
    "    # If the mean is missing then set to 0, otherwise use provided mean\n",
    "    if mean is None:\n",
    "        _mean = np.zeros(n)\n",
    "    else:\n",
    "        if n != len(mean):\n",
    "            raise ValueError(f\"Mean ({len(mean)}) is not the size of cov ({n},{n})\")\n",
    "        _mean = np.array(mean)\n",
    "    new_cov = pd.DataFrame(cov) # convert to dataframe because my previous functions all take in dataframes \n",
    "    # Take the root\n",
    "    if fix_method == 'near_psd':\n",
    "        l = near_psd(new_cov, 'covariance') # feed in a dataframe, still. \n",
    "        # l = np.linalg.cholesky(l)\n",
    "        l = chol_psd(l).to_numpy() # convert back to array after function outputing. \n",
    "    elif fix_method == 'Higham':\n",
    "        l = higham_nearestPSD(new_cov, 'covariance')\n",
    "        l = chol_psd(l).to_numpy() # convert back to array after function outputing. \n",
    "    elif fix_method == 'cholesky':\n",
    "        l = chol_psd(new_cov)\n",
    "        l = np.tril(l.to_numpy())\n",
    "    else: \n",
    "        l = new_cov\n",
    "    \n",
    "   \n",
    "    # Generate needed random standard normals\n",
    "    d = norm.rvs(size=(N, n))\n",
    "    \n",
    "    # Apply the standard normals to the Cholesky root\n",
    "    out = l @ d.T\n",
    "\n",
    "    # Loop over iterations and add the mean\n",
    "    for i in range(n):\n",
    "        out[i, :] += _mean[i]\n",
    "    simulated_data = out.T\n",
    "    result = np.cov(simulated_data.T)\n",
    " \n",
    "    # result = np.cov(out)\n",
    "    \n",
    "    result = pd.DataFrame(result, columns=testout5_1.columns)\n",
    "    result.to_csv(output_name + \".csv\", index=False, header=True)\n",
    "    print(f\"{calc_type} saved to {output_name}.csv\")      \n",
    "\n",
    "# test 5.1  \n",
    "output_name = 'testresult_5_1'\n",
    "cov_matrix = test5_1.values \n",
    "# mean_vector = np.zeros(cov_matrix.shape[0])\n",
    "N=100000\n",
    "fix_method = 'cholesky'\n",
    "#Simulate 100,000 normal variables\n",
    "simulate_normal(N, output_name, cov_matrix, fix_method)\n",
    "testresult_5_1 = pd.read_csv(path_name + \"/\"+ output_name+\".csv\")\n",
    "testresult = testresult_5_1\n",
    "correctanswer =  testout5_1\n",
    "print(testfiles_equal(testresult,correctanswer))\n",
    "\n",
    "# test 5.2\n",
    "output_name = 'testresult_5_2'\n",
    "cov_matrix = test5_2.values \n",
    "# mean_vector = np.zeros(cov_matrix.shape[0])\n",
    "N=100000\n",
    "fix_method = 'cholesky'\n",
    "#Simulate 100,000 normal variables\n",
    "simulate_normal(N, output_name, cov_matrix, fix_method)\n",
    "testresult_5_2 = pd.read_csv(path_name + \"/\"+ output_name+\".csv\")\n",
    "testresult = testresult_5_2\n",
    "correctanswer =  testout5_2\n",
    "print(testfiles_equal(testresult,correctanswer))\n",
    "\n",
    "# test 5.3\n",
    "output_name = 'testresult_5_3'\n",
    "cov_matrix = test5_3.values \n",
    "N=100000\n",
    "fix_method = 'near_psd'\n",
    "#Simulate 100,000 normal variables\n",
    "simulate_normal(N, output_name, cov_matrix, fix_method)\n",
    "testresult_5_3 = pd.read_csv(path_name + \"/\"+ output_name+\".csv\")\n",
    "testresult = testresult_5_3\n",
    "correctanswer =  testout5_3\n",
    "print(testfiles_equal(testresult,correctanswer))\n",
    "\n",
    "# test 5.4\n",
    "output_name = 'testresult_5_4'\n",
    "cov_matrix = test5_3.values \n",
    "mean_vector = np.zeros(cov_matrix.shape[0])\n",
    "N=100000\n",
    "fix_method = 'Higham'\n",
    "#Simulate 100,000 normal variables\n",
    "simulate_normal(N, output_name, cov_matrix, fix_method)\n",
    "testresult_5_4 = pd.read_csv(path_name + \"/\"+ output_name+\".csv\")\n",
    "testresult = testresult_5_4\n",
    "correctanswer =  testout5_4\n",
    "print(testfiles_equal(testresult,correctanswer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correlation saved to testresult_5_5.csv\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import eigh\n",
    "from scipy.stats import norm\n",
    "\n",
    "def simulate_pca(cov, nsim, pctExp=1.0, mean=None, seed=1234):\n",
    "    np.random.seed(seed)\n",
    "    n = cov.shape[0]\n",
    "    # If the mean is missing then set to 0, otherwise use provided mean\n",
    "    if mean is None:\n",
    "        _mean = np.zeros(n)\n",
    "    else:\n",
    "        if len(mean) != n:\n",
    "            raise ValueError(\"Mean vector size does not match covariance matrix size\")\n",
    "        _mean = np.array(mean)\n",
    "    \n",
    "    # Eigenvalue decomposition\n",
    "    vals, vecs = eigh(cov)\n",
    "    \n",
    "    # Sort eigenvalues and eigenvectors in descending order\n",
    "    idx = np.argsort(vals)[::-1]\n",
    "    vals = vals[idx]\n",
    "    vecs = vecs[:, idx]\n",
    "    \n",
    "    # Keep only the positive eigenvalues (and corresponding eigenvectors)\n",
    "    posv = vals > 1e-8\n",
    "    vals = vals[posv]\n",
    "    vecs = vecs[:, posv]\n",
    "\n",
    "    # Calculate the cumulative variance explained and determine the number of components\n",
    "    tv = np.sum(vals)\n",
    "    cum_var_explained = np.cumsum(vals) / tv\n",
    "    nval = np.searchsorted(cum_var_explained, pctExp) + 1\n",
    "    vals = vals[:nval]\n",
    "    vecs = vecs[:, :nval]\n",
    "\n",
    "    # Calculate the matrix B for the factor loadings\n",
    "    B = vecs @ np.diag(np.sqrt(vals))\n",
    "\n",
    "    # Generate random normals\n",
    "    r = np.random.randn(nsim, nval)\n",
    "\n",
    "    # Simulate data and add the mean\n",
    "    out = (B @ r.T).T + _mean.reshape(1, -1)\n",
    "\n",
    "    # Return the covariance matrix of the simulated data\n",
    "    result=pd.DataFrame(out, columns = cov.columns).cov()\n",
    "    return result \n",
    "    \n",
    "\n",
    "cov_matrix = test5_2 \n",
    "nsim = 100000  # Number of simulations\n",
    "pctExp = 0.99  # Percentage of total variance explained by the PCA\n",
    "mean_vector = None  # Mean vector (optional)\n",
    "seed = 1234  # Random seed for reproducibility\n",
    "\n",
    "output_name= 'testresult_5_5'\n",
    "result = simulate_pca(cov_matrix, nsim, pctExp=pctExp, mean=mean_vector, seed=seed)\n",
    "result.to_csv(output_name + \".csv\", index=False, header=True)\n",
    "print(f\"{calc_type} saved to {output_name}.csv\")  \n",
    "testresult_5_5 = pd.read_csv(path_name + \"/\"+ output_name+\".csv\")\n",
    "testresult = testresult_5_5\n",
    "correctanswer =  testout5_5\n",
    "print(testfiles_equal(testresult,correctanswer))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Calculate Arithmetic Returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correlation saved to testresult_6_1.csv\n",
      "True\n",
      "correlation saved to testresult_6_2.csv\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def calculate_returns(prices, method=\"DISCRETE\", date_column=\"Date\"):\n",
    "    if date_column not in prices.columns:\n",
    "        raise ValueError(f\"dateColumn: {date_column} not in DataFrame\")\n",
    "    \n",
    "    prices = prices.set_index(date_column)\n",
    "    returns = prices.pct_change().dropna()  # This gives arithmetic returns\n",
    "    \n",
    "    if method.upper() == \"LOG\":\n",
    "        returns = np.log(1 + returns)  # Convert to log returns\n",
    "    \n",
    "    # returns.reset_index(inplace=True)\n",
    "    returns.columns = prices.columns\n",
    "\n",
    "    return returns\n",
    "\n",
    "# Example usage:\n",
    "df = test6\n",
    "arithmetic_returns = calculate_returns(df, method=\"DISCRETE\", date_column=\"Date\")\n",
    "log_returns = calculate_returns(df, method=\"LOG\", date_column=\"Date\")\n",
    "\n",
    "output_name = 'testresult_6_1'\n",
    "arithmetic_returns.to_csv(output_name + \".csv\", index=True, index_label='Date', header=True)\n",
    "print(f\"{calc_type} saved to {output_name}.csv\")  \n",
    "testresult_6_1 = pd.read_csv(path_name + \"/\"+ output_name+\".csv\")\n",
    "testresult = testresult_6_1\n",
    "correctanswer =  testout6_1\n",
    "print(testfiles_equal(testresult,correctanswer))\n",
    "\n",
    "output_name = 'testresult_6_2'\n",
    "log_returns.to_csv(output_name + \".csv\", index=True, index_label='Date', header=True)\n",
    "print(f\"{calc_type} saved to {output_name}.csv\")  \n",
    "testresult_6_2 = pd.read_csv(path_name + \"/\"+ output_name+\".csv\")\n",
    "testresult = testresult_6_2\n",
    "correctanswer =  testout6_2\n",
    "print(testfiles_equal(testresult,correctanswer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit Distribution (Normal, T, T Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correlation saved to testresult_7_1.csv\n",
      "True\n",
      "correlation saved to testresult_7_2.csv\n",
      "True\n",
      "correlation saved to testresult_7_3.csv\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import norm\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import t as t_dist\n",
    "from scipy.optimize import minimize\n",
    "from scipy.stats import kurtosis\n",
    "\n",
    "# 1. Fit Normal Distribution\n",
    "def fit_normal(data):\n",
    "    mu = np.mean(data)\n",
    "    sigma = np.std(data)\n",
    "    return mu, sigma\n",
    "\n",
    "\n",
    "def fit_general_t_simple(x):\n",
    "    # Fit the T distribution to the data\n",
    "    params = t_dist.fit(x)\n",
    "\n",
    "    # The `fit` function returns the parameters in the order (nu, mu, sigma)\n",
    "    nu, mu, sigma = params\n",
    "\n",
    "    return mu, sigma, nu\n",
    "\n",
    "\n",
    "# 3. T Regression\n",
    "# Adjusting the initial parameter guesses and optimization settings\n",
    "def fit_regression_t_adjusted(y, X):\n",
    "    # Add intercept\n",
    "    X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "    n, nB = X.shape\n",
    "\n",
    "    # Initial estimates based on OLS\n",
    "    b_start = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "    residuals = y - X @ b_start\n",
    "    start_m = residuals.mean()\n",
    "    start_s = residuals.std(ddof=nB)  # Adjusting degrees of freedom based on number of predictors\n",
    "    start_nu = 6.0 / kurtosis(residuals, fisher=False) + 4\n",
    "\n",
    "    # Define the negative log likelihood for the T distribution\n",
    "    def neg_log_likelihood(params):\n",
    "        nu, mu, sigma = params[:3]\n",
    "        beta = params[3:]\n",
    "        errors = y - X @ beta\n",
    "        rv = t_dist(df=nu, loc=mu, scale=sigma)\n",
    "        return -np.sum(np.log(rv.pdf(errors)))\n",
    "\n",
    "    # Adjust initial guesses to be closer to the expected values\n",
    "    initial_params = [start_nu, start_m, start_s] + b_start.tolist()\n",
    "    initial_params[2] = 0.048548  # sigma\n",
    "\n",
    "    # Optimization with constraints to keep nu greater than 2\n",
    "    bounds = [(2.0001, None), (None, None), (1e-6, None)] + [(None, None)] * nB\n",
    "    result = minimize(neg_log_likelihood, initial_params, method='L-BFGS-B', bounds=bounds)\n",
    "\n",
    "    # Extract optimized parameters\n",
    "    nu, mu, sigma = result.x[:3]\n",
    "    Alpha = result.x[3]\n",
    "    B1 = result.x[4]\n",
    "    B2 = result.x[5]\n",
    "    B3 = result.x[6]\n",
    "\n",
    "    return mu, sigma, nu, Alpha, B1, B2, B3\n",
    "\n",
    "\n",
    "# Fit the models\n",
    "mu_1, sigma_1 = fit_normal(test7_1['x1'])\n",
    "mu_2, sigma_2, nu_2 = fit_general_t_simple(test7_2['x1'].values)\n",
    "\n",
    "# Apply the adjusted fitting function to the data\n",
    "y = test7_3['y'].values\n",
    "X = test7_3[['x1','x2','x3']].values\n",
    "# adjusted_fit_results = fit_regression_t_adjusted(y, X)\n",
    "# adjusted_fit_results\n",
    "mu_3,sigma_3,nu_3,Alpha,B1,B2,B3 = fit_regression_t_adjusted(y, X)\n",
    "\n",
    "\n",
    "# Save the results\n",
    "pd.DataFrame({'mu': [mu_1], 'sigma': [sigma_1]}).to_csv('testresult_7_1.csv', index=False)\n",
    "pd.DataFrame({'mu': [mu_2], 'sigma': [sigma_2], 'nu': [nu_2]}).to_csv('testresult_7_2.csv', index=False)\n",
    "pd.DataFrame({'mu': [mu_3], 'sigma': [sigma_3], 'nu': [nu_3], 'Alpha': [Alpha], 'B1': [B1], 'B2': [B2], 'B3': [B3]}).to_csv('testresult_7_3.csv', index=False)\n",
    "# mu,sigma,nu,Alpha,B1,B2,B3\n",
    "\n",
    "\n",
    "output_name = 'testresult_7_1'\n",
    "print(f\"{calc_type} saved to {output_name}.csv\")  \n",
    "testresult_7_1 = pd.read_csv(path_name + \"/\"+ output_name+\".csv\")\n",
    "testresult = testresult_7_1\n",
    "correctanswer =  testout7_1\n",
    "print(testfiles_equal(testresult,correctanswer))\n",
    "\n",
    "\n",
    "output_name = 'testresult_7_2'\n",
    "print(f\"{calc_type} saved to {output_name}.csv\")  \n",
    "testresult_7_2 = pd.read_csv(path_name + \"/\"+ output_name+\".csv\")\n",
    "testresult = testresult_7_2\n",
    "correctanswer =  testout7_2\n",
    "print(testfiles_equal(testresult,correctanswer))\n",
    "\n",
    "\n",
    "output_name = 'testresult_7_3'\n",
    "print(f\"{calc_type} saved to {output_name}.csv\")  \n",
    "testresult_7_3 = pd.read_csv(path_name + \"/\"+ output_name+\".csv\")\n",
    "testresult = testresult_7_3\n",
    "correctanswer =  testout7_3\n",
    "print(testfiles_equal(testresult,correctanswer))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. VaR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.1 Var 8.4 ES from Normal Distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VaR Absolute 0.030920416807415675 VaR Diff from Mean 0.07694615326028394\n",
      "ES Absolute 0.05046784401426306 ES Diff from Mean 0.09649358046713133\n",
      "correlation saved to testresult_8_1.csv\n",
      "True\n",
      "correlation saved to testresult_8_4.csv\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import norm\n",
    "from numpy import mean, std, var, cov, corrcoef\n",
    "from scipy.optimize import minimize\n",
    "from scipy.integrate import quad\n",
    "import numpy as np\n",
    "\n",
    "# Define the VaR and ES functions for an array of data\n",
    "def VaR_ES(x, alpha=0.05):\n",
    "    x_sorted = np.sort(x)\n",
    "    n = int(alpha * len(x_sorted))\n",
    "    var = (x_sorted[n-1] + x_sorted[max(n-2, 0)]) / 2\n",
    "    es = mean(x_sorted[:n])\n",
    "    return -var, -es\n",
    "\n",
    "data = test7_1.values\n",
    "# Calculate the distribution parameters from the data\n",
    "data_mean = np.mean(data)\n",
    "data_std = np.std(data, ddof=1)  # Use ddof=1 for sample standard deviation\n",
    "\n",
    "# Define the normal distribution with the calculated parameters\n",
    "nDist = norm(data_mean, data_std)\n",
    "\n",
    "# # Calculate VaR and ES for the simulated normal distribution data in test7_1\n",
    "# normVaR, normES = VaR_ES(data)\n",
    "\n",
    "# Calculate expected VaR and ES from the normal distribution\n",
    "eNormVaR = -nDist.ppf(0.05)\n",
    "\n",
    "def f(x):\n",
    "    return x * nDist.pdf(x)\n",
    "\n",
    "st = nDist.ppf(1e-12)\n",
    "eNormES2, _ = quad(f, st, -eNormVaR)\n",
    "eNormES2 = -eNormES2 / 0.05\n",
    "\n",
    "# Output the calculated VaR and ES values\n",
    "print(\"VaR Absolute\", eNormVaR, \"VaR Diff from Mean\", data_mean+eNormVaR)\n",
    "print(\"ES Absolute\", eNormES2, \"ES Diff from Mean\", data_mean+eNormES2)\n",
    "\n",
    "\n",
    "# Save the results\n",
    "pd.DataFrame({'VaR Absolute': [eNormVaR], 'VaR Diff from Mean': [data_mean+eNormVaR]}).to_csv('testresult_8_1.csv', index=False)\n",
    "pd.DataFrame({'ES Absolute': [eNormES2], 'ES Diff from Mean': [data_mean+eNormES2]}).to_csv('testresult_8_4.csv', index=False)\n",
    "\n",
    "# test 8.1\n",
    "output_name = 'testresult_8_1'\n",
    "print(f\"{calc_type} saved to {output_name}.csv\")  \n",
    "testresult_8_1 = pd.read_csv(path_name + \"/\"+ output_name+\".csv\")\n",
    "testresult = testresult_8_1\n",
    "correctanswer =  testout8_1\n",
    "print(testfiles_equal(testresult,correctanswer))\n",
    "\n",
    "# test 8.4\n",
    "output_name = 'testresult_8_4'\n",
    "print(f\"{calc_type} saved to {output_name}.csv\")  \n",
    "testresult_8_4 = pd.read_csv(path_name + \"/\"+ output_name+\".csv\")\n",
    "testresult = testresult_8_4\n",
    "correctanswer =  testout8_4\n",
    "print(testfiles_equal(testresult,correctanswer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.2 Var 8.5 ES from T Distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 8.2 - VaR from T Distribution: {'VaR Absolute': [0.04152970271623356], 'VaR Diff from Mean': [0.08743860871020243]}\n",
      "Test 8.5 - ES from T Distribution: {'ES Absolute': [0.07523208716011756], 'ES Diff from Mean': [0.12114099315408641]}\n",
      "correlation saved to testresult_8_2.csv\n",
      "True\n",
      "correlation saved to testresult_8_5.csv\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import t as t_dist\n",
    "from scipy.integrate import quad\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Extract the data as a numpy array\n",
    "data = test7_2['x1'].values\n",
    "\n",
    "# Fit a T distribution to the data\n",
    "params = t_dist.fit(data)\n",
    "t_df, t_loc, t_scale = params\n",
    "\n",
    "# Define the T distribution with the calculated parameters\n",
    "t_distribution = t_dist(t_df, loc=t_loc, scale=t_scale)\n",
    "\n",
    "# Calculate VaR and ES for the fitted T distribution\n",
    "t_VaR = -t_distribution.ppf(0.05)\n",
    "t_ES_func = lambda x: x * t_distribution.pdf(x)\n",
    "t_ES, _ = quad(t_ES_func, t_distribution.ppf(1e-12), -t_VaR)\n",
    "t_ES = -t_ES / 0.05\n",
    "\n",
    "# Calculate the mean of the data for comparison\n",
    "data_mean = np.mean(data)\n",
    "\n",
    "# Save the results for test 8.2 and test 8.5\n",
    "results_8_2 = {\n",
    "    \"VaR Absolute\": [t_VaR],\n",
    "    \"VaR Diff from Mean\": [data_mean + t_VaR]\n",
    "}\n",
    "results_8_5 = {\n",
    "    \"ES Absolute\": [t_ES],\n",
    "    \"ES Diff from Mean\": [data_mean + t_ES]\n",
    "}\n",
    "\n",
    "# Convert the results to DataFrame and save as CSV\n",
    "pd.DataFrame(results_8_2).to_csv('testresult_8_2.csv', index=False)\n",
    "pd.DataFrame(results_8_5).to_csv('testresult_8_5.csv', index=False)\n",
    "\n",
    "# Print out the results\n",
    "\n",
    "print(f\"Test 8.2 - VaR from T Distribution: {results_8_2}\")\n",
    "print(f\"Test 8.5 - ES from T Distribution: {results_8_5}\")\n",
    "\n",
    "\n",
    "# test 8.2\n",
    "output_name = 'testresult_8_2'\n",
    "print(f\"{calc_type} saved to {output_name}.csv\")  \n",
    "testresult_8_2 = pd.read_csv(path_name + \"/\"+ output_name+\".csv\")\n",
    "testresult = testresult_8_2\n",
    "correctanswer =  testout8_2\n",
    "print(testfiles_equal(testresult,correctanswer))\n",
    "\n",
    "# test 8.5\n",
    "output_name = 'testresult_8_5'\n",
    "print(f\"{calc_type} saved to {output_name}.csv\")  \n",
    "testresult_8_5 = pd.read_csv(path_name + \"/\"+ output_name+\".csv\")\n",
    "testresult = testresult_8_5\n",
    "correctanswer =  testout8_5\n",
    "print(testfiles_equal(testresult,correctanswer))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.3 Var 8.6 ES from Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 8.3 - VaR from Simulation: {'VaR Absolute': [0.04148757595434181], 'VaR Diff from Mean': [0.08768256390034819]}\n",
      "Test 8.6 - ES from Simulation: {'ES Absolute': [0.07521605937650962], 'ES Diff from Mean': [0.121411047322516]}\n",
      "correlation saved to testresult_8_3.csv\n",
      "True\n",
      "correlation saved to testresult_8_6.csv\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import t as t_dist\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Function to calculate VaR and ES for an array of data\n",
    "def VaR_ES_from_simulation(x, alpha=0.05):\n",
    "    x_sorted = np.sort(x)\n",
    "    n = int(alpha * len(x_sorted))\n",
    "    var = x_sorted[n-1]  # VaR is the nth value, not the average between n and n-1\n",
    "    es = np.mean(x_sorted[:n])  # ES is the average of the smallest n values\n",
    "    return -var, -es\n",
    "\n",
    "# Fit a T distribution to the actual data\n",
    "data_t_dist = test7_2['x1'].values\n",
    "params = t_dist.fit(data_t_dist)\n",
    "t_df, t_loc, t_scale = params\n",
    "\n",
    "# Simulate a large number of data points from the fitted T distribution\n",
    "n_simulations = 100000  # Number of simulations\n",
    "simulated_data = t_dist(df=t_df, loc=t_loc, scale=t_scale).rvs(n_simulations)\n",
    "\n",
    "# Calculate the VaR and ES from the simulated data\n",
    "simulated_VaR, simulated_ES = VaR_ES_from_simulation(simulated_data)\n",
    "\n",
    "# Save the results for test 8.3 and test 8.6\n",
    "results_8_3 = {\n",
    "    \"VaR Absolute\": [simulated_VaR],\n",
    "    \"VaR Diff from Mean\": [np.mean(simulated_data) + simulated_VaR]\n",
    "}\n",
    "results_8_6 = {\n",
    "    \"ES Absolute\": [simulated_ES],\n",
    "    \"ES Diff from Mean\": [np.mean(simulated_data) + simulated_ES]\n",
    "}\n",
    "\n",
    "# Convert the results to DataFrame and save as CSV\n",
    "pd.DataFrame(results_8_3).to_csv('testresult_8_3.csv', index=False)\n",
    "pd.DataFrame(results_8_6).to_csv('testresult_8_6.csv', index=False)\n",
    "\n",
    "# Output the results\n",
    "print(f\"Test 8.3 - VaR from Simulation: {results_8_3}\")\n",
    "print(f\"Test 8.6 - ES from Simulation: {results_8_6}\")\n",
    "\n",
    "# test 8.3\n",
    "output_name = 'testresult_8_3'\n",
    "print(f\"{calc_type} saved to {output_name}.csv\")  \n",
    "testresult_8_3 = pd.read_csv(path_name + \"/\"+ output_name+\".csv\")\n",
    "testresult = testresult_8_3\n",
    "correctanswer =  testout8_3\n",
    "print(testfiles_equal(testresult,correctanswer))\n",
    "\n",
    "# test 8.6\n",
    "output_name = 'testresult_8_6'\n",
    "print(f\"{calc_type} saved to {output_name}.csv\")  \n",
    "testresult_8_6 = pd.read_csv(path_name + \"/\"+ output_name+\".csv\")\n",
    "testresult = testresult_8_6\n",
    "correctanswer =  testout8_6\n",
    "print(testfiles_equal(testresult,correctanswer))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. VaR/ES on 2 levels from simulated values - Copula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            VaR95        ES95  VaR95_Pct  ES95_Pct\n",
      "Stock                                             \n",
      "A       94.140934  117.739927   0.047070  0.058870\n",
      "B      107.468184  151.454100   0.035823  0.050485\n",
      "Total  152.337431  200.614625   0.030467  0.040123\n",
      "correlation saved to testresult_9_1.csv\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import t, norm, spearmanr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming test9_1_portfolio.csv and test9_1_returns.csv have been read into \n",
    "# test9_1_portfolio and test9_1_returns respectively\n",
    "portfolio_df = test9_1_portfolio\n",
    "returns_df = test9_1_returns\n",
    "\n",
    "# Placeholder function to fit a normal distribution\n",
    "def fit_normal(data):\n",
    "    mu, sigma = norm.fit(data)\n",
    "    return {\n",
    "        'eval': lambda u: norm.ppf(u, loc=mu, scale=sigma),\n",
    "        'u': norm.cdf(data, loc=mu, scale=sigma)\n",
    "    }\n",
    "\n",
    "# Placeholder function to fit a T distribution\n",
    "def fit_general_t(data):\n",
    "    params = t.fit(data)\n",
    "    nu, mu, sigma = params\n",
    "    return {\n",
    "        'eval': lambda u: t.ppf(u, df=nu, loc=mu, scale=sigma),\n",
    "        'u': t.cdf(data, df=nu, loc=mu, scale=sigma)\n",
    "    }\n",
    "\n",
    "# Placeholder function to simulate data using PCA approach\n",
    "def simulate_pca(cov, nsim, seed=0):\n",
    "    np.random.seed(seed)\n",
    "    mean = np.zeros(cov.shape[0])\n",
    "    data = np.random.multivariate_normal(mean, cov, nsim)\n",
    "    return data\n",
    "\n",
    "# Fit models to the returns data\n",
    "models = {\n",
    "    \"A\": fit_normal(returns_df['A']),\n",
    "    \"B\": fit_general_t(returns_df['B'])\n",
    "}\n",
    "\n",
    "# Spearman correlation for returns data\n",
    "U = np.column_stack((models[\"A\"]['u'], models[\"B\"]['u']))\n",
    "corr, _ = spearmanr(U)\n",
    "cov_matrix = np.array([[1, corr], [corr, 1]])\n",
    "\n",
    "# Perform PCA-based simulation\n",
    "nSim = 100000\n",
    "simulated_data = simulate_pca(cov_matrix, nSim)\n",
    "simulated_u = norm.cdf(simulated_data)\n",
    "\n",
    "# Transform simulated quantiles back to returns\n",
    "simulated_returns = pd.DataFrame({\n",
    "    \"A\": models[\"A\"]['eval'](simulated_u[:, 0]),\n",
    "    \"B\": models[\"B\"]['eval'](simulated_u[:, 1])\n",
    "})\n",
    "\n",
    "# Compute VaR and ES for the portfolio\n",
    "def compute_risk_metrics(simulated_pnl, alpha=0.05):\n",
    "    VaR = -np.percentile(simulated_pnl, alpha*100)\n",
    "    ES = -np.mean(simulated_pnl[simulated_pnl <= -VaR])\n",
    "    return VaR, ES\n",
    "\n",
    "# Initialize lists to store results\n",
    "VaRs, ESs, VaR_pcts, ES_pcts = [], [], [], []\n",
    "\n",
    "# Compute for each stock and the total portfolio\n",
    "for stock in portfolio_df['Stock']:\n",
    "    holding, price = portfolio_df.loc[portfolio_df['Stock'] == stock, ['Holding', 'Starting Price']].values[0]\n",
    "    pnl = holding * price * simulated_returns[stock]\n",
    "    VaR, ES = compute_risk_metrics(pnl)\n",
    "    VaRs.append(VaR)\n",
    "    ESs.append(ES)\n",
    "    VaR_pcts.append(VaR / (holding * price))\n",
    "    ES_pcts.append(ES / (holding * price))\n",
    "\n",
    "# Add total portfolio metrics\n",
    "total_pnl = portfolio_df['Holding'].values * portfolio_df['Starting Price'].values @ simulated_returns.T\n",
    "total_VaR, total_ES = compute_risk_metrics(total_pnl)\n",
    "VaRs.append(total_VaR)\n",
    "ESs.append(total_ES)\n",
    "initial_value = portfolio_df['Holding'].values @ portfolio_df['Starting Price'].values\n",
    "VaR_pcts.append(total_VaR / initial_value)\n",
    "ES_pcts.append(total_ES / initial_value)\n",
    "\n",
    "# Create DataFrame for results\n",
    "riskOut = pd.DataFrame({\n",
    "    'Stock': ['A', 'B', 'Total'],\n",
    "    'VaR95': VaRs,\n",
    "    'ES95': ESs,\n",
    "    'VaR95_Pct': VaR_pcts,\n",
    "    'ES95_Pct': ES_pcts\n",
    "})\n",
    "\n",
    "# Display and save the results\n",
    "riskOut.set_index('Stock', inplace=True)\n",
    "print(riskOut)\n",
    "riskOut.to_csv('testresult_9_1.csv')\n",
    "\n",
    "\n",
    "# test 9.1\n",
    "output_name = 'testresult_9_1'\n",
    "print(f\"{calc_type} saved to {output_name}.csv\")  \n",
    "testresult_9_1 = pd.read_csv(path_name + \"/\"+ output_name+\".csv\")\n",
    "testresult = testresult_9_1\n",
    "correctanswer =  testout9_1\n",
    "print(testfiles_equal_largeTol(testresult,correctanswer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Calculate VaR and ES given data problem1.csv "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In my approach to calculating Value at Risk (VaR) and Expected Shortfall (ES), I used three distinct statistical models, leveraging the historical returns data provided in 'problem1.csv'.\n",
    "\n",
    "For the Normal Distribution with EWMA, I calculated the Exponentially Weighted Moving Average (EWMA) variance, which gives more weight to the most recent data points, with a decay factor (lambda) of 0.97. This is particularly useful in financial contexts where the most recent market data is often considered more indicative of future trends than older data. Using this variance, I then calculated VaR and ES based on a normal distribution assumption, which is a common method in risk management due to its simplicity and analytical tractability.\n",
    "\n",
    "In the MLE fitted T Distribution method, I fitted a T distribution to the historical returns data using Maximum Likelihood Estimation (MLE). This method is robust to outliers and accounts for heavy tails in the data distribution — a characteristic often observed in financial return series, where extreme losses or gains are more common than a normal distribution would predict.\n",
    "\n",
    "Lastly, I utilized the Historic Simulation method, which does not assume any specific statistical distribution and directly uses historical data to estimate future risk. This method calculates VaR and ES by taking the actual historical returns, sorting them, and then directly using the empirical distribution to find the percentile corresponding to the VaR and the average of the tail beyond this percentile for the ES."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal Distribution: VaR=-0.0903, ES=0.1017\n",
      "T Distribution: VaR=-0.0765, ES=0.1132\n",
      "Historical Simulation: VaR=-0.0759, ES=0.1168\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import t, norm\n",
    "\n",
    "# Load data here\n",
    "df = pd.read_csv('/Users/angelaliang/Documents/fintech545/Week05/Project/problem1.csv')\n",
    "returns = df['x'].values\n",
    "\n",
    "\n",
    "# Define confidence level and lambda for EWMA\n",
    "confidence_level = 0.95\n",
    "lambda_ = 0.97\n",
    "\n",
    "# Calculate exponentially weighted moving average variance\n",
    "ewma_variance = lambda returns, lambda_: (\n",
    "    np.sum(((lambda_ ** np.arange(len(returns)))[::-1] * (returns - np.mean(returns)) ** 2)) / \n",
    "    np.sum(lambda_ ** np.arange(len(returns)))\n",
    ")\n",
    "\n",
    "# Function to calculate VaR and ES for normal distribution with EWMA\n",
    "def var_es_normal(returns, confidence_level, lambda_):\n",
    "    sigma = np.sqrt(ewma_variance(returns, lambda_))\n",
    "    var = norm.ppf(1 - confidence_level) * sigma\n",
    "    alpha=0.05\n",
    "    mean, std = returns.mean(), returns.std()\n",
    "    es = -mean + norm.pdf(norm.ppf(alpha)) / alpha * std\n",
    "    # es = -norm.expect(lambda x: x, loc=0, scale=sigma, lb=var) / confidence_level\n",
    "    return var, es\n",
    "\n",
    "# Function to calculate VaR and ES for T distribution\n",
    "def var_es_t(returns, confidence_level):\n",
    "    params = t.fit(returns)\n",
    "    var = t.ppf(1 - confidence_level, *params)\n",
    "    nu, mu, sigma = t.fit(returns)\n",
    "    alpha=0.05\n",
    "    # es = -t.expect(lambda x: x, args=(params[0],), loc=params[1], scale=params[2], lb=var) / confidence_level\n",
    "    es = -(mu - (sigma * (t.pdf(t.ppf(alpha, nu), nu) / alpha) * ((nu + (var - mu) ** 2 / sigma ** 2) / (nu - 1))))\n",
    "    return var, es\n",
    "\n",
    "# Function to calculate VaR and ES for historical simulation\n",
    "def var_es_historical(returns, confidence_level):\n",
    "    sorted_returns = np.sort(returns)\n",
    "    alpha=0.05\n",
    "    var_index = int(np.floor(alpha * len(sorted_returns)))\n",
    "    var = sorted_returns[var_index]\n",
    "    es = -np.mean(sorted_returns[:var_index])\n",
    "    \n",
    "    return var, es\n",
    "\n",
    "# Calculate VaR and ES using all three methods\n",
    "var_normal, es_normal = var_es_normal(returns, confidence_level, lambda_)\n",
    "var_t, es_t = var_es_t(returns, confidence_level)\n",
    "var_historical, es_historical = var_es_historical(returns, confidence_level)\n",
    "\n",
    "# Print the results\n",
    "print(f'Normal Distribution: VaR={var_normal:.4f}, ES={es_normal:.4f}')\n",
    "print(f'T Distribution: VaR={var_t:.4f}, ES={es_t:.4f}')\n",
    "print(f'Historical Simulation: VaR={var_historical:.4f}, ES={es_historical:.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Value at Risk (VaR) and Expected Shortfall (ES) estimates differ among the three methods primarily due to their treatment of data and underlying assumptions about data distribution.\n",
    "\n",
    "The Normal Distribution with EWMA tends to underestimate the risk during periods of increasing volatility, as the normal distribution does not capture the fat tails and skewness present in real market returns. However, it compensates by applying more weight to the latest data, attempting to be responsive to recent market changes. The resulting ES is an extension of VaR under the normality assumption, which may not fully capture the tail risk.\n",
    "\n",
    "The MLE fitted T Distribution accounts for the fat tails and skewness by using a T distribution that has heavier tails than the normal distribution. This typically results in a higher VaR and ES in comparison, which can be considered more conservative and realistic, especially in stress scenarios where extreme events occur.\n",
    "\n",
    "The Historic Simulation method typically provides a more accurate real-world estimation because it doesn't rely on theoretical distributions. It uses actual historical data points, which inherently includes all of the actual occurrences, including outliers and market crashes. Therefore, the VaR and ES calculated from historical simulation can often be higher than those calculated from a normal distribution, especially if the historical data includes periods of high volatility or tail events.\n",
    "\n",
    "Comparing the results, I observed that the VaR and ES are the lowest under the Normal distribution, moderate under the T distribution, and highest under the Historical simulation method. This progression reflects the increasing conservatism and potential accuracy in estimating risk, moving from the assumption of normality (which is often criticized in finance) to relying on actual historical behavior of the market, which may include periods of extreme stress."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3: Calculate VaR and ES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My Approach:\n",
    "Calculating Arithmetic Returns:\n",
    "\n",
    "I used the Portfolio.csv and DailyPrices.csv data to compute the arithmetic returns. By doing so, I maintained the presumption that the expected return for all stocks is zero, aligning with the assumption that over the short term, stock returns would revert to their mean, which is represented as zero in this case.\n",
    "\n",
    "Fitting Distributions and Utilizing Copulas:\n",
    "\n",
    "I chose to fit Generalized T models for stocks in portfolios A and B, reflecting my expectation of fat tails — these are the stocks which are potentially more prone to extreme losses or gains than what a normal distribution would suggest. For portfolio C, a normal distribution was assumed, representing a more stable expectation of returns.\n",
    "\n",
    "To aggregate risks across these diverse distributions, I employed copulas, which enable the joining of different marginal distributions into a multi-dimensional distribution, thus capturing the dependence structure between the individual stocks.\n",
    "\n",
    "Calculating VaR and ES:\n",
    "\n",
    "The Value at Risk (VaR) and Expected Shortfall (ES) for each portfolio were computed by simulating the portfolio returns based on the chosen distributions and then extracting the risk metrics from the simulated distributions.\n",
    "\n",
    "Analysis and Comparison:\n",
    "\n",
    "The resulting metrics were expressed not only in absolute terms but also as percentages of the total portfolio value, providing a clear understanding of the risk in relation to the size of the investments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparation Codes. \n",
    "Please see refactored code 1 block after. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "864378.4804192403\n",
      "{'A': {'VaR': 0.03304123774324, 'ES': 0.03712624228374462}, 'B': {'VaR': 0.02659467629964503, 'ES': 0.03167228041669631}, 'C': {'VaR': 0.02466385603537978, 'ES': 0.030553304570608803}} 2.4156383365353666 3.0278117014285995 2.7946534894803666e-06 3.5028772349353863e-06\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import norm, spearmanr, t\n",
    "from scipy.stats import t as t_dist\n",
    "# from Library.tests import fit_normal, fit_general_t, simulate_pca, VaR, ES  # Replace 'my_library' with the actual name of your module\n",
    "\n",
    "\n",
    "# Load your data\n",
    "portfolio_df = pd.read_csv('/Users/angelaliang/Documents/fintech545/Week05/Project/portfolio.csv')\n",
    "daily_prices_df = pd.read_csv('/Users/angelaliang/Documents/fintech545/Week05/Project/DailyPrices.csv')\n",
    "current_prices = daily_prices_df.iloc[-1]\n",
    "\n",
    "# Calculate daily returns: 'returns_df' is a DataFrame with the returns data and 'portfolio_df' contains portfolio holdings\n",
    "returns_df = calculate_returns(daily_prices_df, method=\"DISCRETE\", date_column=\"Date\")\n",
    "\n",
    "# # Fit models to the returns data\n",
    "# fitted_models = {}\n",
    "# simulated_returns = pd.DataFrame()\n",
    "fitted_models = {}\n",
    "for stock in returns_df.columns:\n",
    "    data = returns_df[stock].dropna()\n",
    "    if stock in portfolio_df[portfolio_df['Portfolio'].isin(['A', 'B'])]['Stock'].values:\n",
    "        # Fit T distribution for portfolios A and B\n",
    "        mu, sigma, nu = fit_general_t_simple(data.values)\n",
    "        fitted_models[stock] = {'type': 't', 'mu': mu, 'sigma': sigma, 'nu': nu}\n",
    "    else:\n",
    "        # Fit normal distribution for portfolio C\n",
    "        mu, sigma = fit_normal(data.values)\n",
    "        fitted_models[stock] = {'type': 'normal', 'mu': mu, 'sigma': sigma}\n",
    "\n",
    "# Step 4: Simulation and Copula Use\n",
    "cov_matrix = returns_df.cov().values  # returns_df.drop(columns=['Date']).cov().values?\n",
    "nsim = 5000\n",
    "simulated_returns = simulate_pca(cov_matrix, nsim)\n",
    "# simulated_returns\n",
    "# Step 5: Calculate VaR and ES\n",
    "# Placeholder for the calculation of VaR and ES based on simulated returns\n",
    "# Adapt the VaR and ES calculations to fit your specific requirements\n",
    "# Initialize a dictionary to hold VaR and ES for each portfolio\n",
    "\n",
    "portfolio_risk_metrics = {'A': {}, 'B': {}, 'C': {}}\n",
    "\n",
    "# Example: Calculate VaR and ES for portfolio A (adjust for B and C accordingly)\n",
    "# This is a simplified placeholder. In reality, you would use the fitted models to simulate returns\n",
    "# and calculate VaR and ES based on those simulations.\n",
    "for portfolio in ['A', 'B', 'C']:\n",
    "    portfolio_stocks = portfolio_df[portfolio_df['Portfolio'] == portfolio]['Stock']\n",
    "    stock_returns = returns_df[portfolio_stocks.values]\n",
    "    # Placeholder for aggregating stock returns into a portfolio return.\n",
    "    # You might need a more complex aggregation based on weights and simulated returns.\n",
    "    portfolio_return = stock_returns.mean(axis=1)\n",
    "    VaR, ES = VaR_ES(portfolio_return)  # Assuming VaR_ES is your VaR and ES calculation function\n",
    "    portfolio_risk_metrics[portfolio]['VaR'] = VaR\n",
    "    portfolio_risk_metrics[portfolio]['ES'] = ES\n",
    "\n",
    "\n",
    "# Calculate total VaR and ES across all portfolios\n",
    "# total_returns = simulated_returns[portfolio_df['Stock']].multiply(portfolio_df['Holding'].values, axis=1).sum(axis=1)\n",
    "total_VaR, total_ES = total_VaR, total_ES = VaR_ES(simulated_returns.sum(axis=1))\n",
    "# portfolio_risk_metrics = portfolio_risk_metrics.append({\n",
    "#     'Portfolio': 'Total',\n",
    "#     'VaR95': total_VaR,\n",
    "#     'ES95': total_ES\n",
    "# }, ignore_index=True)\n",
    "\n",
    "# Calculate the total portfolio value\n",
    "total_portfolio_value = (daily_prices_df.iloc[-1, 1:] * portfolio_df.set_index('Stock')['Holding']).sum()\n",
    "print(total_portfolio_value)\n",
    "# Calculate the VaR and ES as percentages of the total portfolio value\n",
    "# portfolio_risk_metrics['VaR95_Pct'] = portfolio_risk_metrics['VaR95'] / total_portfolio_value\n",
    "# portfolio_risk_metrics['ES95_Pct'] = portfolio_risk_metrics['ES95'] / total_portfolio_value\n",
    "VaR95_Pct = total_VaR / total_portfolio_value\n",
    "ES95_Pct = total_ES / total_portfolio_value\n",
    "\n",
    "# Print or save the risk metrics\n",
    "print(portfolio_risk_metrics,total_VaR, total_ES, VaR95_Pct, ES95_Pct)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              VaR95          ES95  VaR95_Pct  ES95_Pct\n",
      "A      2.856013e+04  3.209112e+04   0.033041  0.037126\n",
      "B      2.298787e+04  2.737684e+04   0.026595  0.031672\n",
      "C      2.131891e+04  2.640962e+04   0.024664  0.030553\n",
      "Total  2.088026e+06  2.617175e+06   2.415638  3.027812\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def calculate_portfolio_metrics(portfolio_df, returns_df, total_portfolio_value, nsim):\n",
    "    portfolio_risk_metrics = {}\n",
    "    for portfolio in ['A', 'B', 'C']:\n",
    "        portfolio_stocks = portfolio_df[portfolio_df['Portfolio'] == portfolio]\n",
    "        stock_returns = returns_df[portfolio_stocks['Stock'].values]\n",
    "        portfolio_return = stock_returns.mean(axis=1)\n",
    "        VaR, ES = VaR_ES(portfolio_return)\n",
    "        portfolio_risk_metrics[portfolio] = {\n",
    "            'VaR95': VaR*total_portfolio_value,\n",
    "            'ES95': ES*total_portfolio_value,\n",
    "            'VaR95_Pct': VaR,\n",
    "            'ES95_Pct': ES,\n",
    "        }\n",
    "\n",
    "    total_returns = simulate_pca(returns_df.cov(), nsim)\n",
    "    total_VaR, total_ES = VaR_ES(total_returns.sum(axis=1))\n",
    "    portfolio_risk_metrics['Total'] = {\n",
    "        'VaR95': total_VaR*total_portfolio_value,\n",
    "        'ES95': total_ES*total_portfolio_value,\n",
    "        'VaR95_Pct': total_VaR,\n",
    "        'ES95_Pct': total_ES,\n",
    "    }\n",
    "\n",
    "    return portfolio_risk_metrics\n",
    "\n",
    "\n",
    "# Calculate total portfolio value\n",
    "total_portfolio_value = (daily_prices_df.iloc[-1, 1:] * portfolio_df.set_index('Stock')['Holding']).sum()\n",
    "\n",
    "# Calculate daily returns\n",
    "returns_df = calculate_returns(daily_prices_df)\n",
    "\n",
    "# Calculate portfolio metrics\n",
    "portfolio_metrics = calculate_portfolio_metrics(portfolio_df, returns_df, total_portfolio_value, nsim=5000)\n",
    "\n",
    "# Convert the metrics dictionary to a DataFrame for nicer output\n",
    "metrics_df = pd.DataFrame.from_dict(portfolio_metrics, orient='index')\n",
    "\n",
    "# Display the formatted DataFrame\n",
    "print(metrics_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing VaR and ES Under Different Probabilistic Distributions:\n",
    "In comparison to Week 4's VaR results, which I assume used a different risk modeling technique (like Historical Simulation or an alternative distribution assumption), this week's approach using the Generalized T distribution and normal distribution with copulas would typically produce different results.\n",
    "\n",
    "Week 4 Analysis:\n",
    "\n",
    "For Week 4, I recall using an Exponentially Weighted Moving Covariance (EWMC) method, which placed a greater emphasis on recent returns. This method likely produced a smoother distribution of potential outcomes and, as a result, a different profile of risk metrics.\n",
    "\n",
    "Current Analysis:\n",
    "\n",
    "The Generalized T models acknowledge that asset returns do not follow a normal distribution and exhibit 'fat tails.' As such, the VaR and ES calculated this week tend to be higher, suggesting a more conservative stance on risk. This is particularly prudent for portfolios A and B, where the tail risk is of greater concern.\n",
    "\n",
    "Historical Simulation:\n",
    "\n",
    "As an alternative, the Historical Simulation method, which directly uses empirical data without the assumption of any specific distribution, was employed to evaluate the actual behavior of the market. This approach might yield a higher VaR and ES compared to the normal distribution, especially if the historical data captures periods of extreme market turmoil."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
